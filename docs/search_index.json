[["index.html", "Interactions et modifications d’effet en Epidémiologie Chapitre 1 Présentation", " Interactions et modifications d’effet en Epidémiologie CERPOP, INSERM, EQUITY Team Last compiled on 18 septembre, 2023 Chapitre 1 Présentation Ce document a été rédigé en tant que document de synthèse du travail du groupe “Interaction” de l’équipe EQUITY, CERPOP. Ce travail a consisté en une revue de la littérature et en une application détaillée des méthodes sur des analyses illustratives, dans un but d’auto-formation et pédagogique. Les participant.e.s du groupe de travail sont : Hélène COLINEAUX Léna BONIN Camille JOANNES Benoit LEPAGE Lola NEUFCOURT Ainhoa UGARTECHE The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapitre 2 Introduction 2.1 Quand étudier les interactions ? 2.2 Les points les plus importants 2.3 Avertissement", " Chapitre 2 Introduction Comment telle prédisposition génétique et telle exposition environnementale inter-agissent-elles ? L’effet de tel traitement varie-t-il selon les circonstances ? Selon les caractéristiques du patient ? Telle intervention peut-elle être bénéfique pour un groupe social et délétère pour un autre ? De nombreuses questions épidémiologiques impliquent des mécanismes d’interactions ou de modifications d’effet. Pourtant, étudier ces mécanismes restent encore complexe aujourd’hui sur le plan méthodologique : quelle démarche adopter ? sur quelle échelle mesurer cette interaction ? comment interpréter les coefficients ? et cetera. Dans ce document, nous proposons une synthèse de la littérature et une démarche progressive et appliquée pour explorer ces questions. 2.1 Quand étudier les interactions ? 2.1.1 Prediction versus causalité La science des données cherche à répondre à 3 types d’objectifs [1] : Selon le type d’objectif, la démarche d’analyse et les enjeux méthodologiques ne vont pas être les mêmes. Si l’objectif est prédictif, la démarche va être centrée sur la prédiction de l’outcome, à partir de covariables sélectionnées afin d’optimiser les performances de la prédiction, tout en prenant en compte leur disponibilité en pratique et la parcimonie du modèle. Dans une démarche explicative, ou étiologique, au contraire, la démarche va être centrée sur l’estimation d’un effet causal, en prenant en compte les covariables en fonction de leur rôle vis-à-vis de l’effet d’intérêt (facteurs de confusion, colliders, médiateurs…). En épidémiologie, à l’exception des cas où l’on souhaite développer un test ou score diagnostique ou pronostique, les objectifs sont le plus souvent explicatifs. On cherche en effet, la plupart du temps, à identifier des liens de cause à effet, afin de pouvoir agir sur les causes pour modifier les effets. Finalement, pour répondre à la question “quand doit-on prendre en compte les interactions ?”, il est d’abord nécessaire d’identifier dans quel type de démarche l’on s’inscrit : Démarche prédictive : on ajoutera alors les interactions dans le modèle de prédiction, pour le rendre plus flexible, si cela améliore les performances de la prédiction [2]. Démarche explicative/étiologique : on étudiera les interactions ou modifications d’effet, si cela répond directement à l’objectif. Par exemple : Si l’objectif est du type “l’effet de \\(\\small X\\) sur \\(\\small Y\\) varie-t-il en fonction de \\(\\small V\\) ?”, on prendra en compte l’interaction entre \\(\\small X\\) et \\(\\small V\\). Les objectifs qui nécessitent la prise en compte de l’interaction peuvent aussi être du type : “Quel est l’effet conjoint de \\(\\small X\\) et \\(\\small V\\) sur \\(\\small Y\\) ?” ou “Quel part de l’effet de \\(\\small X\\) sur \\(\\small Y\\) disparaît quand \\(\\small V\\) est modifié ?”, etc. Par contre, si l’objectif est simplement d’estimer l’effet de \\(\\small X\\) sur \\(\\small Y\\), ou l’effet médié par un médiateur \\(\\small M\\), la prise en compte des interactions entre \\(\\small X\\) et des covariables (facteurs de confusion ou médiateurs) n’est pas indispensable pour répondre à la question scientifique. Un effet “moyen” pourra être estimé. Des termes d’interactions peuvent cependant être ajoutés (mais non interprétés), si cela améliore la précision de l’estimation (enjeu d’optimisation du modèle). 2.1.2 Types d’objectifs Dans ce document, nous nous intéresserons principalement aux interactions et modifications d’effet dans une démarche étiologique/ explicative. Les objectifs pouvant nécessiter l’étude de l’interaction/modification d’effet sont [2] : Cibler des sous-groupes. Par exemple, identifier des sous-groupes pour lesquels l’intervention aura le plus d’effet afin de pouvoir cibler l’intervention en cas de ressources limitées, ou s’assurer que l’intervention est bénéfique pour tous les groupes et pas délétères pour certains groupes. Explorer les mécanismes d’un effet. Par exemple, en cas d’intervention qui n’a d’effet qu’en présence ou absence d’une caractéristiques particulière (définition mécanistique de l’interaction) ou seulement conjointement à une autre intervention. Etudier l’effet d’une intervention pour éliminer une partie de l’effet d’une exposition non modifiable. Par exemple, quelle part de l’effet du niveau d’éducation des parents sur la mortalité disparaîtrait si on intervenait sur le tabagisme à l’adolescence ? Ce type d’objectif est proche d’un objectif ciblant la médiation d’un effet, par exemple la médiation de l’effet du niveau d’éducation des parents par le tabagisme, mais les mécanismes envisagés et explorés ne sont pas exactement les mêmes. Explorer ces deux types de mécanismes peut nécessiter des approches spécifiques (voir chapitre 17) 2.2 Les points les plus importants La première étape importante consiste donc à définir précisément l’objectif : L’objectif est-il de type descriptif, prédictif ou explicatif ? Si l’on est dans une démarche explicative, d’inférence causale, est-ce que la mesure d’un effet d’interaction est nécessaire pour y répondre ? (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Ensuite, de nombreuses questions se posent pour réaliser une analyse d’interaction, auxquelles nous tentons de répondre dans ce document : S’agit-il d’une interaction ou une modification d’effet ? (Chapitre 4) Sur quelle échelle la mesure-t-on ? Un effet d’interaction peut en effet être défini sur une échelle multiplicative ou additive, et les résultats entre ces échelles peuvent être contradictoires. (Chapitre 5) Quels paramètres présenter et comment les interpréter ? (Chapitre 6) Comment estimer ces paramètres ? (Chapitre 9 et Chapitre 10) Comment représenter cette interaction graphiquement ? (Chapitre 11) 2.3 Avertissement Les analyses d’effets d’interaction (ou de modifications d’effets) sont peu puissantes. Pour observer un effet d’interaction “statistiquement signitificatif”, le nombre de sujets nécessaire est habituellement beaucoup plus élevé que le nombre de sujets nécessaire permettant d’observer une différence global entre 2 moyennes ou pourcentages. A titre d’exemple, Brookes S et al (2004) [3] décrivent que dans un contexte d’essai contrôlé randomisé à deux bras parallèles, équilibrés, incluant un nombre de sujets \\(N\\) optimisé pour observer une différence \\(\\Delta\\) significative entre un groupe exposé et un groupe non-exposé avec une puissance de 80%, la puissance pour observer un effet d’interaction de taille similaire (\\(\\approx \\Delta\\)) ne sera que de 29%. Pour observer une effet de taille similaire (\\(\\approx \\Delta\\)) de manière significative, le nombre de sujets à recruter sera 4 fois plus élevé (\\(4 \\times N\\)). Si l’on cherche à mesurer de manière significative des effets d’interaction plus petits que l’effet global entre groupe exposé et non-exposé, le nombre de sujets nécessaires augmente de manière spéctaculaire : 6 fois plus élevé (\\(6 \\times N\\)) pour rechercher une interaction un peu plus petite correspondant à 80% de l’effet global (\\(0,80 \\times \\Delta\\)), 15 fois plus élevé (\\(15 \\times N\\)) pour rechercher une interaction égale à la moitié de l’effet global \\(\\left(\\frac{\\Delta}{2}\\right)\\), 100 fois plus élevé (\\(100 \\times N\\)) pour rechercher une petite interaction égale à 20% de l’effet global (\\(\\left(\\frac{\\Delta}{5}\\right)\\)). Références "],["définitions-préalables.html", "Chapitre 3 Définitions préalables 3.1 Variables et probabilités 3.2 Mesures d’effets 3.3 Effets conditionnels et marginaux", " Chapitre 3 Définitions préalables 3.1 Variables et probabilités On note : un outcome : \\(\\small Y\\), deux expositions : \\(\\small X\\) et \\(\\small V\\) La probabilité de l’outcome \\(\\small Y\\) dans chaque strate définie par les 2 expositions est notée : \\(\\small p_{xv} = P(Y = 1|X = x,V = v)\\) Exemple On a deux expositions \\(\\small X\\), le tabagisme actif à 20 ans, et \\(\\small V\\), le fait d’avoir vécu un évènement traumatique pendant l’enfance. L’outcome \\(\\small Y\\) est binaire et représente le fait d’avoir au moins une pathologie chronique à 60 ans (\\(\\small Y=1\\)) ou aucune (\\(\\small Y=0\\)). On décrit (données complètement fictives) : Interprétation : La probabilité d’avoir au moins une pathologie chronique à 60 ans quand on n’a pas vécu d’événement traumatique pendant l’enfance et pas fumé à 20 ans est de 10%, tandis qu’elle est de 90% quand on a vécu un événement traumatique et fumé. 3.2 Mesures d’effets L’effet d’une variable \\(\\small X\\) sur \\(\\small Y\\) peut être mesuré sur deux échelles : additive (différence de risques ou de probabilités) ou multiplicative (rapport de risques ou de probabilités). Concernant les différences de risques (DR, effets additifs) On va noter \\(\\small P(Y = 1|do(X = 1))\\) la probabilité d’observer \\(\\small Y=1\\) sous une intervention contrefactuelle où la totalité de la population étudiée est exposée à \\(\\small X=1\\) (notée \\(\\small do(X=1)\\)). De même, on va noter \\(\\small P(Y = 1|do(X=1,V=1))\\) la probabilité d’observer \\(\\small Y=1\\) sous une intervention contrefactuelle conjointe à la fois sur \\(\\small X\\) et sur \\(\\small V\\) où la totalité de la population étudiée est exposée à \\(\\small X=1\\) et \\(\\small V=1\\) (notée \\(\\small do(X=1,V=1)\\)). L’effet d’une exposition \\(\\small X\\) binaire sur \\(\\small Y\\) est : \\(\\small DR(X) = P(Y = 1|do(X = 1)) - P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1|X = 1) - P(Y = 1|X = 0) = p_1-p_0\\) L’effet conjoint de \\(\\small X\\) et \\(\\small V\\) est : \\(\\small DR(X,V) = p_{11}-p_{00}\\) L’effet de \\(\\small X\\) sur \\(\\small Y\\) pour chaque valeur fixée de \\(\\small V\\) est : \\(\\small DR(X,V=0) = p_{10}-p_{00}\\) et \\(\\small DR(X,V=1) = p_{11}-p_{01}\\) Exemple Différences de risques pour l’exemple 1 \\(\\small DR(X,V) = p_{11}-p_{00} = 0,90 - 0,10 = +0,80\\) \\(\\small DR (X,V=0) = p_{10}-p_{00} = 0,40 - 0,10 = +0,30\\) \\(\\small DR (X,V=1) = p_{11}-p_{01} = 0,90 - 0,20 = +0,70\\) Le fait d’être doublement exposé (tabagisme + événement traumatique) par rapport à pas du tout augmente le risque d’avoir au moins une pathologie chronique à 60 ans de +80%. Dans une population n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 ans augmente le risque d’avoir au moins une pathologie chronique à 60 ans de +30%, alors que dans une population ayant vécu un événement traumatique, il est augmenté de +70%. Concernant les rapports de risques (RR, effets multiplicatifs) On peut notamment utiliser les risques relatifs (RR). On donc : L’effet d’une exposition \\(\\small X\\) binaire sur \\(\\small Y\\) est : \\(\\small RR(X) = \\frac{P(Y = 1| do(X = 1)) }{ P(Y = 1|do(X = 0))}\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par : \\(\\small \\frac{P(Y = 1| do(X = 1)) }{ P(Y = 1|do(X = 0))} = \\frac{p_1}{p_0}\\) L’effet conjoint de \\(\\small X\\) et \\(\\small V\\) est : \\(\\small RR(X,V) = \\frac{p_{11}}{p_{00}}\\) L’effet de \\(\\small X\\) sur \\(\\small Y\\) pour chaque valeur fixée de \\(\\small V\\) est : \\(\\small RR(X,V=0) = \\frac{p_{10}}{p_{00}}\\) et \\(\\small RR(X,V=1) = \\frac{p_{11}}{p_{01}}\\) Exemple Risques relatifs pour l’exemple 1 \\(\\small RR(X,V) = \\frac{0,9}{0,1} = \\times 9\\) \\(\\small RR(X,V=0) = \\frac{0,4}{0,1} = \\times 4\\) \\(\\small RR(X,V=1) = \\frac{0,9}{0,2} = \\times 4,5\\) Le risque d’avoir au moins une pathologie chronique à 60 ans quand on est doublement exposé (tabagisme + événement traumatique) par rapport à pas du tout est multiplié par 9. Dans une population n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 ans multiplie le risque par 4, alors que dans une population ayant vécu un événement traumatique, il est multiplié par 4,5. Une autre échelle multiplicative fréquemment utilisée est l’échelle des odds-ratios (OR) L’échelle des Odds-Ratios (OR) est fréquemment utilisée car on peut l’obtenir facilement à partir d’un modèle de régression logistique (en utilisant l’exponentielle des coefficients de la régression logistique). L’odds correspond à la cote d’une probabilité \\(p\\) et est définie par \\(\\text{odds}(p) = \\frac{p}{1-p}\\). L’odds-ratio est le rapport de la cote dans le groupe exposé divisée par la cote dans le groupe non-exposé. Si on reprend l’exemple précédent : L’effet d’une exposition \\(\\small X\\) binaire sur \\(\\small Y\\) est : \\(\\small OR(X) = \\frac{P(Y = 1| do(X = 1)) / [1 - P(Y=1 | do(X=1))] }{ P(Y = 1|do(X = 0)) / [1 - P(Y = 1|do(X = 0))]}\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par : \\(\\small \\frac{P(Y = 1| do(X = 1)) / [1 - P(Y=1 | do(X=1))] }{ P(Y = 1|do(X = 0)) / [1 - P(Y = 1|do(X = 0))]} = \\frac{p_1 / (1-p_1)}{p_0 / (1-p_0)}\\) L’effet conjoint de \\(\\small X\\) et \\(\\small V\\) est : \\(\\small OR(X,V) = \\frac{p_{11} / (1-p_{11})}{p_{00}/(1 - p_{00})}\\) L’effet de \\(\\small X\\) sur \\(\\small Y\\) pour chaque valeur fixée de \\(\\small V\\) est : \\(\\small OR(X,V=0) = \\frac{p_{10} / (1-p_{10})}{p_{00} / (1-p_{00})}\\) et \\(\\small OR(X,V=1) = \\frac{p_{11} / (1-p_{11})}{p_{01} / (1-p_{01})}\\) Exemple Odds-ratios pour l’exemple 1 \\(\\small OR(X,V) = \\frac{0,9 / (1-0,9)}{0,1 / (1-0,1)} = \\times 81\\) \\(\\small OR(X,V=0) = \\frac{0,4 / (1-0,4)}{0,1/(1-0,1)} = \\times 6\\) \\(\\small OR(X,V=1) = \\frac{0,9/(1-0,9)}{0,2/(1-0,2)} = \\times 36\\) La cote d’avoir au moins une pathologie chronique à 60 ans quand on est doublement exposé (tabagisme + événement traumatique) par rapport à pas du tout est multiplié par 81. Dans une population n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 ans multiplie par 6 la cote d’avoir au moins une pathologie chronique à 60 ans, alors que dans une population ayant vécu un événement traumatique, elle est multipliée par 36. 3.3 Effets conditionnels et marginaux Dans une analyse de l’effet d’interaction entre deux expositions binaires \\(X\\) et \\(V\\) sur un outcome \\(Y\\), il sera parfois nécessaire de prendre en compte un ensemble de facteurs de confusion pour estimer les effets causaux. On note \\(L\\) cet ensemble qui peut être constitué par exemple de 3 facteurs de confusion \\(L = \\{L_1 = age, L_2 = sexe, L_3 = comorbidités\\}\\). Au-delà de l’échelle des mesures d’association (additive pour les DR, multiplicative pour les RR et OR), il faudra choisir si on présente des mesures d’associations : conditionnelles c’est-à-dire estimées dans des strates définies par l’ensemble (ou par un sous-ensemble) des facteurs de confusion ou marginales, c’est à dire un effet moyen estimé pour l’ensemble de la population (une moyenne pondérée des associations observées dans les différentes strates de la population). Par exemple, sur l’échelle des odds-ratios, une méthode classiquement utilisée pour estimer l’effet d’interaction entre \\(X\\) et \\(V\\) est d’appliquer une régression logistique de \\(Y\\) en fonction de \\(X\\) et \\(V\\), de leur interaction, ajustée sur les 3 facteurs de confusion (et on suppose que le modèle est correctement spécifié) : \\[\\begin{equation*} \\text{logit} P(Y=1 \\mid X,V,L_1,L_2,L_3) = \\beta_0 + \\beta_X X + \\beta_V V + \\beta_{X \\ast V} X \\ast V + \\beta_{L1} L_1 + \\beta_{L2} L_2 + \\beta_{L3} L_3 \\end{equation*}\\] A partir de ce modèle, il est possible d’estimer directement : l’interaction conjointe de \\(\\small X\\) et \\(\\small V\\) : \\(OR(X,V) \\big| L_1,L_2,L_3 = \\exp(\\beta_X + \\beta_V + \\beta_{X \\ast V})\\) L’effet de \\(\\small X\\) sur \\(\\small Y\\) pour chaque valeur fixée de \\(\\small V\\) : \\(OR(X,V=0) \\big| L_1,L_2,L_3 = \\exp(\\beta_X)\\) \\(OR(X,V=1) \\big| L_1,L_2,L_3 = \\exp(\\beta_X + \\beta_{X \\ast V})\\) Il s’agit d’OR conditionnels, c’est-à-dire “toutes choses égales par ailleurs au niveau individuel”, conditionnellement au sexe, à l’âge et aux comorbidités de chaque individu : d’après ce modèle, l’odds ratio obtenu est indépendant du sexe, de l’âge et des comorbidités. Sa valeur sera identique chez un homme de 35 ans sans comorbidités et chez une femmes de 60 ans avec comorbidités. A partir du même modèle, on peut également estimer des associations marginales, en calculant l’effet moyen observé dans la population. Par exemple pour l’interaction conjointe de \\(\\small X\\) et \\(\\small V\\), on calcule d’abord l’effet populationnel associé à une double exposition \\(X=1\\) et \\(V=1\\), comme une moyenne pondérée des probabilités attendues dans chaque strate \\(\\{l_1,l_2,l_3\\}\\) définie par les facteurs de confusion : \\[\\begin{equation*} p_{11} = \\sum_{l_1,l_2,l_3} P(Y=1 \\mid X = 1, V = 1, L_1 = l_1, L_2 = l_2, L_3 = l_3) \\times P(L_1 = l_1, L_2 = l_2, L_3 = l_3) \\end{equation*}\\] puis on calcul l’effet populationnel associé à une double absence d’exposition \\(X=0\\) et \\(V=0\\) : \\[\\begin{equation*} p_{00} = \\sum_{l_1,l_2,l_3} P(Y=1 \\mid X = 0, V = 0, L_1 = l_1, L_2 = l_2, L_3 = l_3) \\times P(L_1 = l_1, L_2 = l_2, L_3 = l_3) \\end{equation*}\\] l’odds-ratio marginal peut-être obtenu à partir de ces deux probabilités populationnelles \\(OR(X,V) = \\frac{p_{11} / (1-p_{11})}{p_{00} / (1-p_{00})}\\). C’est la méthode qui est appliquée en G-computation (cf. paragraphe 10.1). Si l’on a bien pris en compte les facteurs de confusion, l’interprétation se fait comme une mesure d’association causale moyennée au niveau de l’ensemble de la population (“toutes choses égales par ailleurs au niveau populationnel”, la population étant caractérisée par sa distribution de sexe, d’âge et de comorbidités). Selon le même principe, on peut calculer des risques relatifs (RR) conditionnels ou marginaux, et des différences de risques (DR) conditionnelles ou marginales. Une propriété intéressantes des RR et des DR est que se sont des mesures d’associations collapsibles (anglicisme venant du terme anglais collapsibility) : la mesure conditionnelle est la même que la mesure marginale. [4] En revanche, les odds-ratios (OR) sont des mesures d’associations non-collapsibles, c’est-à-dire qu’un OR conditionnel sera différent d’un OR marginal (en dehors de cas particuliers où l’exposition n’a aucun effet causal sur l’outcome ou bien lorsqu’aucun des facteurs de confusion potentiel n’a d’effet sur l’outcome \\(Y\\)). Cela est parfois source de confusion car : il s’agit de deux estimands différents (par définition \\(OR_\\text{marginal} \\neq OR_\\text{conditionnel}\\), sauf cas particulier), mais l’OR marginal comme l’OR conditionnel sont tous les deux des mesures d’association causales valides (à partir du moment où les facteurs de confusion ont bien été pris en compte dans le calcul de l’OR conditionnel ou de l’OR marginal). [5] Le choix de présenter une association marginale ou une association conditionnelle va donc influencer la valeur du résultat présenté, en particulier si l’on présente des mesures d’association “non-collapsibles” comme les OR. Références "],["intmodif.html", "Chapitre 4 Interaction ou modification d’effets 4.1 Modification d’effets 4.2 Interaction 4.3 Synthèse", " Chapitre 4 Interaction ou modification d’effets Dans le champ des analyses d’interaction, deux termes peuvent être rencontrés : “interaction” et “modification d’effet”. Quel est la différence entre ces deux termes ? 4.1 Modification d’effets La question de la modification d’effet consiste à identifier si un scénario contrefactuel modifiant le traitement ou l’exposition \\(\\small X\\) donne un résultat différent dans différents groupes \\(\\small V\\) de patients (estimer l’effet d’une exposition séparément en fonction d’une autre variable) [6]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait une seule intervention \\(\\small X\\) et que l’analyse était stratifiée sur \\(\\small V\\). On analyse donc l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de \\(\\small V\\). En observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X\\) mais pas celui de \\(\\small V\\). On ajustera sur les facteurs de confusion de la relation \\(\\small X \\rightarrow Y\\). On ne fait pas d’hypothèse sur les mécanismes de la modification d’effet, qui peut être causale (de façon directe ou indirecte), ou non-causale (présence d’une modification d’effet par proxy ou cause commune, sans qu’il existe d’effet direct ou indirect du modificateur d’effet vers le critère de jugement, comme dans la figure en bas de page) [7]. Exemples d’objectifs : identifier des groupes pour lesquels le traitement ne serait pas utile, ou explorer si l’effet du traitement est homogène/hétérogène en fonction de l’âge, du sexe, etc. On a une modification de l’effet de \\(\\small X\\) par \\(\\small V\\) si l’effet de \\(\\small X\\) est différent dans deux strates définies par \\(\\small V\\): en additif : \\(\\small DR(X | V=0) \\neq DR(X | V=1)\\) soit \\(\\small p_{10}-p_{00} \\neq p_{11}-p_{01}\\) en multiplicatif : \\(\\small RR(X | V=0) \\neq RR(X | V=1)\\) soit \\(\\small \\frac{p_{10}}{p_{00}} \\neq \\frac{p_{11}}{p_{01}}\\) Exemple Modification d’effet dans l’exemple 1 L’objectif serait formulé ainsi : l’effet du tabagisme X sur le risque de maladie chronique Y est-il différent lorsqu’on a ou non vécu un événement traumatique V antérieurement ? Les données (fictives) : En additif : effet dans le groupe \\(\\small V=0\\) : \\(\\small DR (X | V=0) = 0,40 - 0,10 = +0,30\\) effet dans le groupe \\(\\small V=1\\) : \\(\\small DR (X | V=1) = 0,90 - 0,20 = +0,70\\) donc \\(\\small DR (X | V=0) \\neq DR (X | V=1)\\) En multiplicatif : effet dans le groupe \\(\\small V=0\\) : \\(\\small RR(X | V=0) = \\frac{0,40}{0,10} = \\times 4,0\\) effet dans le groupe \\(\\small V=1\\) : \\(\\small RR(X | V=1) = \\frac{0,90}{0,20} = \\times 4,5\\) donc \\(\\small RR(X | V=0) \\neq RR(X | V=1)\\) Ici l’effet du tabagisme est différent selon que les personnes ont vécu un événement traumatique ou non, sur l’échelle additive et multiplicative (données fictives). On peut donc dire que le fait d’avoir vécu un événement traumatique modifie l’effet du tabac. Attention, dans cet exemple, on fait l’hypothèse de l’absence de facteurs de confusion entre le tabagisme et l’outcome Y, ce qui est en réalité peu probable. Lorsqu’on utilise les approches causales pour estimer l’effet de \\(\\small X\\) sur \\(\\small Y\\), on va intervenir seulement sur \\(\\small X\\). En G-computation, le code serait : #modèle Q.model &lt;- glm(data=bootData, formula = Y ~ X + V + X*V + L,family = binomial) # Scénarios # data.X0 &lt;- data.X1 &lt;- bootData data.X0$X &lt;- 0 data.X1$X &lt;- 1 # Y contrefactuel bootData$Y.X0.pred &lt;- predict(Q.model, newdata = data.X0, type = &quot;response&quot;) bootData$Y.X1.pred &lt;- predict(Q.model, newdata = data.X1, type = &quot;response&quot;) # Modification d&#39;effet, échelle additive simu.base$est.AI[simu.base$i.simu==i] = round( # effet de X quand V==1 mean(bootData$Y.X1.pred[which(bootData$V == 1),]) - bootData$Y.X0.pred[which(bootData$V == 1),]) - # effet de X quand V==0 mean(bootData$Y.X1.pred[which(bootData$V == 0),]) - bootData$Y.X0.pred[which(bootData$V == 0),]),4) Remarque : on ne peut pas considérer \\(\\small V\\) comme un modificateur de l’effet de \\(\\small X\\) si \\(\\small X\\) est une cause de \\(\\small V\\). Par exemple, si \\(\\small X\\) était le tabagisme à 20 ans, \\(\\small V\\) le fait de souffrir de bronchite chronique obstructive à 50 ans et \\(\\small Y\\) la mortalité. Ca n’aurait pas de sens de demander si l’effet du tabac sur la mortalité varie en fonction de la présence ou non de BPCO, car \\(\\small V\\) est un descendant de \\(\\small X\\) (le tabagisme augmente le risque de BPCO). Lorsqu’on intervient sur \\(\\small X\\), \\(\\small do(X)\\), on modifie donc aussi \\(\\small V\\) car \\(\\small X \\rightarrow V\\) , on est donc obligé d’intervenir aussi sur \\(\\small V\\) (en faisant une analyse de médiation ou d’interaction) pour étudier l’effet de \\(\\small X\\) en fonction de \\(\\small V\\), nous ne sommes donc plus dans le cadre d’une modification d’effet. 4.2 Interaction Quand on s’intéresse à l’interaction, on s’intéresse plutôt à l’effet conjoint de 2 expositions (ou plus) sur un outcome. Il y a une interaction synergique si l’effet conjoint est supérieur à la somme de l’effet individuels. Il y a une interaction antagoniste lorsque l’effet conjoint est inférieur à la somme des effets individuels [6]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait plusieurs interventions, selon le nombre de combinaisons. On analyse donc l’effet du scénario \\(\\small do(X, V)\\). Ici l’effet causal d’intérêt est vraiment l’effet conjoint des deux variables. Dans un schéma observationnel, l’effet causal qui nous intéresse est donc celui de l’interaction \\(\\small X*V\\). On ajustera sur les facteurs de confusion des deux relations \\(\\small X \\rightarrow Y\\) et \\(\\small V \\rightarrow Y\\). On fait l’hypothèse que les mécanismes de l’effet conjoint de \\(\\small X\\) et \\(\\small V\\) sont causaux. Par définition, on a une interaction si l’effet conjoint de \\(\\small X\\) et \\(\\small V\\) sur \\(\\small Y\\) (\\(\\small DR(X,V)\\)) est différent de la somme (ou du produit sur l’échelle multiplicative) : de l’effet isolé de \\(\\small X\\) sur \\(\\small Y\\) (où \\(\\small V\\) est constant, fixé à \\(\\small V=0\\)), noté \\(\\small DR(X,V=0)\\) (ou \\(\\small RR(X,V=0)\\)) et de l’effet isolé de \\(\\small V\\) sur \\(\\small Y\\) (où \\(\\small X\\) est constant, fixé à \\(\\small X=0\\)), noté \\(\\small DR(V,X=0)\\) (ou \\(\\small RR(V,X=0)\\)) On a ainsi, en additif : \\(\\small DR(X,V) \\neq DR(X,V=0) + DR(V,X=0)\\) \\(\\small p_{11}-p_{00} \\neq (p_{10}-p_{00})+(p_{01}-p_{00})\\) \\(\\small p_{11} \\neq p_{10} + p_{01} - p_{00}\\) \\(\\small p_{11} - p_{10} - p_{01} + p_{00} \\neq 0\\) en multiplicatif \\(\\small RR(X,V) \\neq RR(X,V=0) \\times RR(V,X=0)\\) \\(\\small \\frac{p_{11}}{p_{00}} \\neq \\frac{p_{10}}{p_{00}} \\times \\frac{p_{01}}{p_{00}}\\) \\(\\small p_{11} \\neq \\frac{\\frac{p_{10}}{p_{01}}}{p_{00}}\\) \\(\\small \\frac{p_{00} \\times p_{11}}{p_{10} \\times p_{01}} \\neq 1\\) Exemple Interaction dans l’exemple 1 L’objectif serait formulé ainsi : le tabagisme X et le vécu d’un événement traumatique V se potentialisent-il l’un autre pour augmenter le risque de maladie chronique Y ? En additif : effet joint : \\(\\small DR(X,V) = 0,90 - 0,10 = +0.80\\) somme des effets individuels : \\(\\small DR(X,V=0) + DR(V,X=0) = +0,30 +0,10 = +0,40\\) donc \\(\\small DR(X,V) \\neq DR(X,V=0) + DR(V,X=0)\\) En multiplicatif : effet joint : \\(\\small RR(X,V) = \\frac{0,9}{0,1} = \\times 9\\) produit des effets individuels : \\(\\small RR(X,V=0) \\times RR(V,X=0) = 4 \\times 2 = \\times 8\\) donc \\(\\small DR(X,V) \\neq DR(X,V=0) \\times DR(V,X=0)\\) Ici l’effet joint des 2 expositions est supérieur à la somme ou au produit des effets individuels, il y a donc une interaction synergique entre les deux expositions. On peut conclure que l’expérience d’un événement traumatique et le tabagisme se potentialisent pour aboutir à une augmentation du risque de maladies chroniques : ces expositions ont un effet plus fort lorsqu’elles sont présentes toutes les deux que la somme/le produit des deux. Lorsqu’on utilise les approches causales pour estimer l’effet de \\(\\small X\\) sur \\(\\small Y\\), on va intervenir sur \\(\\small X\\) et sur \\(\\small Y\\), contrairement à l’approche précédente ou l’on intervenait seulement sur \\(\\small X\\). En G-computation, le code serait : #modèle Q.model &lt;- glm(data=bootData, formula = Y ~ X + V + X*V + L,family = binomial) # Scénarios # data.X0V0 &lt;- data.X0V1 &lt;- data.X1V0 &lt;- data.X1V1 &lt;- bootData data.X0V0$X &lt;- data.X0V1$X &lt;- 0 data.X1V0$X &lt;- data.X1V1$X &lt;- 1 data.X0V0$V &lt;- data.X1V0$V &lt;- 0 data.X0V1$V &lt;- data.X1V1$V &lt;- 1 # Y contrefactuel Y.X0V0.pred &lt;- predict(Q.model, newdata = data.X0V0, type = &quot;response&quot;) Y.X1V0.pred &lt;- predict(Q.model, newdata = data.X1V0, type = &quot;response&quot;) Y.X0V1.pred &lt;- predict(Q.model, newdata = data.X0V1, type = &quot;response&quot;) Y.X1V1.pred &lt;- predict(Q.model, newdata = data.X1V1, type = &quot;response&quot;) # Interaction additive simu.base$est.AI[simu.base$i.simu==i] = round( # effet joint mean(bootData$Y.X1V1.pred - bootData$Y.X0V0.pred) - # somme des effets individuels mean(bootData$Y.X0V1.pred - bootData$Y.X0V0.pred) + mean(bootData$Y.X1V0.pred - bootData$Y.X0V0.pred),4) 4.3 Synthèse Mathématiquement, les formulations sont équivalentes : échelle additive: \\(\\small p_{10} -p_{00} \\neq p_{11}- p_{01} \\iff p_{11} \\neq (p_{10}+p_{01})- p_{00}\\) échelle multiplicative : \\(\\small p_{10} /p_{00} \\neq p_{11}/ p_{01} \\iff p_{11} \\neq (p_{10} \\times p_{01})/p_{00}\\) La différence se joue plutôt sur : la façon dont la question est posée (effet de \\(\\small X\\) selon \\(\\small V\\), versus effet conjoint de \\(\\small X\\) et \\(\\small V\\)), les hypothèses causales formulées (scénario \\(\\small do(X)|V\\) versus \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur la relation \\(\\small X \\rightarrow Y\\) versus les deux relations \\(\\small X \\rightarrow Y\\) et \\(\\small V \\rightarrow Y\\)) et sur l’intervention contrefactuelle que l’on va réalisée si l’on utilise des approches causales (\\(\\small do(X)|V\\) versus \\(\\small do(X,V)\\)). Il existe des cas où l’identification d’une interaction ou d’une modification d’effet ne conduira pas à la même démarche et donc au même résultat [8]. Prenons le DAG suivant : Dans ce cas, il n’y a pas d’interaction entre \\(\\small A1\\) et \\(\\small A2\\), car il n’y a pas d’effet direct ni indirect de \\(\\small A2 \\rightarrow Y\\). L’effet de \\(\\small A1 \\rightarrow Y\\) restera le même quelle que soit la valeur que l’on pourrait attribuer à \\(\\small A2\\) : \\[\\begin{multline*} \\scriptsize P\\left[Y=1|do(A1=1, A2=0)\\right] - P\\left[Y=1|do(A1=0, A2=0)\\right] = P\\left[Y=1|do(A1=1, A2=1)\\right] \\\\ \\scriptsize - P\\left[Y=1|do(A1=0, A2=1)\\right] \\end{multline*}\\] Par contre, il peut y avoir une modification de l’effet de \\(\\small A1\\) par \\(\\small A2\\), en particulier s’il existe une interaction \\(\\small A1 * L2 \\rightarrow Y\\) ou \\(\\small A1 * L3 \\rightarrow Y\\), on s’attend à ce que les contrastes suivants soient différents : \\[\\begin{multline*} \\scriptsize P[Y=1|do(A1=1),A2=1] - P[Y=1|do(A1=2),A2=1] \\neq P[Y=1|do(A1=1),A2=0] \\\\ \\scriptsize - P[Y=1|do(A1=2),A2=0] \\end{multline*}\\] Références "],["echelle.html", "Chapitre 5 La question des échelles 5.1 Mesures des interactions 5.2 Lien entre les deux échelles 5.3 Synthèse", " Chapitre 5 La question des échelles 5.1 Mesures des interactions Echelle additive Une façon simple de mesurer l’interaction est de mesurer à quel point l’effet conjoint de deux facteurs est différents de la somme de leurs effets individuels [2] : \\(\\small AI = DR(X,V) - [DR(X|V=0) + DR(V|X=0)]\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) Exemple Mesure de l’interaction dans l’exemple 1 On retrouve l’effet d’interaction, calculé/exprimé de différentes façon, Soit la différence entre l’effet joint et la somme des effets individuels (flèche rouge) : \\(\\small DR(X, V) - [DR(X|V=0) + DR(V|X=0)] = 0.8 - (0,3 + 0,1) = +0,4\\) \\(\\small p_{11} - p_{10} - p_{01} + p_{00} = 0,9 - 0,4 - 0,2 + 0,1 = +0,4\\) Soit la différence entre l’effet de X quand V = 1 et quand V = 0 (flèche verte) : \\(\\small (p_{11} - p_{01}) - (p_{10} - p_{00}) = (0,9 - 0,2) - (0,4 - 0,1) = 0,7 - 0,3 = +0,4\\) Soit la différence entre l’effet de V quand X = 1 et quand X = 0 (flèche bleue) : \\(\\small (p_{11} - p_{10}) - (p_{01} - p_{00}) = (0,9 - 0,4) - (0,2 - 0,1) = 0,5 - 0,1 = +0,4\\) On peut l’interpréter ainsi : la probabilité d’avoir une maladie chronique quand on fume augmente de +30% quand on n’a pas vécu d’évémement traumatique (40% contre 10%), et de +70% quand on a vécu un événement traumatique (de 20 à 90%). Donc l’effet du tabac est augmenté de +40% (0,70 - 0,30) quand on a vécu un événement traumatique par rapport à l’effet du tabac quand on n’a pas vécu d’événement traumatique. Echelle multiplicative En cas d’outcome binaire, c’est souvent le RR ou l’OR qui est utilisé pour mesurer les effets. La mesure de l’interaction sur une échelle multiplicative serait donc [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Exemple Mesure de l’nteraction dans l’exemple 1 On retrouve l’effet d’interaction, calculé/exprimé de différentes façon, Soit le rapport entre l’effet joint et le produit des effets individuels (flèche rouge) : \\(\\small \\frac{RR(X, V)}{RR(X| V=0)*RR(V|X=0)} = \\frac{9}{4 \\times 2} = \\times 1,1\\) \\(\\small\\frac{p_{11} / p_{00}}{(p_{10} + p_{01}) / p_{00}} = \\frac{0,9 / 0,1}{(0,4 \\times 0,2) / 0,1} = \\times 1,1\\) Soit le produit de l’effet de X quand V = 1 et quand V = 0 (flèche verte) : \\(\\small \\frac{p_{11} / p_{01}}{p_{10} / p_{00}} = \\frac{0,9 / 0,2}{0,4 / 0,1} = \\frac{\\times 4,5 }{\\times 4} = \\times 1,1\\) Soit le produit de l’effet de V quand X = 1 et quand X = 0 (flèche bleue): ou \\(\\small \\frac{p_{11} / p_{10}}{p_{01} / p_{00}} = \\frac{0,9 / 0,4}{0,2 / 0,1} = \\frac{\\times 2,25}{\\times 2} = \\times 1,1\\) On peut l’interpréter ainsi : la probabilité d’avoir une maladie chronique quand on fume est multiplier par 4 quand on n’a pas vécu d’évémement traumatique (40% contre 10%), et par 4,5 quand on a vécu un événement traumatique (90% contre 20%). Donc l’effet du tabac est multiplié par 1,1 (\\(\\small \\frac{4,5}{4}\\)) quand on a vécu un événement traumatique par rapport à l’effet du tabac quand on n’a pas vécu d’événement traumatique. 5.2 Lien entre les deux échelles Un apparent paradoxe Mesurer l’interaction sur une seule échelle peut être trompeur [9]. On peut régulièrement observer une interaction positive dans une échelle (par exemple \\(\\small p11 - p10 - p01 + p00 &gt; 0\\)) et négative dans l’autre (par exemple \\(\\small (p11 \\times p00) / (p10 \\times p01) &lt;1\\)). Exemple Dans cet exemple (on modifie seulement la probabilité \\(\\small p_{11}\\), en jaune dans le tableau), on observe une interaction additive positive (l’effet de \\(\\small X\\) augmente de +20% quand \\(\\small V=1\\) par rapport à \\(\\small V=0\\)) mais une interaction multiplicative négative (l’effet de \\(\\small X\\) est multiplié par 0,9 - donc diminue - quand \\(\\small V=1\\) par rapport à \\(\\small V=0\\)). Remarque : on retrouverait les mêmes résultats en comparant les effets de \\(\\small V\\) dans les strates de \\(\\small X\\) ou les effets conjoints et somme/produit des effets individuels. Il a même été démontré que si on n’observe pas d’interaction sur une échelle, alors on en observera obligatoirement sur l’autre échelle… [2]. Exemple Dans cet exemple, il n’y a pas d’interaction multiplicative (effet de \\(\\small X\\) identique quelque soit \\(\\small V\\)), mais sur l’echelle additive, on observe une interaction positive. Dans cet autre exemple, il n’y a pas d’interaction additive (effet de \\(\\small X\\) identique quelque soit \\(\\small V\\)), mais sur l’echelle multiplicative, on observe une interaction négative. Le continuum Dans un article de 2019 [10], Vanderweele décrit le continuum existant entre les 2 échelles. Par exemple, dans l’exemple 1, l’interaction additive et multiplicative sont positives. Mais si l’on fait varier la probabilité \\(\\small p_{11}\\) en la diminuant, l’interaction multiplicative devient négative alors que l’interaction additive reste positive. Puis, lorsque la probabilité diminue encore, l’interaction devient négative sur les deux échelles : Interactions pures et qualitatives, interactions inversées Dans ce continuum, si l’on continue à faire varier \\(\\small p_{11}\\), des cas particuliers d’interaction peuvent être retrouvés : Interaction pure de \\(\\small X\\) en fonction de \\(\\small V\\), si \\(\\small X\\) n’a un effet que dans une seule strate de \\(\\small V\\). Par exemple, \\(\\small p_{10} = p_{00}\\) et \\(\\small p_{11} \\neq p_{01}\\). Par exemple (ligne 6) ici, \\(\\small V\\) a un effet (sur les deux échelles) si \\(\\small X=0\\) mais pas si \\(\\small X=1\\) : Interaction qualitative de \\(\\small X\\) en fonction de V, si l’effet de \\(\\small X\\) dans une strate de \\(\\small V\\) va dans la direction opposée de l’autre strate de \\(\\small V\\). Par exemple (ligne 7), \\(\\small V\\) a un effet positif si \\(\\small X=0\\) mais négatif si \\(\\small X=1\\) : Antagonisme parfait : l’effet joint est nul \\(\\small p_{11} - p_{00} = 0\\), alors que les effets individuels sont positifs. Par exemple (ligne 10), \\(\\small p_{11} - p_{00} = 0\\) alors que \\(\\small p_{01} - p_{00} &gt; 0\\) et \\(\\small p_{10} - p_{00} &gt; 0\\) Interaction inversée (ligne 11): l’effet joint est négatif, alors que les effets individuels sont positifs. Par exemple (ligne 10), \\(\\small p_{11} - p_{00} &lt; 0\\) alors que \\(\\small p_{01} - p_{00} &gt; 0\\) et \\(\\small p_{10} - p_{00} &gt; 0\\) 5.3 Synthèse Quelle échelle choisir pour mesurer un effet d’interaction ? Même si en pratique l’échelle multiplicative est plus utilisée, car les outcomes sont souvent binaires en épidémiologie et donc les modèles logistiques sont souvent utilisés [11], il semble y avoir un consensus pour privilégier plutôt l’échelle additive, plus appropriée pour évaluer l’utilité en santé publique [2] [11]. Si on reprend l’exemple ci dessous : \\(\\small X\\) représente un traitement dont on ne dispose que de 100 doses et \\(\\small Y\\) un outcome de santé favorable (guérison). Il faut choisir si on donne 100 doses au groupe \\(\\small V = 0\\) ou au groupe \\(\\small V = 1\\). Si on donne 100 doses : au groupe \\(\\small V = 0\\), 40 personnes seront guéries, soit 30 personnes de plus que l’évolution naturelle (40 - 10) au groupe \\(\\small V = 1\\), 70 personnes seront guéries, soit 50 personnes de plus que l’évolution naturelle (70 - 20). Il semble donc préférable d’allouer les doses au groupe \\(\\small V=1\\), car on guéri 20 personnes de plus (50 - 30). Pourtant si on avait réfléchi à partir de l’échelle multiplicative, on aurait choisi le groupe \\(\\small V=0\\) car : l’effet du traitement est de RR=4 dans le groupe \\(\\small V = 0\\) (\\(\\small \\frac{40}{10} =4\\)x plus de personnes guéries par rapport à l’évolution naturelle) et de RR=3,5 dans le groupe \\(\\small V = 1\\) (\\(\\small \\frac{70}{20}) =3.5\\)x plus de personnes guéries par rapport à l’évolution naturelle. On peut donc conclure à un effet multiplicatif plus fort d’un traitement dans un groupe alors qu’en terme d’utilité (nombre de personnes favorablement impactées), l’échelle additive nous conduirait à choisir l’autre groupe… Idéalement, les interactions devraient cependant être reportées sur les 2 échelles [11] [2]. Références "],["param.html", "Chapitre 6 Types de paramètres 6.1 Sur l’échelle multiplicative 6.2 Sur l’échelle additive", " Chapitre 6 Types de paramètres Plusieurs paramètres peuvent être utilisés pour décrire une interaction, sur l’échelle additive ou multiplicative. 6.1 Sur l’échelle multiplicative Avec les risques relatifs (MI) On a déjà défini précédemment un paramètre d’interaction sur l’échelle multiplicative (MI),défini à partir des risques relatifs [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Avec les Odds Ratio (MI) Souvent en épidémiologie, lorsque l’outcome Y est binaire, les effets sont mesurés par des odds ratios estimés à partir de modèles de régression logistique. Un paramètre d’interaction sur l’échelle multiplicative (\\(\\small MI_{OR}\\)) peut être estimé à partir de ces OR [2] : \\(\\small MI_{OR} = \\frac{OR_{11}}{OR_{10} \\times OR_{01}}\\) En général, la mesure \\(\\small MI_{OR}\\) et \\(\\small MI_{RR}\\) seront proches si l’outcome est rare [2]. 6.2 Sur l’échelle additive Avec les différences de risques (AI) On a déjà défini un paramètre d’interaction sur l’échelle additive (AI) à partir des différences d’effets [2] : \\(\\small AI = DR(X, V) - [DR(X|V=0) + DR(V|X=0)]\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) Excès de risque, à partir des RR (RERI) Lorsque seulement les risques relatifs sont donnés mais que l’on souhaite évaluer l’interaction sur l’échelle additive, “l’excès de risque du à l’interaction” (RERI) ou “interaction contrast ratio” (ICR), peut être estimé à partir des risques relatifs [2] : \\(\\small RERI = \\frac{AI}{p_{00}} = \\frac{p_{11} - p_{10} - p_{01} + p_{00}}{p_{00}}\\) \\(\\small RERI = RR_{11} - RR_{10} - RR_{01} + 1\\) On voit que le RERI correspond à l’interaction mesurée sur l’échelle additive, rapportée au risque de base \\(p_{00}\\). Il faut noter que, bien que le RERI donne la direction (positive, négative ou nulle) de l’interaction additive, nous ne pouvons pas utiliser le RERI pour évaluer l’ampleur de l’interaction additive, à moins de connaître au moins \\(\\small p_{00}\\), dans ce cas on retrouve \\(\\small AI = p_{00} \\times RERI\\). Si l’on a seulement l’OR et que l’outcome est rare, les OR peuvent approximer les RR, on a donc : \\(\\small RERI_{OR} = OR_{11} - OR_{10} - OR_{01} + 1 \\approx RERI_{RR}\\) Le “Synergie index” (SI) Il s’agit d’un paramètre explorant aussi l’interaction additive [2]. Il est définit à partir des Augmentation Relatif du Risque (ARR). Pour rappel, l’Augmentation Relative du Risque liée à l’exposition jointe correspond à l’augmentation absolue du risque (différence de risques), exprimée en pourcentage par rapport au risque de base \\(p_{00}\\). \\(\\small ARR(X,V) = \\frac{DR(X,V)}{p_{00}} = \\frac{p_{11}-p_{00}}{p_{00}}= RR_{11}-1\\) L’augmentation relative du risque liée à l’exposition \\(\\small X\\) ou \\(\\small V\\), exprimées en pourcentage par rapport au risque de base \\(p_{00}\\) sont respectivement : \\(\\small ARR(X|V=0) = \\frac{p_{10}-p_{00}}{p_{00}}= RR_{10}-1\\) et \\(\\small ARR(V|X=0) = \\frac{p_{01}-p_{00}}{p_{00}}= RR_{01}-1\\) L’index synergique correspond à l’augmentation relative du risque liée à l’exposition jointe, rapportée à la somme des augmentations relatives du risque liées à la 1ère et la 2ème exposition. \\(\\small SI = \\frac{RR_{11} - 1}{(RR_{10} - 1) + (RR_{01}-1)}\\). On peut aussi l’interpréter ainsi : la différence liée à l’effet joint \\(\\small DR(X,V)\\) est égale à \\(SI\\) fois la somme des différences liées aux effets individuels \\(\\small DR(X|V=0) + DR(V|X=0)\\), car : \\(\\small SI = \\frac{p_{11}-p_{00}}{(p_{10}-p_{00}) + (p_{01} - p_{00})}\\) Si le dénominateur est positif: si \\(\\small SI &gt; 1\\), alors \\(\\small AI &gt; 0\\) et \\(\\small RERI_{RR} &gt; 0\\) si \\(\\small SI &lt; 1\\), alors \\(\\small AI &lt; 0\\) et \\(\\small RERI_{RR} &lt; 0\\) L’interprétation de l’indice de synergie devient difficile dans les cas où l’effet de l’une des expositions a un effet négatif et que le dénominateur de \\(S\\) est inférieur à 1. Proportion attribuable (AP) Il s’agit aussi d’un paramètre explorant l’interaction additive : \\(\\small AP = \\frac{RR_{11} - RR_{10} - RR_{01} + 1}{RR_{11}}\\). Ce paramètre mesure la proportion du risque dans le groupe doublement exposé qui est due à l’interaction. L’AP est en lien avec le \\(\\small RERI_{RR}\\) : AP &gt; 0 si et seulement si \\(\\small RERI_{RR}\\) &gt; 0 AP &lt; 0 si et seulement si \\(\\small RERI_{RR}\\) &lt; 0. En fait \\(\\small AP = \\frac{RERI_{RR}}{RR_{11}-1}\\). Références "],["présentation-des-résultats.html", "Chapitre 7 Présentation des résultats 7.1 Recommandations 7.2 Proposition", " Chapitre 7 Présentation des résultats 7.1 Recommandations Knol et VanderWeele ont émis des recommandations concernant la présentation des résultats d’une analyse d’interaction [11]. Ces recommandations sont : Pour une analyse d’une modification d’effet de \\(\\small X\\) sur \\(\\small Y\\) par \\(\\small V\\) Présenter les effectifs dans chaque catégorie avec et sans l’outcome (\\(\\small N_{x,v}(Y=1)\\) et \\(\\small N_{x,v}(Y=0)\\)) Présenter les risques relatifs (RR), les OR ou les différences de risques (RD) avec les intervalles de confiance (IC) pour chaque strate de \\(\\small X\\) et de \\(\\small V\\) avec une seule catégorie de référence (éventuellement prise comme la strate \\(\\small X \\cap V\\) présentant le plus faible risque de \\(\\small Y\\)). Présenter les RR, OR ou RD avec les IC de l’effet de \\(\\small X\\) sur \\(\\small Y\\) dans les strates de \\(\\small V\\) Présenter les mesures de la modification de l’effet avec les IC, sur des échelles additives (par exemple, RERI) et multiplicatives. Énumérer les facteurs de confusion pour lesquels la relation entre \\(\\small X\\) et \\(\\small Y\\) a été ajustée. Exemple de présentation avec les données fictives de l’exemple 1, modification de l’effet de X par V : Interaction \\(\\small X*V\\) sur \\(\\small Y\\) Présenter les effectifs dans chaque catégorie avec et sans l’outcome (\\(\\small N_{x,v}(Y=1)\\) et \\(\\small N_{x,v}(Y=0)\\)) Présenter les risques relatifs (RR), les OR ou les différences de risques (RD) avec les intervalles de confiance (IC) pour chaque strate de \\(\\small X\\) et de \\(\\small V\\) avec une seule catégorie de référence (éventuellement prise comme la strate \\(\\small X \\cap V\\) présentant le plus faible risque de \\(\\small Y\\)). Présenter les RR, OR ou RD avec les IC de l’effet de \\(\\small X\\) sur \\(\\small Y\\) dans les strates de \\(\\small V\\) et de \\(\\small V\\) sur \\(\\small Y\\) dans les strates de \\(\\small X\\). Présenter les mesures de la modification de l’effet d’interaction avec les IC sur des échelles additives (par exemple, RERI) et multiplicatives. Énumérer les facteurs de confusion pour lesquels la relation entre \\(\\small X\\) et \\(\\small Y\\) et la relation entre \\(\\small V\\) et \\(\\small Y\\) ont été ajustées. Exemple de présentation avec les données fictives de l’exemple 1, interaction entre X et V : 7.2 Proposition Ces recommandations sont très utiles lorsque les interactions ont été évaluées à partir de modèles de régression (logistiques, log-linéaires ou linéaires) permettant d’estimer directement des OR, des RR ou des DR, condionnellement aux facteurs de confusion. En inférence causale, des assocations marginales plutôt que conditionnelles sont souvent estimées (que ce soit en termes de difference de risques, de risques relatifs ou d’odds ratio). Dans la suite de ce document, nous proposons une variante des recommandations de Knol et VanderWeele, adaptée à des estimations marginales. Nous proposons en effet : De présenter les effets marginaux ou proportions prédites de \\(\\small Y\\) dans chaque strate \\(\\small X \\cap V\\), plutôt les effectifs avec et sans l’outcome Ne pas forcément présenter une différence de risques ou un rapport de risques pour chaque strate de \\(\\small X\\) et de \\(\\small V\\) avec une seule catégorie de référence Mais présenter les effets de \\(\\small X\\) dans chaque strate de \\(\\small V\\) et de \\(\\small V\\) dans chaque strate de \\(\\small X\\) (si analyse d’interaction) sur une échelle multiplicative et additive. Exemple de présentation avec les données fictives de l’exemple 1, interaction entre X et V : Références "],["simulations.html", "Chapitre 8 Simulations", " Chapitre 8 Simulations Pour la description des différents types d’estimation, on a simulé des données selon le DAG suivant (toutes les variables sont binaires). Les deux expositions d’intérêt sont \\(A_1\\) et \\(A_2\\), l’outcome est \\(Y\\), et \\(L_1\\), \\(L_2\\) et \\(L_3\\) sont 3 facteurs de confusion : Les équations structurelles associées au DAG sont décrites ci-dessous, les paramètres correspondent aux paramètres renseignés dans le code de simulation. \\[\\begin{align*} \\small P(L1 = 1) =&amp; p_{L_1} \\\\ \\small P(L2 = 1) =&amp; p_{L_2} \\\\ \\small P(L3 = 1) =&amp; p_{L_3} \\\\ \\small P(A1 = 1 \\mid L1, L2) =&amp; \\beta_{A_1} + \\beta_{L_1,A_1} L1 + \\beta_{L_2,A_1} L2 \\\\ \\small P(A2 = 1 \\mid L1, L3) =&amp; \\beta_{A_2} + \\beta_{L_1,A_2} L1 + \\beta_{L_3,A_2} L3 \\\\ \\small P(Y = 1 \\mid L1, L2, L3, A1, A2) =&amp; \\beta_{Y} + \\beta_{L_1,Y} L1 + \\beta_{L_2,Y} L2 + \\beta_{L_3,Y} L3 \\\\ &amp; + \\beta_{A_1,Y} A1 + \\beta_{A_2,Y} A2 + \\beta_{A_1 \\ast A_2,Y} (A1 \\ast A2) \\end{align*}\\] Le code ayant permis de simuler les données est le suivant : rm(list=ls()) param.causal.model &lt;- function(p_L1 = 0.50, # baseline confounders p_L2 = 0.20, # baseline confounders p_L3 = 0.70, # baseline confounders b_A1 = 0.10, # modèle de A1 b_L1_A1 = 0.15, # modèle de A1 b_L2_A1 = 0.25, # modèle de A1 b_A2 = 0.15, # modèle de A2 b_L1_A2 = 0.20, # modèle de A2 b_L3_A2 = 0.20, # modèle de A2 b_Y = 0.10, # modèle de Y b_L1_Y = 0.02, # modèle de Y b_L2_Y = 0.02, # modèle de Y b_L3_Y = -0.02, # modèle de Y b_A1_Y = 0.3, # modèle de Y b_A2_Y = 0.1, # modèle de Y b_A1A2_Y = 0.4 ) { # &lt;- effet d&#39;interaction Delta) # coefficients pour simuler l&#39;exposition # exposition A1 # vérif try(if(b_A1 + b_L1_A1 + b_L1_A1 &gt; 1) stop(&quot;la somme des coefficient du modèle A1 dépasse 100%&quot;)) # exposition A2 # vérif try(if(b_A2 + b_L1_A2 + b_L3_A2 &gt; 1) stop(&quot;la somme des coefficients du modèle A2 dépasse 100%&quot;)) # coefficients pour simuler l&#39;outcome, vérif try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &gt; 1) stop(&quot;la somme des coefficients du modèle Y dépasse 100%&quot;)) try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &lt; 0) stop(&quot;la somme des coefficients du modèle Y est inférieure à 0%&quot;)) coef &lt;- list(c(p_L1 = p_L1, p_L2 = p_L2, p_L3 = p_L3), c(b_A1 = b_A1, b_L1_A1 = b_L1_A1, b_L2_A1 = b_L2_A1), c(b_A2 = b_A2, b_L1_A2 = b_L1_A2, b_L3_A2 = b_L3_A2), c(b_Y = b_Y, b_L1_Y = b_L1_Y, b_L2_Y = b_L2_Y, b_L3_Y = b_L3_Y, b_A1_Y = b_A1_Y, b_A2_Y = b_A2_Y, b_A1A2_Y = b_A1A2_Y)) return(coef) } generate.data &lt;- function(N, b = param.causal.model()) { L1 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L1&quot;]) L2 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L2&quot;]) L3 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L3&quot;]) A1 &lt;- rbinom(N, size = 1, prob = b[[2]][&quot;b_A1&quot;] + (b[[2]][&quot;b_L1_A1&quot;] * L1) + (b[[2]][&quot;b_L2_A1&quot;] * L2)) A2 &lt;- rbinom(N, size = 1, prob = b[[3]][&quot;b_A2&quot;] + (b[[3]][&quot;b_L1_A2&quot;] * L1) + (b[[3]][&quot;b_L3_A2&quot;] * L3)) Y &lt;- rbinom(N, size = 1, prob = (b[[4]][&quot;b_Y&quot;] + (b[[4]][&quot;b_L1_Y&quot;] * L1) + (b[[4]][&quot;b_L2_Y&quot;] * L2) + (b[[4]][&quot;b_L3_Y&quot;] * L3) + (b[[4]][&quot;b_A1_Y&quot;] * A1) + (b[[4]][&quot;b_A2_Y&quot;] * A2) + (b[[4]][&quot;b_A1A2_Y&quot;] * A1 * A2)) ) data.sim &lt;- data.frame(L1, L2, L3, A1, A2, Y) return(data.sim) } #### On simule une base de données set.seed(12345) # b = param.causal.model(b_A1A2_Y = -0.45) b = param.causal.model() df &lt;- generate.data(N = 10000, b = b) summary(df) prop.table(table(df$Y, df$A1, df$A2, deparse.level = 2)) Au final, les probabilités de l’outcome P(Y=1), dans chaque catégorie sont : A2 label levels value 0 A1 0 0.10 (0.30) 0 1 0.41 (0.49) 1 A1 0 0.20 (0.40) 1 1 0.90 (0.30) Les paramètres utilisés pour simuler les données ont été choisis de sorte que les “vraies” valeurs des paramètres de la distribution correspondent au tableau présenté au paragraphe 5 “Mesure des interactions”. "],["regression.html", "Chapitre 9 A partir de modèles de régression 9.1 Régression logistique 9.2 Régression lineaire", " Chapitre 9 A partir de modèles de régression Dans une première étape exploratoire, on peut simplement utiliser les modèles de régression habituels : les modèles de régression logistique et linéaire. 9.1 Régression logistique Lorsque l’on étudie un outcome binaire, on utilise souvent les modèles de régression logistique. ## ## Call: ## glm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## family = binomial, data = df_f) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2419 -0.6580 -0.4678 -0.4341 2.1949 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.16540 0.06708 -32.281 &lt; 2e-16 *** ## as.factor(A1)1 1.75607 0.07604 23.093 &lt; 2e-16 *** ## as.factor(A2)1 0.75332 0.06831 11.028 &lt; 2e-16 *** ## as.factor(L1)1 0.15753 0.05702 2.763 0.00573 ** ## as.factor(L2)1 0.14128 0.06878 2.054 0.03996 * ## as.factor(L3)1 -0.14926 0.06141 -2.431 0.01507 * ## as.factor(A1)1:as.factor(A2)1 1.78587 0.14131 12.638 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 11037.7 on 9999 degrees of freedom ## Residual deviance: 8460.4 on 9993 degrees of freedom ## AIC: 8474.4 ## ## Number of Fisher Scoring iterations: 4 A partir de cette sortie, on peut extraire : A1|A2=0 à partir du coefficient as.factor(A1)1 qui correspond à l’effet de A1 dans la catégorie de référence de A2, soit \\(\\small OR_{A1|A2=0} = exp(1.756) =\\) 5.789. A1|A2=1 à partir du coefficient as.factor(A1)1:as.factor(A2)1, qui correspond à la différence d’effet de A1 quand on passe dans l’autre catégorie de A2. L’effet de A1 dans la catégorie A2=1 est donc \\(\\small OR_{A1|A2=1} = exp(1.756+1.786) =\\) 34.536. L’interaction multiplicative (IM) peut être estimée à partir du coefficient as.factor(A1)1:as.factor(A2)1 par \\(\\small IM = exp(1.786) =\\) 5.966, qu’on peut retrouver en faisant \\(\\small OR_{A1|A2=1}/ OR_{A1|A2=0}\\). Ici l’interaction est significative (p-value &gt;0.05). A2|A1=0 et A2|A1=1 On aurait aussi pu décrire l’interaction à partir de l’effet d’A2 dans chaque strate de A1 à partir de as.factor(A2)1 et as.factor(A1)1:as.factor(A2)1, avec : \\(\\small OR_{A2|A1=0} = exp(0.753) =\\) 2.123 et \\(\\small OR_{A2|A1=1} = exp(0.753+1.786) =\\) 12.667 L’interaction additive On peut explorer l’interaction sur l’échelle additive en estimant le RERI par \\(\\small RERI \\approx OR_{11} - OR_{10} - OR_{01} + 1 =\\) \\(\\small OR_{A1,A2} - OR_{A1|A2=0} - OR_{A2|A1=0} + 1 =\\) \\(\\small exp(1.786+0.753+1.786) - exp(1.786) - exp(0.753) + 1 =\\) 68.477. En résumé, (le package finalfit permet de sortir quelques résultats proprement) : explanatory = c(&quot;as.factor(A1)&quot;, &quot;as.factor(A2)&quot;, &quot;as.factor(A1)*as.factor(A2)&quot;, &quot;as.factor(L1)&quot;, &quot;as.factor(L2)&quot;, &quot;as.factor(L3)&quot;) dependent = &quot;Y&quot; df_f %&gt;% finalfit(dependent, explanatory)-&gt; t # le tableau t entier peut être imprimé, mais ici je sélectionne seulement les effets d&#39;intéret # pour éviter la table 2 fallacy (les coefficient des facteurs de confusion L ne sont pas interprétables) cbind(names = c(&quot;A1|A2=0&quot;, &quot;A2|A1=0&quot;, &quot;Interaction&quot;), OR = t[c(12,14,13),6]) %&gt;% as.data.frame %&gt;% kbl() %&gt;% kable_classic() names OR A1|A2=0 5.79 (4.99-6.72, p&lt;0.001) A2|A1=0 2.12 (1.86-2.43, p&lt;0.001) Interaction 5.96 (4.54-7.90, p&lt;0.001) Attention, les modèles de régressions logistiques sont ici biaisés car les données sont générées à partir de modèles additifs. 9.2 Régression lineaire Même si l’outcome binaire, on peut en théorie utiliser un modèle de régression linéaire et explorer les effets sur une échelle additive. Si l’outcome est quantitatif, on utilise aussi, en général, les modèles de régression linéaire. ## ## Call: ## lm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93110 -0.19602 -0.10494 -0.08426 0.91574 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.103835 0.008146 12.746 &lt; 2e-16 *** ## as.factor(A1)1 0.300796 0.011592 25.948 &lt; 2e-16 *** ## as.factor(A2)1 0.092280 0.008671 10.642 &lt; 2e-16 *** ## as.factor(L1)1 0.020677 0.007495 2.759 0.00581 ** ## as.factor(L2)1 0.019476 0.009410 2.070 0.03851 * ## as.factor(L3)1 -0.019574 0.008085 -2.421 0.01549 * ## as.factor(A1)1:as.factor(A2)1 0.394034 0.017854 22.070 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3615 on 9993 degrees of freedom ## Multiple R-squared: 0.2856, Adjusted R-squared: 0.2852 ## F-statistic: 665.8 on 6 and 9993 DF, p-value: &lt; 2.2e-16 A partir de cette sortie, on peut extraire : A1|A2=0 à partir du coefficient as.factor(A1)1 qui correspond à l’effet de A1 dans la catégorie de référence de A2, soit \\(\\small DR = +30,08 \\%\\). A1|A2=1 à partir du coefficient as.factor(A1)1:as.factor(A2)1, qui correspond à la différence d’effet de A1 quand on passe dans l’autre catégorie de A2. L’effet de A1 dans la catégorie A2=1 est donc \\(\\small DR = 30.08+39.40 =\\) 69.48 %. L’interaction additive à partir du coefficient as.factor(A1)1:as.factor(A2)1 avec \\(\\small AI = +39.40 \\%\\), qu’on peut retrouver en faisant \\(\\small DR(A1|A2=1) - DR(A1|A2=0)\\). Ici l’interaction est significative (p-value &gt;0.05). A2|A1=0 et A2|A1=1 On aurait aussi pu décrire cette interaction à partir de l’effet d’A2 dans chaque strate de A1 à partir de as.factor(A2)1 et as.factor(A1)1:as.factor(A2)1, avec : \\(\\small DR_{A1|A2=0} = +9.23\\%\\) et \\(\\small DR_{A1|A2=1} = 9.23+39.40 =\\) 48.63%. En résumé, (le package finalfit permet de sortir quelques résultats proprement) : explanatory = c(&quot;as.factor(A1)&quot;, &quot;as.factor(A2)&quot;, &quot;as.factor(A1)*as.factor(A2)&quot;, &quot;as.factor(L1)&quot;, &quot;as.factor(L2)&quot;, &quot;as.factor(L3)&quot;) dependent = &quot;Y&quot; df %&gt;% finalfit(dependent, explanatory)-&gt; t cbind(names = c(&quot;A1|A2=0&quot;, &quot;A2|A1=0&quot;, &quot;Interaction&quot;), DR = t[c(12,14,13),6]) %&gt;% as.data.frame %&gt;% kbl() %&gt;% kable_classic() names DR A1|A2=0 0.30 (0.28 to 0.32, p&lt;0.001) A2|A1=0 0.09 (0.08 to 0.11, p&lt;0.001) Interaction 0.39 (0.36 to 0.43, p&lt;0.001) "],["conf.html", "Chapitre 10 Approches causales 10.1 Estimation par G-computation 10.2 Estimation par Modèle Structurel Marginal 10.3 Estimation avec TMLE", " Chapitre 10 Approches causales 10.1 Estimation par G-computation Il s’agit d’une “G-méthode” qui peut être décrite comme une “standardisation” par régression (Hernàn [12]). Le principe est le suivant : Modéliser le critère de jugement en fonction des deux expositions d’intérêt, de leur interaction et des facteurs de confusion \\[\\begin{equation*} \\overline{Q}(A_1,A_2,L) = \\mathbb{E}(Y | A_1, A_2, L_1,L_2,L_3) \\end{equation*}\\] A partir de ce modèle \\(\\overline{Q}\\), estimer pour chaque individu \\(i\\) les valeurs moyennes attendues (contrefactuelles) \\(\\overline{Q}_i(0,0,L)\\), \\(\\overline{Q}_i(1,0,L)\\), \\(\\overline{Q}_i(0,1,L)\\) et \\(\\overline{Q}_i(1,1,L)\\), sous les 4 scénarios possibles \\(\\{A_1 = 0, A_2 = 0\\}\\), \\(\\{A_1 = 1, A_2 = 0\\}\\), \\(\\{A_1 = 0, A_2 = 1\\}\\) et \\(\\{A_1 = 1, A_2 = 1\\}\\). Utiliser les moyennes contrefactuelles estimées pour calculer les différents indicateurs d’intérêt pour l’analyse d’interaction : Les moyennes marginales pour chaque case du tableau d’interaction \\[\\begin{align*} p_{00} &amp;= \\frac{1}{n} \\sum \\overline{Q}_i(0,0,L) \\\\ p_{10} &amp;= \\frac{1}{n} \\sum \\overline{Q}_i(1,0,L) \\\\ p_{01} &amp;= \\frac{1}{n} \\sum \\overline{Q}_i(0,1,L) \\\\ p_{11} &amp;= \\frac{1}{n} \\sum \\overline{Q}_i(1,1,L) \\end{align*}\\] Les différences de risques marginales \\[\\begin{align*} DR(A_1, A_2 = 0) &amp;= p_{10} - p_{00} \\\\ DR(A_1, A_2 = 1) &amp;= p_{11} - p_{01} \\\\ DR(A_2, A_1 = 0) &amp;= p_{01} - p_{00} \\\\ DR(A_2, A_1 = 1) &amp;= p_{11} - p_{10} \\end{align*}\\] Les risques relatifs marginaux \\[\\begin{align*} RR(A_1, A_2 = 0) &amp;= p_{10} / p_{00} \\\\ RR(A_1, A_2 = 1) &amp;= p_{11} / p_{01} \\\\ RR(A_2, A_1 = 0) &amp;= p_{01} / p_{00} \\\\ RR(A_2, A_1 = 1) &amp;= p_{11} / p_{10} \\end{align*}\\] Les mesures d’interaction \\[\\begin{align*} MI &amp;= \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}} \\\\ AI &amp;= p_{11} - p_{10} - p_{01} + p_{00} \\\\ RERI &amp;= \\frac{p_{11} - p_{10} - p_{01} + p_{00}}{p_{00}} \\end{align*}\\] ## i) Modéliser le critère de jugement en fonction des deux expositions d&#39;intérêt, ## de leur interaction et des facteurs de confusion model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;binomial&quot;) # dans cette exemple, les données étaient simulées à partir d&#39;un modèle additif, # le modèle plus adapté serait donc plutôt le suivant : # model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, # family = &quot;gaussian&quot;) # (en pratique, on ne connaît pas la nature du modèle générateur des données) ## ii) Estimer les valeurs attendues sous les 4 scenarios contrefactuels ## ii.a) on crée 4 bases de données correspondant aux 4 scenarios contrefactuels df.A1_0.A2_0 &lt;- df.A1_1.A2_0 &lt;- df.A1_0.A2_1 &lt;- df.A1_1.A2_1 &lt;- df # scénario do(A1 = 0, A2 = 0) pour toute la population df.A1_0.A2_0$A1 &lt;- df.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) # scénario do(A1 = 1, A2 = 0) pour toute la population df.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) df.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) # scénario do(A1 = 0, A2 = 1) pour toute la population df.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) # scénario do(A1 = 1, A2 = 1) pour toute la population df.A1_1.A2_1$A1 &lt;- df.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) ## ii.b) on prédit le critère de jugement sous les scenarios contrefactuels Qbar_00 &lt;- predict(model.Y, newdata = df.A1_0.A2_0, type = &quot;response&quot;) Qbar_10 &lt;- predict(model.Y, newdata = df.A1_1.A2_0, type = &quot;response&quot;) Qbar_01 &lt;- predict(model.Y, newdata = df.A1_0.A2_1, type = &quot;response&quot;) Qbar_11 &lt;- predict(model.Y, newdata = df.A1_1.A2_1, type = &quot;response&quot;) ## iii) Utiliser les moyennes contrefactuelles estimées pour calculer les différents ## indicateurs d&#39;intérêt pour l&#39;analyse d&#39;interaction ## iii.a) on va enregistrer l&#39;ensemble des résultats pertinents dans une table &#39;int.r&#39; ## de longueur 2 x 2 int.r &lt;- matrix(NA, ncol = 26, nrow = nlevels(as.factor(df$A1)) * nlevels(as.factor(df$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;,&quot;RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;,&quot;RR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) ## iii.b) Les moyennes marginales pour chaque case du tableau d’interaction # dans chaque case de la table 2 x 2 # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- mean(Qbar_00) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Qbar_10) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Qbar_01) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) ## iii.c) Les différences de risques marginales # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Qbar_10) - mean(Qbar_00) # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) - mean(Qbar_01) # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Qbar_01) - mean(Qbar_00) # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) - mean(Qbar_10) ## iii.d) Les risques relatifs marginaux # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Qbar_10) / mean(Qbar_00) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) / mean(Qbar_01) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Qbar_01) / mean(Qbar_00) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) / mean(Qbar_10) ## iii.e) Les mesures d&#39;interaction # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Qbar_11) - mean(Qbar_10) - mean(Qbar_01) + mean(Qbar_00) # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Qbar_11) - mean(Qbar_10) - mean(Qbar_01) + mean(Qbar_00)) / mean(Qbar_00) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Qbar_11) * mean(Qbar_00)) / (mean(Qbar_10) * mean(Qbar_01)) ## iii.f) Calcul des intervalles de confiance # Le script ci-dessous présente une méthode de bootstrap # pour calculer les intervalles de confiance à 95% des différents indicateurs set.seed(5678) B &lt;- 1000 # on utilise 1000 échantillons boostrap dans cet exemple bootstrap.est &lt;- data.frame(matrix(NA, nrow = B, ncol = 15)) colnames(bootstrap.est) &lt;- c(&quot;p.A1is0.A2is0&quot;, &quot;p.A1is1.A2is0&quot;, &quot;p.A1is0.A2is1&quot;, &quot;p.A1is1.A2is1&quot;, &quot;RD.A1.A2is0&quot;, &quot;RD.A1.A2is1&quot;, &quot;RD.A2.A1is0&quot;, &quot;RD.A2.A1is1&quot;, &quot;lnRR.A1.A2is0&quot;, &quot;lnRR.A1.A2is1&quot;, &quot;lnRR.A2.A1is0&quot;, &quot;lnRR.A2.A1is1&quot;, &quot;INT.a&quot;, &quot;lnRERI&quot;, &quot;lnINT.m&quot;) for (b in 1:B){ # on refait toute l&#39;analyse dans chaque échantillon bootstrap # constituer un échantillon bootstrap (échantillonnage avec remise) bootIndices &lt;- sample(1:nrow(df), replace=T) bootData &lt;- df[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) # étape (i) de modélisation du critère de jugement model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = bootData, # use BootData here +++ family = &quot;binomial&quot;) # étape (ii) d&#39;estimation des valeurs attendus sous les 4 scénarios contrefactuels boot.A1_0.A2_0 &lt;- boot.A1_1.A2_0 &lt;- boot.A1_0.A2_1 &lt;- boot.A1_1.A2_1 &lt;- bootData boot.A1_0.A2_0$A1 &lt;- boot.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) boot.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) boot.A1_1.A2_1$A1 &lt;- boot.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) # prédire les résultats contrefactuels sous les différents scénarios Qbar_00 &lt;- predict(model.Y, newdata = boot.A1_0.A2_0, type = &quot;response&quot;) Qbar_10 &lt;- predict(model.Y, newdata = boot.A1_1.A2_0, type = &quot;response&quot;) Qbar_01 &lt;- predict(model.Y, newdata = boot.A1_0.A2_1, type = &quot;response&quot;) Qbar_11 &lt;- predict(model.Y, newdata = boot.A1_1.A2_1, type = &quot;response&quot;) # étape (iii) de calcul des différents indicateurs d&#39;intérêt # on sauve les résultats dans la table bootstrap.est bootstrap.est[b,&quot;p.A1is0.A2is0&quot;] &lt;- mean(Qbar_00) bootstrap.est[b,&quot;p.A1is1.A2is0&quot;] &lt;- mean(Qbar_10) bootstrap.est[b,&quot;p.A1is0.A2is1&quot;] &lt;- mean(Qbar_01) bootstrap.est[b,&quot;p.A1is1.A2is1&quot;] &lt;- mean(Qbar_11) bootstrap.est[b,&quot;RD.A1.A2is0&quot;] &lt;- mean(Qbar_10) - mean(Qbar_00) bootstrap.est[b,&quot;RD.A1.A2is1&quot;] &lt;- mean(Qbar_11) - mean(Qbar_01) bootstrap.est[b,&quot;RD.A2.A1is0&quot;] &lt;- mean(Qbar_01) - mean(Qbar_00) bootstrap.est[b,&quot;RD.A2.A1is1&quot;] &lt;- mean(Qbar_11) - mean(Qbar_10) # les Ic95% des RR sont d&#39;abord calculés sur une échelle logarithmique bootstrap.est[b,&quot;lnRR.A1.A2is0&quot;] &lt;- log(mean(Qbar_10) / mean(Qbar_00)) bootstrap.est[b,&quot;lnRR.A1.A2is1&quot;] &lt;- log(mean(Qbar_11) / mean(Qbar_01)) bootstrap.est[b,&quot;lnRR.A2.A1is0&quot;] &lt;- log(mean(Qbar_01) / mean(Qbar_00)) bootstrap.est[b,&quot;lnRR.A2.A1is1&quot;] &lt;- log(mean(Qbar_11) / mean(Qbar_10)) bootstrap.est[b,&quot;INT.a&quot;] &lt;- (mean(Qbar_11) - mean(Qbar_10) - mean(Qbar_01) + mean(Qbar_00)) # les IC95% du RERI et de l&#39;interaction multiplicative sont d&#39;abord calculés # sur une échelle logarithmique bootstrap.est[b,&quot;lnRERI&quot;] &lt;- log((mean(Qbar_11) - mean(Qbar_10) - mean(Qbar_01) + mean(Qbar_00)) / mean(Qbar_00)) bootstrap.est[b,&quot;lnINT.m&quot;] &lt;- log((mean(Qbar_11) * mean(Qbar_00)) / (mean(Qbar_10) * mean(Qbar_01))) } ## On peut vérifier la normalité des distributions bootstrap avec le script ## proposé en commentaire ci-dessous : # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * # exp(-1/2 * ((x-mean(bootstrap.est[,c])) / sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } # Les distributions ont bien une allure normale, on peut donc utiliser la # déviation standard des distributions pour calculer les IC95%. # En cas de distributions asymétriques, il serait préférable d&#39;utiliser les # percentiles 2.5% et 97.5% (ou un méthode bootstrap BCa) ## on calcule les IC95% et on les intègre dans la table de synthèse &#39;int.r&#39; # A1 = 0 et A2 = 0 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) # A1 = 1 et A2 = 0 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) # A1 = 0 et A2 = 1 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) # A1 = 1 et A2 = 1 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) # risk difference # RD.A1.A2is0 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- (int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0)) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- (int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0)) # RD.A1.A2is1 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1)) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1)) # RD.A2.A1is0 int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0)) int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0)) # RD.A2.A1is1 int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1)) int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1)) # relative risk # RR.A1.A2is0 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) # RR.A1.A2is1 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) # RR.A2.A1is0 int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) # RR.A2.A1is1 int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) # additive interaction int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$INT.a)) int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$INT.a)) # RERI int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRERI)) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRERI)) # multiplicative interaction int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnINT.m)) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnINT.m)) Au final, on a : ## On peut présenter les différents résultats dans un tableau des effets marginaux out.table &lt;- data.frame(c1 = rep(&quot;&quot;,4), c2 = rep(&quot;&quot;,4), c3 = rep(&quot;&quot;,4), c4 = rep(&quot;&quot;,4)) names(out.table) &lt;- c(&quot;A2=0&quot;, &quot;A2=1&quot;, &quot;RD.A2|A1&quot;, &quot;RR.A2|A1&quot;) rownames(out.table) &lt;- c(&quot;A1=0&quot;, &quot;A1=1&quot;, &quot;RD.A1|A2&quot;, &quot;RR.A1|A2&quot;) # p out.table[&quot;A1=0&quot;,&quot;A2=0&quot;] &lt;- paste0(&quot;$p_{00}$=&quot;, round(int.r$p[which(int.r$A1==0 &amp; int.r$A2==0)], digits = 3), &quot; [&quot;, round(int.r$p.lo[which(int.r$A1==0 &amp; int.r$A2==0)], digits = 3), &quot;,&quot;, round(int.r$p.up[which(int.r$A1==0 &amp; int.r$A2==0)], digits = 3), &quot;]&quot;) out.table[&quot;A1=0&quot;,&quot;A2=1&quot;] &lt;- paste0(&quot;$p_{01}$=&quot;,round(int.r$p[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$p.lo[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot;,&quot;, round(int.r$p.up[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;) out.table[&quot;A1=1&quot;,&quot;A2=0&quot;] &lt;- paste0(&quot;$p_{10}$=&quot;,round(int.r$p[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot; [&quot;, round(int.r$p.lo[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot;,&quot;, round(int.r$p.up[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot;]&quot;) out.table[&quot;A1=1&quot;,&quot;A2=1&quot;] &lt;- paste0(&quot;$p_{11}$=&quot;,round(int.r$p[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$p.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;,&quot;, round(int.r$p.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;) # RD out.table[&quot;A1=0&quot;,&quot;RD.A2|A1&quot;] &lt;- paste0(round(int.r$RD.A2[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$RD.A2.lo[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot;,&quot;, round(int.r$RD.A2.up[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;) out.table[&quot;A1=1&quot;,&quot;RD.A2|A1&quot;] &lt;- paste0(round(int.r$RD.A2[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$RD.A2.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;,&quot;, round(int.r$RD.A2.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;) out.table[&quot;RD.A1|A2&quot;,&quot;A2=0&quot;] &lt;- paste0(round(int.r$RD.A1[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot; [&quot;, round(int.r$RD.A1.lo[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot;,&quot;, round(int.r$RD.A1.up[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 3), &quot;]&quot;) out.table[&quot;RD.A1|A2&quot;,&quot;A2=1&quot;] &lt;- paste0(round(int.r$RD.A1[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$RD.A1.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;,&quot;, round(int.r$RD.A1.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;) # RR out.table[&quot;A1=0&quot;,&quot;RR.A2|A1&quot;] &lt;- paste0(round(int.r$RR.A2[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 2), &quot; [&quot;, round(int.r$RR.A2.lo[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 2), &quot;,&quot;, round(int.r$RR.A2.up[which(int.r$A1==0 &amp; int.r$A2==1)], digits = 2), &quot;]&quot;) out.table[&quot;A1=1&quot;,&quot;RR.A2|A1&quot;] &lt;- paste0(round(int.r$RR.A2[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot; [&quot;, round(int.r$RR.A2.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;,&quot;, round(int.r$RR.A2.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;]&quot;) out.table[&quot;RR.A1|A2&quot;,&quot;A2=0&quot;] &lt;- paste0(round(int.r$RR.A1[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 2), &quot; [&quot;, round(int.r$RR.A1.lo[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 2), &quot;,&quot;, round(int.r$RR.A1.up[which(int.r$A1==1 &amp; int.r$A2==0)], digits = 2), &quot;]&quot;) out.table[&quot;RR.A1|A2&quot;,&quot;A2=1&quot;] &lt;- paste0(round(int.r$RR.A1[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot; [&quot;, round(int.r$RR.A1.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;,&quot;, round(int.r$RR.A1.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;]&quot;) interaction.effects &lt;- c(paste0(&quot;additive Interaction = &quot;, round(int.r$a.INT[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot; [&quot;, round(int.r$a.INT.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;;&quot;, round(int.r$a.INT.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 3), &quot;]&quot;), paste0(&quot;RERI = &quot;, round(int.r$RERI[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot; [&quot;, round(int.r$RERI.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;;&quot;, round(int.r$RERI.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;]&quot;), paste0(&quot;multiplicative Interaction = &quot;, round(int.r$m.INT[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot; [&quot;, round(int.r$m.INT.lo[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;;&quot;, round(int.r$m.INT.up[which(int.r$A1==1 &amp; int.r$A2==1)], digits = 2), &quot;]&quot;)) library(kableExtra) kbl(out.table) %&gt;% kable_classic() %&gt;% footnote(general = interaction.effects) A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.094,0.113] \\(p_{01}\\)=0.197 [0.182,0.212] 0.093 [0.076,0.11] 1.9 [1.7,2.13] A1=1 \\(p_{10}\\)=0.4 [0.373,0.427] \\(p_{11}\\)=0.893 [0.872,0.915] 0.494 [0.46,0.527] 2.23 [2.08,2.4] RD.A1|A2 0.296 [0.268,0.325] 0.697 [0.67,0.723] RR.A1|A2 3.86 [3.44,4.33] 4.54 [4.19,4.91] Note: additive Interaction = 0.4 [0.362;0.438] RERI = 3.86 [3.45;4.33] multiplicative Interaction = 1.18 [1.03;1.34] 10.2 Estimation par Modèle Structurel Marginal # On récupère les Y prédit précédents, que l&#39;on fusionne Y &lt;- c(Y.A1_0.A2_0, Y.A1_1.A2_0, Y.A1_0.A2_1, Y.A1_1.A2_1) length(Y) # on aura une base de données de 40000 lignes # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(df.A1_0.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_0.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;))) # dim(X) ## Modèle structurel marginal msm.RD &lt;- glm(Y ~ A1 + A2 + A1:A2, data = data.frame(Y,X), family = &quot;gaussian&quot;) # ne pas ajuster sur les facteurs de confusion msm.RD ## tableau des effets marignaux results.MSM &lt;- matrix(NA, ncol = 4, nrow = 4) colnames(results.MSM) &lt;- c(&quot;A2 = 0&quot;, &quot;A2 = 1&quot;, &quot;RD within strata of A1&quot;, &quot;RR within strata of A1&quot;) rownames(results.MSM) &lt;- c(&quot;A1 = 0&quot;, &quot;A1 = 1&quot;, &quot;RD within strata of A2&quot;, &quot;RR within strata of A2&quot;) # 4 risques marginaux results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A1&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] + msm.RD$coefficients[&quot;A1&quot;] + msm.RD$coefficients[&quot;A1:A2&quot;] # within strata of A2 results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] # within strata of A1 results.MSM[&quot;A1 = 0&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 0&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM &lt;- round(results.MSM,3) RD.interaction &lt;- msm.RD$coefficients[&quot;A1:A2&quot;] RR.interaction &lt;- (results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;]) / ( results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] ) Au final, on a (sans les IC): A2 = 0 A2 = 1 RD within strata of A1 RR within strata of A1 A1 = 0 0.107 0.198 0.091 1.851 A1 = 1 0.411 0.889 0.478 2.164 RD within strata of A2 0.303 0.690 NA NA RR within strata of A2 3.834 4.483 NA NA Note: additive Interaction = 0.387 multiplicative Interaction = 1.17 10.3 Estimation avec TMLE ## 3- int.ltmleMSM() pour estimer les différentes quantités d&#39;intérêt, ### par gcomputation, IPTW ou tmle int.ltmleMSM &lt;- function(data = data, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, final.Ynodes = final.Ynodes, SL.library = list(Q=&quot;SL.glm&quot;, g=&quot;SL.glm&quot;), gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 2000, boot.seed = 12345) { # regime= # binary array: n x numAnodes x numRegimes of counterfactual treatment or a list of &#39;rule&#39; functions regimes.MSM &lt;- array(NA, dim = c(nrow(data), 2, 4)) # 2 variables d&#39;exposition (A1, A2), 4 régimes d&#39;exposition (0,0) (1,0) (0,1) (1,1) regimes.MSM[,,1] &lt;- matrix(c(0,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé ni à A1, ni à A2 regimes.MSM[,,2] &lt;- matrix(c(1,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 uniquement regimes.MSM[,,3] &lt;- matrix(c(0,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A2 uniquement regimes.MSM[,,4] &lt;- matrix(c(1,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 et à A2 # summary.measures = valeurs des coefficients du MSM associés à chaque régime # array: num.regimes x num.summary.measures x num.final.Ynodes - # measures summarizing the regimes that will be used on the right hand side of working.msm # (baseline covariates may also be used in the right hand side of working.msm and do not need to be included in summary.measures) summary.measures.reg &lt;- array(NA, dim = c(4, 3, 1)) summary.measures.reg[,,1] &lt;- matrix(c(0, 0, 0, # aucun effet ni de A1, ni de A2 1, 0, 0, # effet de A1 isolé 0, 1, 0, # effet de A2 isolé 1, 1, 1), # effet de A1 + A2 + A1:A2 ncol = 3, nrow = 4, byrow = TRUE) colnames(summary.measures.reg) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A1:A2&quot;) if(gcomp == TRUE) { # test length SL.library$Q SL.library$Q &lt;- ifelse(length(SL.library$Q) &gt; 1, &quot;SL.glm&quot;, SL.library$Q) # simplify SL.library$g because g functions are useless with g-computation SL.library$g &lt;- &quot;SL.mean&quot; iptw.only &lt;- FALSE } ltmle_MSM &lt;- ltmleMSM(data = data, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res &lt;- data.frame(&quot;beta.Intercept&quot; = rep(NA, B), &quot;beta.A1&quot; = rep(NA, B), &quot;beta.A2&quot; = rep(NA, B), &quot;beta.A1A2&quot; = rep(NA, B)) if(gcomp == TRUE) { set.seed &lt;- boot.seed for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) bootData &lt;- data[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) boot_ltmle_MSM &lt;- ltmleMSM(data = bootData, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res$beta.Intercept[b] &lt;- boot_ltmle_MSM$beta[&quot;(Intercept)&quot;] bootstrap.res$beta.A1[b] &lt;- boot_ltmle_MSM$beta[&quot;A1&quot;] bootstrap.res$beta.A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A2&quot;] bootstrap.res$beta.A1A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A1:A2&quot;] } } return(list(ltmle_MSM = ltmle_MSM, bootstrap.res = bootstrap.res)) } ### 4- summary.int() pour enregistrer l&#39;ensemble des estimations summary.int &lt;- function(data = data, ltmle_MSM = ltmle_MSM, estimator = c(&quot;gcomp&quot;, &quot;iptw&quot;, &quot;tmle&quot;)) { if(estimator == &quot;gcomp&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == FALSE) stop(&quot;The ltmle function did not use the gcomp estimator, but the iptw +/- tmle estimator&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta } if(estimator == &quot;iptw&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, iptw is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta.iptw IC &lt;- ltmle_MSM$ltmle_MSM$IC.iptw } if(estimator == &quot;tmle&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, tmle is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta IC &lt;- ltmle_MSM$ltmle_MSM$IC } # on va enregitrer l&#39;ensemble des résultats pertinent dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 34, nrow = nlevels(as.factor(data$A1)) * nlevels(as.factor(data$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;sd.p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;sd.RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;, &quot;RD.A2&quot;,&quot;sd.RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;sd.lnRR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;, &quot;RR.A2&quot;,&quot;sd.lnRR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;sd.a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;sd.lnRERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;sd.ln.m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # on peut retrouver les IC95% par delta method # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;]) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;]) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A2&quot;]) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;] + beta[&quot;A2&quot;] + beta[&quot;A1:A2&quot;]) # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) ## IC95% if(estimator == &quot;iptw&quot; | estimator == &quot;tmle&quot;) { # A1 = 0 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]),0,0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]),0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 grad &lt;- rep(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), 4) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0, 1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI grad &lt;- c((int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } if(estimator == &quot;gcomp&quot;) { ltmle_MSM$bootstrap.res$p.A1_0.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept) ltmle_MSM$bootstrap.res$p.A1_1.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1) ltmle_MSM$bootstrap.res$p.A1_0.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A2) ltmle_MSM$bootstrap.res$p.A1_1.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1 + ltmle_MSM$bootstrap.res$beta.A2 + ltmle_MSM$bootstrap.res$beta.A1A2) ltmle_MSM$bootstrap.res$RD.A1.A2_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A1.A2_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 ltmle_MSM$bootstrap.res$RD.A2.A1_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_0.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A2.A1_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 ltmle_MSM$bootstrap.res$lnRR.A1.A2_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_0 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A1.A2_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_1) ltmle_MSM$bootstrap.res$lnRR.A2.A1_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_0.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A2.A1_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_1.A2_0) ltmle_MSM$bootstrap.res$a.INT &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$lnRERI &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$ln.m.INT &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 * ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / (ltmle_MSM$bootstrap.res$p.A1_1.A2_0 * ltmle_MSM$bootstrap.res$p.A1_0.A2_1)) # A1 = 0 et A2 = 0 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_0) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_0) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_1) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_1) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_0) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_1) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_0) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_1) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_0) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_1) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_0) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_1) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$a.INT) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRERI) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$ln.m.INT) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } return(list(int.r = int.r, bootstrap.res = bootstrap.res)) } ### Obtention du MSM par la fonction ltmle, estimation par gcomp, iptw ou tmle # avec la fonction int.ltmleMSM() # on définit les arguments de la fonction ltmleMSM du package ltmle library(ltmle) library(SuperLearner) ## arguments à renseigner Q_formulas = c(Y=&quot;Q.kplus1 ~ L1 + L2 + L3 + A1 * A2&quot;) # useful to add A1 * A2 interaction here g_formulas = c(&quot;A1 ~ L1 + L2&quot;, &quot;A2 ~ L1 + L3&quot;) SL.library = list(Q=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;), g=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;)) ### estimation par IPTW et TMLE interaction.ltmle &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, SL.library = SL.library, gcomp = FALSE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;) ## Loading required namespace: xgboost ### estimation par g-computation # par défaut, il fait une régression logistique à partir de la formule Q_formulas # si on veut faire un régression linéaire pour le modèle additif, on peut créer une fonction de SuperLearner # à partir de la fonction SL.glm SL.glm.gaussian &lt;- function (Y, X, newX, family = &quot;gaussian&quot;, # tout est comme SL.glm, sauf cette famille &quot;gaussian&quot; obsWeights, model = TRUE, ...) { if (is.matrix(X)) { X = as.data.frame(X) } fit.glm &lt;- glm(Y ~ ., data = X, family = family, weights = obsWeights, model = model) if (is.matrix(newX)) { newX = as.data.frame(newX) } pred &lt;- predict(fit.glm, newdata = newX, type = &quot;response&quot;) fit &lt;- list(object = fit.glm) class(fit) &lt;- &quot;SL.glm&quot; out &lt;- list(pred = pred, fit = fit) return(out) } environment(SL.glm.gaussian) &lt;-asNamespace(&quot;SuperLearner&quot;) interaction.gcomp &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, # SL.library = SL.library, SL.library = list(Q=&quot;SL.glm.gaussian&quot;, # g=&quot;SL.mean&quot;), gcomp = TRUE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 1000, # nombre d&#39;échantillons bootstrap boot.seed = 54321) # seed pour l&#39;échantillonnage bootstrap ### 3) Calcul des paramètres utiles pour l&#39;analyse de l&#39;interaction # avec la fonction summary.int() ### récupération des résultats tmle summary.tmle &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;tmle&quot;)) # summary.tmle$int.r ### récupération des résultats iptw summary.iptw &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;iptw&quot;)) # summary.iptw$int.r ### récupération des résultats gcomputation summary.gcomp &lt;- summary.int(data = df, ltmle_MSM = interaction.gcomp, estimator = c(&quot;gcomp&quot;)) # summary.gcomp$int.r # head(summary.gcomp$bootstrap.res) # # vérifier la normalité des estimations bootstrap # bootstrap.est &lt;- subset(summary.gcomp$bootstrap.res, # select = # c(&quot;p.A1_0.A2_0&quot;, # &quot;p.A1_1.A2_0&quot;, # &quot;p.A1_0.A2_1&quot;, # &quot;p.A1_1.A2_1&quot;, # &quot;RD.A1.A2_0&quot;, # &quot;RD.A1.A2_1&quot;, # &quot;RD.A2.A1_0&quot;, # &quot;RD.A2.A1_1&quot;, # &quot;lnRR.A1.A2_0&quot;, # &quot;lnRR.A1.A2_1&quot;, # &quot;lnRR.A2.A1_0&quot;, # &quot;lnRR.A2.A1_1&quot;, # &quot;a.INT&quot;, # &quot;lnRERI&quot;, # &quot;ln.m.INT&quot;)) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * exp(-1/2*((x-mean(bootstrap.est[,c]))/sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } Au final, on a (présentation selon recommandation Knol et al. [11]): TMLE ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] ## A1=1 $p_{10}$=0.408 [0.378,0.439] $p_{11}$=0.903 [0.88,0.927] ## RD.A1|A2 0.304 [0.272,0.336] 0.708 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.091 [0.073,0.109] 1.88 [1.67,2.11] ## A1=1 0.495 [0.457,0.534] 2.21 [2.04,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.404 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.89 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; IPTW ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] ## A1=1 $p_{10}$=0.408 [0.377,0.439] $p_{11}$=0.904 [0.88,0.927] ## RD.A1|A2 0.304 [0.272,0.336] 0.709 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.091 [0.073,0.109] 1.88 [1.67,2.11] ## A1=1 0.496 [0.457,0.534] 2.22 [2.05,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.405 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.9 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; G-computation ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.112] $p_{01}$=0.197 [0.183,0.211] ## A1=1 $p_{10}$=0.4 [0.373,0.427] $p_{11}$=0.893 [0.872,0.914] ## RD.A1|A2 0.296 [0.268,0.325] 0.697 [0.671,0.722] ## RR.A1|A2 3.86 [3.46,4.31] 4.54 [4.46,4.61] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.093 [0.077,0.11] 1.9 [1.7,2.12] ## A1=1 0.494 [0.46,0.527] 2.23 [2.08,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.4 [0.363;0.438]&quot; ## [2] &quot;RERI = 3.86 [3.46;4.32]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.03;1.34]&quot; Références "],["graph.html", "Chapitre 11 Représentations graphiques", " Chapitre 11 Représentations graphiques "],["proposition-détapes.html", "Chapitre 12 Proposition d’étapes", " Chapitre 12 Proposition d’étapes Formuler l’objectif Est-ce un objectif prédictif ou explicatif ? Si démarche explicative, s’agit-il plutot d’une analyse d’interaction ou de modification d’effet? Stratégies et méthodes Poser les hypothèses sur un DAG ou schéma conceptuel Identifier le ou les estimand(s), c’est-à-dire l’effet ou le paramètre que l’on va chercher à estimer pour répondre à l’objectif, par exemple : effet conjoint de X et V sur Y, sur l’échelle multiplicative = \\(\\small OR_{X,V}\\) effet de X sur Y dans chaque strate de Y, sur l’échelle additive = \\(\\small DR_{X|V=0}\\) et \\(\\small DR_{X|V=1}\\) effet d’interaction sur l’échelle additive et multiplicative AI et MI Elaborer l’estimateur, notamment : quelles est(sont) l’exposition(s) d’intérêt ? quels sont les facteurs de confusion +/- les médiateurs si besoin ? quels types de modélisation va être utilisée (linéaire, logistique, autre) ? Analyses descriptives Description habituelle de la population Décrire, dans un tableau croisé, le Y moyen ou la proportion de Y = 1 pour chaque catégorie de X et V Analyses exploratoires Analyses stratifiées pour une analyse de modification d’effet, il est possible en exploratoire, d’estimer l’effet de X sur Y de façon stratifiée sur V (on découpe la population) les effets ne seront directement pas comparables Analyses confirmatoires Régressions avec terme d’interaction (voir Chapitre 9) un modèle dans la population totale peut être utilisé avec un terme d’interaction entre X et V les différents paramètres peuvent être déduits des résultats du modèle Approches causales ( voir Chapitre 10) G-computation MSM TMLE "],["exemple-1---y-binaire.html", "Chapitre 13 Exemple 1 - Y binaire 13.1 Formuler les objectifs 13.2 Stratégies et méthodes 13.3 Analyse descriptive 13.4 Analyse exploratoire 13.5 Analyse confirmatoire", " Chapitre 13 Exemple 1 - Y binaire 13.1 Formuler les objectifs Dans cet exemple, on s’intéresse à : Comment l’effet du niveau d’études (X) sur le surpoids/obésité à l’âge adulte (Y) varie en fonction de la défavorisation sociale précoce (D), mesurée par la situation financière du foyer pendant l’enfance. La démarche ici est explicative : on cherche à comprendre des mécanismes causaux. A partir de la formulation des objectifs, on pourrait dire qu’on s’intéresse ici plutôt à une modification d’effets: on analyse l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de défavorisation sociale précoce (D). On ajustera sur les facteurs de confusion de la relation X \\(\\rightarrow\\) Y 13.2 Stratégies et méthodes Le DAG (sans les médiateurs) était : Avec : X, le Niveau d’études : 0 = élevé / 1 = faible (réf) D, la Situation financière pendant l’enfance : 0 = bonne / 1 = difficile (réf) Y, le Surpoids/obésité : 0 = IMC &lt; 25kg/m² / 1 = IMC ≥ 25kg/m² Les estimands étaient définis sur l’échelle multiplicative par : La modification de l’effet du niveau d’études sur le surpoids/obésité en fonction par la défavorisation sociale précoce : \\(\\small (Y_{x=1|d=1} / Y_{x=0|d=1}) / (Y_{x=1|d=0} / Y_{x=0|d=0})\\) Ce qui est équivalent à \\(\\small (Y_{x=1|d=1} \\times Y_{x=0|d=0}) / (Y_{x=1|d=0} \\times Y_{x=0|d=1})\\) L’estimateur : Les effets ont été estimés par g-computation (standardisation par régression) [12]. Des régressions linéaires ont été utilisées pour estimer les potential outcomes pour chaque scénario. A partir des fonctions estimées, nous avons prédit la valeur de l’outcome Y pour chaque individu i pour chaque scénario. Les valeurs moyennes de Y dans chaque scénario vont ensuite nous permettre d’estimer les estimands selon leurs définitions précisées ci-dessus.Ces modèles vont comprendre 4 variables : le niveau d’études et la défavorisation sociale précoce, ainsi que deux facteurs de confusion, le sexe et l’âge. 13.3 Analyse descriptive Dans cette population (N=23 495), il y avait 61.1% d’individus avec un niveau d’études faible et 31.1% de personnes ayant été précocement défavorisées. On peut commencer par décrire les proportions de personnes en surpoids/obésité dans chaque catégorie de niveau d’études et de défavorisation sociale : Niveau d’études Défavorisation % surpoids/obésité Elevé Non 38.4 Elevé Oui 45.2 Faible Non 50.2 Faible Oui 54.6 13.4 Analyse exploratoire La sortie d’un modèle logistique simple serait : # Call: # glm(formula = overw_obesity ~ EDUCATION_2CL.f * CHILDHOOD_ECONOMY_2CL.f + SEX.f + # AGE, family = binomial(link = &quot;logit&quot;)) # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.3731389 0.0621930 -22.079 &lt; 2e-16 *** # EDUCATION_2CL.fHigh -0.1752537 0.0537373 -3.261 0.00111 ** # CHILDHOOD_ECONOMY_2CL.fGood -0.0190075 0.0361206 -0.526 0.59873 # SEX.fMale 0.5882549 0.0270502 21.747 &lt; 2e-16 *** # AGE 0.0234627 0.0009856 23.807 &lt; 2e-16 *** # EDUCATION_2CL.fHigh:CHILDHOOD_ECONOMY_2CL.fGood -0.1312722 0.0623235 -2.106 0.03518 * # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # On peut en déduire (échelle multiplicative) que : L’effet du niveau d’études (élevé plutôt que faible) sur le risque de surpoids/obésité est : Quand on est défavorisé pendant l’enfance: \\(\\small OR(X|D=0) = exp(-0.175) = 0.84\\) Quand on est favorisé pendant l’enfance: \\(\\small OR(X|D=1) = exp(-0.175 - 0.131) = 0.74\\) L’effet d’avoir un niveau d’étude élevé et d’être favorisé pendant l’enfance plutot qu’avoir un niveau d’étude faible et être défavorisé pendant l’enfance est \\(\\small OR(X,D) = exp(-0.175 - 0.019 - 0.131) =0.72\\) La modification d’effet est de: Sur l’échelle multiplicative: \\(\\small MI = exp(-0.131) = 0.88\\) (interaction multiplicative &lt;1 donc négative) Sur l’échelle additive: \\(\\small RERI = exp(-0.175-0.019-0.131) - exp(-0.175) – exp(-0.019) + 1 = -0.098\\) (interaction additive négative) 13.5 Analyse confirmatoire Si l’on utilise le package proposé par B Lepage pour réaliser cette analyse avec la g-computation, les résultats sont : A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.5 [0.487,0.513] \\(p_{01}\\)=0.496 [0.486,0.506] -0.005 [-0.021,0.012] 0.99 [0.96,1.02] A1=1 \\(p_{10}\\)=0.459 [0.438,0.479] \\(p_{11}\\)=0.423 [0.412,0.435] -0.035 [-0.059,-0.012] 0.92 [0.88,0.97] RD.A1|A2 -0.042 [-0.065,-0.018] -0.072 [-0.088,-0.057] RR.A1|A2 0.92 [0.87,0.96] 0.85 [0.83,0.88] a additive Interaction = -0.031 [-0.059;-0.003] b RERI = -0.061 [-0.123;4e-04] c multiplicative Interaction = 0.932 [0.877;0.989] Les résulats peuvent être intérprétés ainsi : l’effet d’un niveau d’études élevé (par rapport à faible) sur le risque de surpoids/obésité est moins fort de 3% lorsqu’on est défavorisé précocement ou encore, un niveau d’études élevé joue un rôle protecteur contre le surpoids/obésité moins important chez les personnes ayant grandi dans un foyer défavorisé Références "],["exemple-2---y-quantitatif.html", "Chapitre 14 Exemple 2 - Y quantitatif 14.1 Formuler les objectifs 14.2 Stratégies et méthodes 14.3 Analyse descriptive 14.4 Analyse exploratoire 14.5 Analyse confirmatoire", " Chapitre 14 Exemple 2 - Y quantitatif 14.1 Formuler les objectifs Dans cette étude [13], on s’est intéressé à : comment l’effet du sexe sur le taux de cholestérol LDL vers 45 ans varie en fonction de la défavorisation sociale précoce, comment l’effet de la défavorisation sociale précoce sur le taux de cholestérol LDL varie en fonction du sexe. La démarche ici est explicative : on cherche à comprendre des mécanismes causaux. A partir de la formulation des objectifs, on pourrait dire qu’on s’intéresse ici plutot à deux modifications d’effet. On va donc devoir à la fois agir sur le sexe \\(\\small do(S)\\) et sur la défavorisation sociale \\(\\small do(D)\\). Donc la démarche, en fait, sera plutôt une analyse d’interaction \\(\\small do(S,D)\\) 14.2 Stratégies et méthodes Le DAG (sans les médiateurs) était : Les estimands étaient définis sur l’échelle additive par : La modification de l’effet du sexe en fonction par la défavorisation sociale précoce : \\(\\small (Y_{s=1|d=0} - Y_{s=0|d=0}) - (Y_{s=1|d=1} - Y_{s=0|d=1})\\) ou \\(\\small (Y_{s=1,d=0} - Y_{s=0,d=0}) - (Y_{s=1,d=1} - Y_{s=0,d=1})\\) La modification de l’effet de la défavorisation sociale précoce par la sexe \\(\\small (Y_{d=1|s=0} - Y_{d=0|s=0}) - (Y_{d=1|s=1} - Y_{d=0|s=1})\\) ou \\(\\small (Y_{d=1,s=0} - Y_{d=0,s=0}) - (Y_{d=1,s=1} - Y_{d=0,s=1})\\) Les deux formulations sont ici équivalentes car il n’y pas de facteurs de confusion, donc, par exemple, \\(\\small Y_{d=1|s=0} = Y_{s=0|d=1} = Y_{d=1,s=0}\\) L’estimateur : Les effets ont été estimés par g-computation (standardisation par régression) [12]. Des régressions linéaires ont été utilisées pour estimer les potential outcomes pour chaque scénario, désignées par \\(\\small \\overline{Q}(S, D) = E(Y|S, D)\\). A partir des fonctions \\(\\small \\overline{Q}(S, D)\\) estimées, nous avons prédit la valeur de l’outcome Y pour chaque individu i pour chaque scénario. Les valeurs moyennes de Y dans chaque scénario vont ensuite nous permettre d’estimer les estimands selon leurs définitions précisées ci-dessus. Ces modèles\\(\\small \\overline{Q}(S, D)\\) vont comprendre 2 variables : le sexe et la défavorisation sociale précoce (il n’y a pas ici de facteurs de confusion). 14.3 Analyse descriptive Dans cette population (N=17 272), il y avait 51,4% d’hommes et 60,5% de personnes ayant été précocement défavorisées. On peut commencer par décrire les moyennes de cholestérol dans chaque catégorie de sexe et de défavorisation sociale : Sexe Défavorisation Mean(Chol LDL) Male Non 3.57 Male Oui 3.60 Female Non 3.24 Female Oui 3.37 14.4 Analyse exploratoire La sortie d’une modèle linéaire simple serait : # Call: # lm(formula = t8_ldl ~ as.factor(sex) + as.factor(soc_group) + # as.factor(sex) * as.factor(soc_group), data = ba_1) # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.24270 0.01594 203.475 &lt; 2e-16 *** # as.factor(sex)1 0.32553 0.02227 14.616 &lt; 2e-16 *** # as.factor(soc_group)2.Défav 0.12614 0.02052 6.148 8.02e-10 *** # as.factor(sex)1:as.factor(soc_group)2.Défav -0.09473 0.02863 -3.308 0.000941 *** # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # On peut en déduire (échelle additive) que : L’effet du sexe (d’être homme plutot que femme) est : Quand on est favorisé : \\(\\small DR(S|D=0) = +0.326\\) mmol/L Quand on est défavorisé : \\(\\small DR(S|D=1) = 0.326 - 0.095 =\\) +0.231 mmol/L L’effet de la défavorisation est : Quand on est une femme : \\(\\small DR(D|S=0) = +0.126\\) mmol/L Quand on est un homme : \\(\\small DR(D|S=1) = 0.126 - 0.095 =\\) +0.031 mmol/L L’effet d’être un homme et défavorisé plutot que femme et favorisé est \\(\\small DR(D,S) = 0.326 + 0.126 - 0.095 =\\) +0.357 mmol/L L’effet d’interaction/modification d’effet est : \\(\\small AI = -0.095\\) mmol/L On pourrait aussi déduire : \\(\\small Y_{00} = 3.24\\) mmol/L \\(\\small Y_{10} = 3.243 + 0.326 =\\) 3.57 mmol/L \\(\\small Y_{01} = 3.243 + 0.126 =\\) 3.37 mmol/L \\(\\small Y_{11} = 3.243 + 0.326 + 0.126 - 0.095 =\\) 3.6 mmol/L 14.5 Analyse confirmatoire Si l’on utilise le package proposé par B Lepage pour réaliser cet analyse avec la TMLE (effets d’intéraction calculés à partir des paramètres d’une modèle structurel marginal estimé à l’aide du package R ltmle), les résultats sont : A2=0 A2=1 RD.A2|A1 A1=0 \\(p_{00}\\)=3.243 [3.213,3.273] \\(p_{01}\\)=3.369 [3.344,3.394] 0.126 [0.087,0.165] A1=1 \\(p_{10}\\)=3.568 [3.538,3.598] \\(p_{11}\\)=3.6 [3.574,3.625] 0.031 [-0.008,0.071] RD.A1|A2 0.326 [0.283,0.368] 0.231 [0.195,0.267] a additive Interaction = -0.095 [-0.15;-0.039] On retrouve des résulats qui peuvent être intérprétés ainsi : l’effet d’être un homme (ou “la différence H-F) est moins fort de additive Interaction = -0.095 [-0.15;-0.039] mmol/L lorsqu’on est défavorisé précocement l’effet de la défavorisation est moins fort de additive Interaction = -0.095 [-0.15;-0.039] mmol/L chez les hommes En réalité, on a réalisé cette analyse par g-computation (voir chapitre 10) sur des données imputées et boostrappées (l’exemple ci-dessus a été réalisé sur une seule des bases bootstrappées, ce qui explique les différences), et les résultats, présentées selon les recommandations modifiées de Knol et VanderWeele, étaient: Références "],["exemple-4---x-quantitatif.html", "Chapitre 15 Exemple 4 - X quantitatif 15.1 Formuler les objectifs 15.2 Stratégies et méthodes 15.3 Analyse descriptive 15.4 Analyse exploratoire 15.5 Analyse confirmatoire", " Chapitre 15 Exemple 4 - X quantitatif Les articles qui se consacrent aux interactions présentent souvent des méthodes applicables lorsque les deux expositions X et V sont binaires. Or, en épidémiologie, les expositions peuvent aussi être continues et, si dichotomiser ces variables peut simplifier l’approche de l’interaction, cela conduit à une perte d’information qui n’est pas souhaitable et pose la question complexe du choix des seuils [14] [15] [16]. Nous présentons ici un exemple dans lequel l’une des expositions, l’âge, est analysée en tant que variable quantitative continue. 15.1 Formuler les objectifs Dans cette étude fictive, on s’est intéressé à la consommation de cannabis : comment le fait d’avoir déjà fumé du cannabis Y varie avec l’âge A et le sexe S. La démarche est explicative : on cherche à comprendre les mécanismes causaux de ce comportement de santé. Ici, on adoptera une démarche d’analyse d’interaction \\(\\small do(S,A)\\) 15.2 Stratégies et méthodes Le DAG (sans les médiateurs) était : Les estimands était définis par : L’effet de l’âge (“avoir 10 ans de plus”) chez les hommes \\(\\small DR = Y_{S=0,A=a+10} - Y_{S=0,A=a}\\) \\(\\small RR = \\frac{Y_{S=0,A=a+10}}{Y_{S=0,A=a}}\\) L’effet de l’âge (“avoir 10 ans de plus”) chez les femmes : \\(\\small DR = Y_{S=1,A=a+10} - Y_{S=1,A=a}\\) \\(\\small RR = \\frac{Y_{S=1,A=a+10}}{Y_{S=1,A=a}}\\) L’effet d’interaction entre l’âge et le sexe (l’effet du sexe est-il différent en fonction de l’âge et l’effet de l’âge est-il différent en fonction du sexe ?) sur l’échelle additive : \\(\\small AI = Y_{S=1,A=a+10} - Y_{S=0,A=a+10} - Y_{S=1,A=a} + Y_{S=0,A=a+10}\\) sur l’échelle multiplicative : \\(\\small MI =\\frac{Y_{S=1,A=a+10} \\times Y_{S=0,A=a}}{Y_{S=1,A=a} \\times Y_{S=0,A=a+10}}\\) 15.3 Analyse descriptive Dans cette population (N=202 768), il y avait 53,7% d’hommes et la moyenne d’âge était de 47,1 ans. On peut commencer par décrire la proportion de personnes ayant déjà fumé du cannabis par sexe et classe d’âge : Sexe Age P(Cannabis), % Male 20- 51,1 Male ]20 à 40] 66,3 Male ]40 à 60] 40,4 Male 60+ 12,1 Female 20- 44,2 Female ]20 à 40] 52,7 Female ]40 à 60] 26,7 Female 60+ 12,1 Il semble y avoir une interaction entre l’âge et le sexe sur la probabilité d’avoir déjà fumé du cannabis. Cependant, la relation entre l’âge et l’outcome ne semble pas linéaire, ce qui est confirmé graphiquement : Pour simplifier les analyses, nous n’allons inclure que les plus de 30 ans (N = 177 940), pour lesquels la relation est linéaire : Le modèle de régression logistique (\\(\\cdot\\cdot\\cdot\\)) semble être plus proche de la modélisation non paramétrique sur données observées (loess, —–) que la modélisation linaire (\\(---\\)) . D’ailleurs, le R² du modèle logistique est de 0,168 contre 0,139 pour le modèle linéaire. 15.4 Analyse exploratoire 15.4.1 Régression logistique L’outcome étant binaire, il est plus classique d’utiliser un modèle logistique, dont les résultats seraient : # Call: # glm(formula = cannabis ~ sexe + age + sexe * age, family = binomial, # data = data) # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) 3.9144609 0.0372560 105.07 &lt;2e-16 *** # sexeWomen -1.1644706 0.0511834 -22.75 &lt;2e-16 *** # age -0.0882928 0.0007566 -116.70 &lt;2e-16 *** # sexeWomen:age 0.0117238 0.0010623 11.04 &lt;2e-16 *** # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Ce qui, en terme d’OR, donnerait : # OR 2.5 % 97.5 % # (Intercept) 50.1220409 46.5985990 53.9259952 # sexeWomen 0.3120878 0.2822910 0.3450107 # age 0.9154927 0.9141333 0.9168485 # sexeWomen:age 1.0117927 1.0096884 1.0139018 Les modèles de régression logistique donnent des résulats sur l’échelle multiplicative : L’effet du sexe (d’être femme plutot que homme) est : “A 0 ans” (à l’origine) : \\(\\small OR(S|A=0) = \\times 0.31\\) “A 1 ans” : \\(\\small OR(S|A=1) = exp(-1,164 + 0,012) \\times\\) 0.32 A 40 ans (par exemple) : \\(\\small OR(S|A=40) = exp(-1,164 + 0,012 \\times 40) = \\times\\) 0.5 L’effet de l’age est : Quand on est un homme : \\(\\small OR(A|S=0) = exp(-0,088\\times 10 ) = \\times\\) 0.41 par 10 ans Quand on est une femme : \\(\\small OR(A|S=1) = exp(-0,088\\times 10 + 0,012\\times 10) = \\times\\) 0.47 par 10 ans L’effet d’être une femme et d’avoir 10 ans de plus plutot que homme “et 0 ans” \\(\\small OR(A,S) = exp(-1,164 -0,088\\times 10 + 0,012\\times 10) = \\times\\) 0.15 L’effet d’interaction/modification d’effet est : \\(\\small MI =\\times 1,01\\) sur 1 an \\(\\small MI_{10} = exp(0,012\\times 10 ) = \\times\\) 1.13 sur 10 ans Un effet d’interaction additif \\(\\small RERI_{OR} = OR_{11} - OR_{01} - OR_{10} + 1 =\\) 0.047 pour 1 ans \\(\\small RERI_{OR,10} =\\) 0.362 On a donc une interaction multiplicative positive (MI&gt;1) et significative et une interaction additive aussi positive (RERI &gt;0). 15.4.2 Régression linéaire La sortie d’une modèle linéaire simple serait : # # Call: # lm(formula = cannabis ~ sexe + age + sexe * age, data = data) # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 1.3197103 0.0066820 197.50 &lt;2e-16 *** # sexeWomen -0.3373482 0.0091573 -36.84 &lt;2e-16 *** # age -0.0183730 0.0001294 -142.03 &lt;2e-16 *** # sexeWomen:age 0.0044248 0.0001781 24.85 &lt;2e-16 *** # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 On peut en déduire, ici sur une échelle additive, que : L’effet du sexe (d’être femme plutot que homme) est : “A 0 ans” (à l’origine) : \\(\\small DR(S|A=0) = -33,73\\)% A 20 ans (par exemple) : \\(\\small DR(S|A=20) = -33,73 + 0,44 \\times 20 =\\) -24.93% A 40 ans (par exemple) : \\(\\small DR(S|A=40) = -33,73 + 0,44 \\times 40 =\\) -16.13% A 60 ans (par exemple) : \\(\\small DR(S|A=60) = -33,73 + 0,44 \\times 60 =\\) -7.33% L’effet de l’age est : Quand on est un homme : \\(\\small DR(A|S=0) = -1,84\\times 10 =\\) -18.4% par 10 ans Quand on est une femme : \\(\\small DR(A|S=1) = -1,84\\times 10 + 0.44\\times 10 =\\) -14 % par année d’âge L’effet d’être une femme et d’avoir 10 ans de plus plutot que homme et un certain âge \\(\\small DR(A,S) = -33,73 -1,84\\times 10 + 0,44\\times 10 =\\) -47.73 % L’effet d’interaction/modification d’effet est : \\(\\small AI = +0.44\\)`% \\(\\small AI_{10} = +0.44\\times 10 =\\) 4.4% On retrouve une interaction additive significative et positive. Les expositions ayant un effet négatif et l’effet d’interaction étant positif, cet effet est difficile à interpréter, mais on pourrait le formuler plus simplement en changeant la catégorie de référence du sexe de homme à femme. Ainsi : globalement, la probabilité d’avoir déjà fumer du cannabis diminue avec l’âge chez les hommes (-1,8% par an) et chez les femmes (-1,4% par an). Cette probabilité est plus élevée chez les hommes (de 16% par exemple à 40 ans), mais cet écart diminue avec l’âge, de 4,4% tous les 10 ans. 15.4.3 Effets prédits A partir des modèles, on peut déduire les effets prédits pour certaines catégories. Par exemple, avec le modèle logistique : \\(\\small Y_{S=0,A=30} = \\frac{exp(3,914 - 0,088 \\times 30)}{1+exp(3,914 - 0,088 \\times 30)} =\\) 78.1% \\(\\small Y_{S=0,A=50} = \\frac{exp(3,914 - 0,088 \\times 50)}{1+exp(3,914 - 0,088 \\times 50)} =\\) 38.1% \\(\\small Y_{S=1,A=30} = \\frac{exp(3,914 - 1,164 - 0,088 \\times 30 + 0,012 \\times 30)}{1+exp(3,914 - 1,164 - 0,088 \\times 30 + 0,012 \\times 30)} =\\) 61.5% \\(\\small Y_{S=1,A=50} = \\frac{exp(3,914 - 1,164 - 0,088 \\times 50 + 0,012 \\times 50)}{1+exp(3,914 - 1,164 - 0,088 \\times 50 + 0,012 \\times 50)} =\\) 25.9% Avec le modèle linéaire, on aurait : \\(\\small Y_{S=0,A=30} = 131,97 - 1,84 \\times 30 =\\) 76.8% \\(\\small Y_{S=0,A=50} = 131,97 - 1,84 \\times 50 =\\) 40% \\(\\small Y_{S=1,A=30} = 131,97 -33,73 - 1,84 \\times 30 + 0,44 \\times 30 =\\) 56.2% \\(\\small Y_{S=1,A=50} = 131,97 -33,73 - 1,84 \\times 50 + 0,44 \\times 50 =\\) 28.2% 15.5 Analyse confirmatoire Nous avons calculé les effets d’intérêt avec une méthode de modèle structurel marginal (Intervalles de confiance estimé par bootstrap, 200 répétitions), le modèle utilisé pour prédire les outcomes contrefactuels étaient un modèle de régression logistique. Le code était : B=200 simu.base &lt;- data.frame(i.simu=c(1:B)) for (i in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) ; set.seed(01062023+i*12) bootData &lt;- data[bootIndices,] #modèle Q.model &lt;- glm(data=bootData, formula = cannabis ~ sexe+ age+ sexe*age,family = binomial) # Scénarios # data.S1 &lt;- data.S2 &lt;- bootData data.S1$sexe &lt;- &quot;Women&quot; data.S2$sexe &lt;- &quot;Men&quot; data.S1A30 &lt;- data.S1A40 &lt;- data.S1A50 &lt;- data.S1A60 &lt;- data.S1A70 &lt;- data.S1 data.S1A35 &lt;- data.S1A45 &lt;- data.S1A55 &lt;- data.S1A65 &lt;- data.S1 data.S2A30 &lt;- data.S2A40 &lt;- data.S2A50 &lt;- data.S2A60 &lt;- data.S2A70 &lt;- data.S2 data.S2A35 &lt;- data.S2A45 &lt;- data.S2A55 &lt;- data.S2A65 &lt;- data.S2 data.S1A30$age &lt;- data.S2A30$age &lt;- 30 data.S1A35$age &lt;- data.S2A35$age &lt;- 35 data.S1A40$age &lt;- data.S2A40$age &lt;- 40 data.S1A45$age &lt;- data.S2A45$age &lt;- 45 data.S1A50$age &lt;- data.S2A50$age &lt;- 50 data.S1A55$age &lt;- data.S2A55$age &lt;- 55 data.S1A60$age &lt;- data.S2A60$age &lt;- 60 data.S1A65$age &lt;- data.S2A65$age &lt;- 65 data.S1A70$age &lt;- data.S2A70$age &lt;- 70 # Y contrefactuel Y.S1A30.pred &lt;- predict(Q.model, newdata = data.S1A30, type = &quot;response&quot;) Y.S1A40.pred &lt;- predict(Q.model, newdata = data.S1A40, type = &quot;response&quot;) Y.S1A50.pred &lt;- predict(Q.model, newdata = data.S1A50, type = &quot;response&quot;) Y.S1A60.pred &lt;- predict(Q.model, newdata = data.S1A60, type = &quot;response&quot;) Y.S1A70.pred &lt;- predict(Q.model, newdata = data.S1A70, type = &quot;response&quot;) Y.S2A30.pred &lt;- predict(Q.model, newdata = data.S2A30, type = &quot;response&quot;) Y.S2A40.pred &lt;- predict(Q.model, newdata = data.S2A40, type = &quot;response&quot;) Y.S2A50.pred &lt;- predict(Q.model, newdata = data.S2A50, type = &quot;response&quot;) Y.S2A60.pred &lt;- predict(Q.model, newdata = data.S2A60, type = &quot;response&quot;) Y.S2A70.pred &lt;- predict(Q.model, newdata = data.S2A70, type = &quot;response&quot;) Y.S1A35.pred &lt;- predict(Q.model, newdata = data.S1A35, type = &quot;response&quot;) Y.S1A45.pred &lt;- predict(Q.model, newdata = data.S1A45, type = &quot;response&quot;) Y.S1A55.pred &lt;- predict(Q.model, newdata = data.S1A55, type = &quot;response&quot;) Y.S1A65.pred &lt;- predict(Q.model, newdata = data.S1A65, type = &quot;response&quot;) Y.S2A35.pred &lt;- predict(Q.model, newdata = data.S2A35, type = &quot;response&quot;) Y.S2A45.pred &lt;- predict(Q.model, newdata = data.S2A45, type = &quot;response&quot;) Y.S2A55.pred &lt;- predict(Q.model, newdata = data.S2A55, type = &quot;response&quot;) Y.S2A65.pred &lt;- predict(Q.model, newdata = data.S2A65, type = &quot;response&quot;) Y &lt;- c(Y.S1A30.pred, Y.S1A40.pred, Y.S1A50.pred, Y.S1A60.pred, Y.S1A70.pred, Y.S1A35.pred, Y.S1A45.pred, Y.S1A55.pred, Y.S1A65.pred, Y.S2A30.pred, Y.S2A40.pred, Y.S2A50.pred, Y.S2A60.pred, Y.S2A70.pred, Y.S2A35.pred, Y.S2A45.pred, Y.S2A55.pred, Y.S2A65.pred) # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(data.S1A30, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A40, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A50, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A60, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A70, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A35, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A45, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A55, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S1A65, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A30, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A40, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A50, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A60, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A70, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A35, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A45, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A55, select = c(&quot;sexe&quot;, &quot;age&quot;)), subset(data.S2A65, select = c(&quot;sexe&quot;, &quot;age&quot;))) ## Modèle structurel marginal # logistique msm.glm &lt;- glm(Y ~ age + sexe + age:sexe, data = data.frame(Y,X), family = &quot;binomial&quot;) #linéaire pour l&#39;interaction additive msm.lm &lt;- glm(Y ~ age + sexe + age:sexe, data = data.frame(Y,X), family = &quot;gaussian&quot;) # Tous les effets simu.base$est.Y0_30[simu.base$i.simu==i] = round(mean(Y.S2A30.pred),4) simu.base$est.Y0_40[simu.base$i.simu==i] = round(mean(Y.S2A40.pred),4) simu.base$est.Y0_50[simu.base$i.simu==i] = round(mean(Y.S2A50.pred),4) simu.base$est.Y0_60[simu.base$i.simu==i] = round(mean(Y.S2A60.pred),4) simu.base$est.Y0_70[simu.base$i.simu==i] = round(mean(Y.S2A70.pred),4) simu.base$est.Y1_30[simu.base$i.simu==i] = round(mean(Y.S1A30.pred),4) simu.base$est.Y1_40[simu.base$i.simu==i] = round(mean(Y.S1A40.pred),4) simu.base$est.Y1_50[simu.base$i.simu==i] = round(mean(Y.S1A50.pred),4) simu.base$est.Y1_60[simu.base$i.simu==i] = round(mean(Y.S1A60.pred),4) simu.base$est.Y1_70[simu.base$i.simu==i] = round(mean(Y.S1A70.pred),4) simu.base$est.RD_30[simu.base$i.simu==i] = round(mean(Y.S1A30.pred - Y.S2A30.pred),4) simu.base$est.RD_40[simu.base$i.simu==i] = round(mean(Y.S1A40.pred - Y.S2A40.pred),4) simu.base$est.RD_50[simu.base$i.simu==i] = round(mean(Y.S1A50.pred - Y.S2A50.pred),4) simu.base$est.RD_60[simu.base$i.simu==i] = round(mean(Y.S1A60.pred - Y.S2A60.pred),4) simu.base$est.RD_70[simu.base$i.simu==i] = round(mean(Y.S1A70.pred - Y.S2A70.pred),4) simu.base$est.RR_30[simu.base$i.simu==i] = round(mean(Y.S1A30.pred / Y.S2A30.pred),4) simu.base$est.RR_40[simu.base$i.simu==i] = round(mean(Y.S1A40.pred / Y.S2A40.pred),4) simu.base$est.RR_50[simu.base$i.simu==i] = round(mean(Y.S1A50.pred / Y.S2A50.pred),4) simu.base$est.RR_60[simu.base$i.simu==i] = round(mean(Y.S1A60.pred / Y.S2A60.pred),4) simu.base$est.RR_70[simu.base$i.simu==i] = round(mean(Y.S1A70.pred / Y.S2A70.pred),4) simu.base$est.RD_Sm[simu.base$i.simu==i] = round(msm.lm$coefficients[&quot;age&quot;]*10,4) simu.base$est.RR_Sm[simu.base$i.simu==i] = round(exp(msm.glm$coefficients[&quot;age&quot;]*10),4) simu.base$est.RD_Sw[simu.base$i.simu==i] = round(msm.lm$coefficients[&quot;age&quot;]*10 + msm.lm$coefficients[&quot;age:sexeWomen&quot;]*10,4) simu.base$est.RR_Sw[simu.base$i.simu==i] = round(exp(msm.glm$coefficients[&quot;age&quot;]*10 + msm.glm$coefficients[&quot;age:sexeWomen&quot;]*10),4) simu.base$est.AI[simu.base$i.simu==i] = round(msm.lm$coefficients[&quot;age:sexeWomen&quot;]*10,4) simu.base$est.MI[simu.base$i.simu==i] = round(exp(msm.glm$coefficients[&quot;age:sexeWomen&quot;]*10),4) simu.base$est.RERI[simu.base$i.simu==i] = round(exp(msm.glm$coefficients[&quot;age&quot;]*10 + msm.glm$coefficients[&quot;sexeWomen&quot;] + msm.glm$coefficients[&quot;age:sexeWomen&quot;]*10) - exp(msm.glm$coefficients[&quot;age&quot;]*10 + msm.glm$coefficients[&quot;age:sexeWomen&quot;]*10) - exp(msm.glm$coefficients[&quot;sexeWomen&quot;] + msm.glm$coefficients[&quot;age:sexeWomen&quot;]*10) + 1, 4) } effect &lt;- round(colMeans(simu.base),2) confint &lt;- apply(simu.base, 2, function(x) round(quantile(x,probs = c(0.025, 0.975)),2)) tab_all &lt;- as.data.frame(rbind(effect,confint)) Au final, les résultats étaient : Sex = Men Sex = Women RD within strata of Age OR within strata of Age Age = 30 0.62 [0.61 to 0.62] 0.78 [0.78 to 0.79] -0.16 [-0.17 to -0.16] 0.79 [0.78 to 0.8] Age = 40 0.42 [0.42 to 0.43] 0.6 [0.59 to 0.6] -0.17 [-0.18 to -0.17] 0.71 [0.7 to 0.72] Age = 50 0.25 [0.25 to 0.25] 0.38 [0.37 to 0.38] -0.13 [-0.13 to -0.12] 0.66 [0.65 to 0.67] Age = 60 0.13 [0.13 to 0.13] 0.2 [0.19 to 0.2] -0.07 [-0.07 to -0.06] 0.66 [0.65 to 0.68] Age = 70 0.06 [0.06 to 0.07] 0.09 [0.09 to 0.09] -0.03 [-0.03 to -0.02] 0.7 [0.67 to 0.73] RD (10 y) within strata of Sex -0.18 [-0.18 to -0.18] -0.14 [-0.14 to -0.14] NA NA OR (10 y) within strata of Sex 0.41 [0.4 to 0.41] 0.45 [0.45 to 0.46] NA NA a Additive interaction (10 years) =0.04 [0.04 to 0.04] b Multiplicative Interaction (10 years) =1.11 [1.09 to 1.13] c RERI (10 years) =0.33 [0.32 to 0.34] On retrouve : une interaction additive significative et positive : l’écart entre les hommes et les femmes diminue avec l’âge, de 4% tous les 10 ans, ou l’effet d’avoir 10 ans est plus faible de 4% chez les hommes par rapport au femmes. Le RERI est aussi positif et significatif (l’OR augmente de 33% tous les 10 ans). une interaction multiplicative significative et positive : l’effet d’être un homme plutôt qu’une femme sur le risque d’avoir consommer du cannabis est moins fort quand l’âge augmente, ou l’effet d’avoir 10 ans est multiplié par 1,11 chez les femmes par rapport aux hommes Références "],["synthèse-générale.html", "Chapitre 16 Synthèse générale", " Chapitre 16 Synthèse générale La première étape importantes consiste à définir précisément l’objectif. Et, si l’on est dans une démarche explicative, d’inférence causale, il s’agit de définir si la mesure d’un effet d’interaction est nécessaire pour y répondre (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Le fait de choisir une démarche d’analyse d’interaction ou de modification d’effet repose sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Concernant le choix de l’échelle, idéalement, les interactions devraient être reportées sur les 2 échelles [11] [2]. Cependant, l’échelle additive est plus appropriée pour évaluer l’utilité en santé publique [2] [11]. Concernant les paramètres, Références "],["plusloin.html", "Chapitre 17 Pour aller plus loin… 17.1 Ajouter de la complexité 17.2 Interaction avec confusion intermédiaire 17.3 Interaction et médiation", " Chapitre 17 Pour aller plus loin… 17.1 Ajouter de la complexité A1 et A2 sont rarement indépendants. Scénario plus probable : 17.2 Interaction avec confusion intermédiaire 17.3 Interaction et médiation [17] [18] Références "],["références.html", "Chapitre 18 Références", " Chapitre 18 Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
