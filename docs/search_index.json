<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
[["index.html", "Interactions et modifications d’effet en Epidémiologie Chapter 1 Présentation", " Interactions et modifications d’effet en Epidémiologie CERPOP, INSERM, EQUITY Team Last compiled on 04 mai, 2023 Chapter 1 Présentation Ce document a été rédigé en tant que document de synthèse du travail du groupe “Interaction” de l’équipe EQUITY, CERPOP. Ce travail a consisté en une revue de la littérature et en une application détaillée des méthodes sur des analyses illustratives, dans un but d’auto-formation et pédagogique. Les participant.e.s du groupe de travail sont : Hélène COLINEAUX Léna BONIN Camille JOANNES Benoit LEPAGE Lola NEUFCOURT Ainhoa UGARTECHE The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapter 2 Introduction 2.1 Quand étudier les interactions ? 2.2 Les points les plus importants", " Chapter 2 Introduction Comment telle prédisposition génétique et telle exposition environnementale inter-agissent-elles ? L’effet de tel traitement varie-t-il selon les circonstances ? Selon les caractéristiques du patient ? Telle intervention peut-elle être bénéfique pour un groupe social et délétère pour un autre ? De nombreuses questions épidémiologiques impliquent des mécanismes d’interactions ou de modifications d’effet. Pourtant, étudier ces mécanismes restent encore complexe aujourd’hui sur le plan méthodologique : quelle démarche adopter ? sur quelle échelle mesurer cette interaction ? comment interpréter les coefficients ? et cetera. Dans ce document, nous proposons une synthèse de la littérature et une démarche progressive et appliquée pour explorer ces questions. 2.1 Quand étudier les interactions ? 2.1.1 Prediction versus causalité La science des données cherche à répondre à 3 types d’objectifs [1] : Selon le type d’objectif, la démarche d’analyse et les enjeux méthodologiques ne vont pas être les mêmes. Si l’objectif est prédictif, la démarche va être centrée sur la prédiction de l’outcome, à partir de covariables sélectionnées afin d’optimiser la précision de l’estimation, tout en prenant en compte leur disponibilité en pratique et la parcimonie du modèle. Dans une démarche explicative, ou étiologique, au contraire, la démarche va être centré sur l’estimation d’un effet causal, en prenant en compte les covariables en fonction de leur rôle vis-à-vis de l’effet d’intérêt (facteurs de confusion, colliders, médiateurs…). En épidémiologie, à l’exception des cas où l’on souhaite développer un test ou score diagnostic ou pronostic, les objectifs sont le plus souvent explicatifs. On cherche en effet, la plupart du temps, à identifier des liens de cause à effet, afin de pouvoir agir sur les causes pour modifier les effets. Finalement, pour répondre à la question “quand doit-on prendre en compte les interactions ?”, il est d’abord nécessaire d’identifier dans quel type de démarche l’on s’inscrit : Démarche prédictive : on ajoutera alors les interactions dans le modèle de prédiction, pour le rendre plus flexible, si cela améliore la précision de l’estimation [2]. Démarche explicative/étiologique : on étudiera les interactions ou modifications d’effet, si cela répond directement à l’objectif. Par exemple : Si l’objectif est du type “l’effet de X sur Y varie-t-il en fonction de V ?”, on prendra en compte l’interactions entre X et V. Les objectifs qui nécessitent la prise en compte de l’interaction peuvent aussi être du type : “Quel est l’effet conjoint de X et V sur Y ?” ou “Quel part de l’effet de X sur Y disparaît quand V est modifié ?”, etc. Par contre, si l’objectif est simplement d’estimer l’effet de X sur Y, ou l’effet médié par M, la prise en compte des interactions entre X et des covariables (facteurs de confusion ou médiateurs) n’est pas indispensable. C’est l’effet “moyen” qui sera estimé. Des termes d’interactions peuvent cependant être ajoutés (mais non interprétés), si cela améliore la précision de l’estimation (enjeu d’optimisation du modèle). 2.1.2 Types d’objectifs Dans ce document, nous nous intéresserons principalement aux interactions et modifications d’effet dans une démarche étiologique/ explicative. Les objectifs pouvant nécessiter l’étude de l’interaction/modification d’effet sont [2] : Cibler des sous-groupes. Par exemple, identifier des sous-groupes pour lesquels l’intervention aura le plus d’effet afin de pouvoir cibler l’intervention en cas de ressources limitées, ou s’assurer que l’intervention est bénéfice pour tous les groupes et pas délétères pour certains groupes. Explorer les mécanismes d’un effet. Par exemple, en cas d’intervention qui n’a d’effet qu’en présence ou absence d’une caractéristiques particulière (définition mécanistique de l’interaction) ou seulement conjointement à une autre intervention. Etudier l’effet d’une intervention pour éliminer une partie de l’effet d’une exposition non modifiable. Par exemple, quelle part de l’effet du niveau d’éducation des parents sur la mortalité disparaîtrait si on intervenait sur le tabagisme à l’adolescence ? 2.2 Les points les plus importants La première étape importante consiste à définir précisément l’objectif : L’objectif est-il de type descriptif, prédictif ou explicatif ? Si l’on est dans une démarche explicative, d’inférence causale, est-ce que la mesure d’un effet d’interaction est nécessaire pour y répondre ? (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Ensuite, de nombreuses questions se posent pour réaliser une analyse d’interaction, auxquelles nous tentons de répondre dans ce document : S’agit-il d’une interaction ou une modification d’effet ? Sur quelle échelle la mesure-t-on ? Un effet d’interaction peut en effet être défini sur une échelle multiplicative ou additive, et les résultats entre ces échelles peuvent être contradictoires. Comment estimer cette interaction ? Quels paramètres présenter et comment les interpréter ? Comment la représenter graphiquement ? Références "],["notations.html", "Chapter 3 Notations 3.1 Variables et probabilités 3.2 Mesures d’effets", " Chapter 3 Notations 3.1 Variables et probabilités On note : un outcome : \\(\\small Y\\), deux expositions : \\(\\small X\\) et \\(\\small V\\) La probabilité de l’outcome Y dans chaque strate définie par les 2 expositions est notée : \\(\\small p_{xv} = P(Y = 1|X = x,V = v)\\) Exemple On a deux exposition \\(\\small X\\), le tabagisme actif à 20 ans, et \\(\\small V\\), le fait d’avoir vécu un évènement traumatique pendant l’enfance. L’outcome \\(\\small Y\\) est binaire et représente le fait d’avoir au moins une pathologie chronique à 60 ans \\(\\small Y=1\\) ou aucune \\(\\small Y=0\\). On décrit (données complètement fictives) : Interprétation : La probabilité d’avoir au moins une pathologie chronique à 60 ans quand on n’a pas vécu d’événement traumatique pendant l’enfance et pas fumé à 20 ans est de 10%, tandis qu’elle est de 90% quand on a vécu un événement traumatique et fumé. 3.2 Mesures d’effets L’effet d’une variable \\(\\small X\\) sur \\(\\small Y\\) peut être mesuré sur deux échelles : additive (différence de risque/probabilité) ou multiplicative (rapport de risque/probabilité). Concernant les différences de risques (DR, effets additifs) On a donc : L’effet d’un X binaire sur Y est : \\(\\small DR(X) = P(Y = 1|do(X = 1)) - P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1|X = 1) - P(Y = 1|X = 0) = p_1-p_0\\) L’effet conjoint de X et V est : \\(\\small DR(X,V) = p_{11}-p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small DR(X|V=0) = p_{10}-p_{00}\\) et \\(\\small DR(X|V=1) = p_{11}-p_{01}\\) Exemple Différences de risques pour l’exemple 1 \\(\\small DR(X \\cap V) = p_{11}-p_{00} = 0,9 - 0,1 = +0,8\\) \\(\\small DR (X | V=0) = p_{10}-p_{00} = 0,4 - 0,1 = +0,3\\) \\(\\small DR (X | V=1) = p_{11}-p_{01} = 0,9 - 0,2 = +0,7\\) Le fait d’être doublement exposé par rapport à pas du tout augmente le risque de +80%. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 augmente le risque de +30%, alors que parmi les personnes ayant vécu un événement traumatique, il est augmenté de +70%. Concernant, les rapports de risque (effets multiplicatifs) on peut notamment utiliser les risques relatifs (RR). On donc : L’effet d’un X binaire sur Y est : \\(\\small RR(X) = P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))= p_1 / p_0\\) L’effet conjoint de X et V est : \\(\\small RR(X,V) = p_{11}/p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small RR(X|V=0) = p_{10}/p_{00}\\) et \\(\\small RR(X|V=1) =p_{11}/p_{01}\\) Exemple Risques relatifs pour l’exemple 1 \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) Le risque quand on est doublement exposé par rapport à pas du tout est multiplié par 9. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 multiplie le risque par 4, alors que parmi les personnes ayant vécu un événement traumatique, il est multiplié par 4,5. "],["interaction-vs-modification-deffets.html", "Chapter 4 Interaction vs modification d’effets 4.1 Modification d’effets 4.2 Interaction 4.3 Synthèse", " Chapter 4 Interaction vs modification d’effets 4.1 Modification d’effets La question de la modification d’effet consiste à d’identifier si l’effet du traitement ou de l’exposition est différent dans différents groupes de patients ayant des caractéristiques différentes (estimer l’effet d’une exposition séparément en fonction d’une autre variable) [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait 1 seule intervention mais que l’analyse est stratifiée sur V. On analyse donc l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de \\(\\small V\\). En observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X\\) mais pas celui de \\(\\small V\\). On ajustera sur les facteurs de confusion de \\(\\small X \\rightarrow Y\\). On ne fait pas d’hypothèse sur les mécanismes de la modification d’effet, qui peut être causale, de façon directe ou indirecte, ou pas du tout (par proxy ou cause commune) [4]. Exemples d’objectifs : identifier des groupes pour lesquels le traitement ne serait pas utile, ou si l’effet du traitement est homogène/hétérogène en fonction de l’âge, du sexe, etc. On a une modification de l’effet de X par V si l’effet de X est différent dans chaque strate définie par V: en additif : \\(\\small DR(X | V=0) ≠ DR(X | V=1)\\) soit \\(\\small p_{10}-p_{00} ≠ p_{11}-p_{01}\\) en multiplicatif : \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) soit \\(\\small p_{10}/p_{00} ≠ p_{11}/p_{01}1\\) Exemple Modification d’effet dans l’exemple 1 En additif : effet quand V=0 : \\(\\small DR (X | V=0) = 0,4 - 0,1 = +0,3\\) effet quand V=1 : \\(\\small DR (X | V=1) = 0,9 - 0,2 = +0,7\\) donc \\(\\small DR (X | V=0) ≠ DR (X | V=1)\\) En multiplicatif : effet quand V=0 : \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) effet quand V=1 : \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) donc \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) Ici l’effet du tabagisme est différent selon que les personnes ont vécu un événement traumatique ou non, sur l’échelle additive et multiplicative. On peut donc dire que le fait d’avoir vécu un événement traumatique modifie l’effet du tabac. Attention, on fait l’hypothèse de l’absence de facteurs de confusion entre le tabagisme et l’outcome, ce qui est en réalité peu probable. 4.2 Interaction Quand on s’intéresse à l’interaction, on s’intéresse plutôt à l’effet conjoints de 2 expositions (ou plus) sur un outcome. Il y a une interaction synergique si l’effet conjoint est supérieur à l’effet de la somme des individuels. Il y a une interaction antagoniste lorsque l’effet conjoint est inférieur à la somme des effets individuels [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y a plusieurs interventions selon le nombre de combinaison. On analyse donc l’effet du scénario \\(\\small do(X, V)\\). Ici l’effet causal d’interêt est vraiment l’effet conjoint des deux variables. Dans un schéma observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X*V\\). On ajustera sur les facteurs de confusion de \\(\\small X.V \\rightarrow Y\\). On fait l’hypothèse que les mécanismes de l’effet conjoint de X et V sont causaux. On a une interaction si : en additif : \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) \\(\\small p_{11}-p_{00} ≠ (p_{10}-p_{00})+(p_{01}-p_{00})\\) \\(\\small p_{11} ≠ p_{10} + p_{01} - p_{00}\\) en multiplicatif \\(\\small RR(X \\cap V) ≠ RR(X| V=0) + RR(V| X=0)\\) \\(\\small p_{11}/p_{00} ≠ (p_{10}/p_{00})+(p_{01}/p_{00})\\) \\(\\small p_{11} ≠ (p_{10} + p_{01}) / p_{00}\\) Exemple Interaction dans l’exemple 1 En additif : effet joint : \\(\\small DR(X \\cap V) = 0,9 - 0,1 = +0.8\\) somme des effets individuel : \\(\\small DR(X| V=0) + DR(V| X=0) = +0,3 +0,1 = +0,4\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) En multiplicatif : effet joint : \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) produit des effets individuel : \\(\\small RR(X | V=0) \\times RR(V | X=0) = 4 \\times 2 = \\times 8\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) \\times DR(V| X=0)\\) Ici l’effet joint des 2 expositions est supérieur à la somme ou au produit des effets individuels, il y a donc une interaction synergique entre les deux expositions. 4.3 Synthèse Mathématiquement, les formulations sont équivalentes : échelle additive: \\(\\small p_{10} -p_{00} ≠ p_{11}- p_{01} ⇔ p_{11}≠(p_{10}+p_{01})- p_{00}\\) échelle multiplicative : \\(\\small p_{10} /p_{00} ≠ p_{11}/ p_{01} ⇔ p_{11}≠(p_{10} \\times p_{01})/p_{00}\\) La différence se joue plutôt sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Il existe des cas où l’identification d’une interaction ou d’une modification d’effet ne conduira pas à la même démarche et donc au même résultat [5]. Prenons le DAG suivant : Dans ce cas, il n’y a pas d’interaction entre A1 et A2, car si on intervient sur les 2 (\\(\\small do(A1, A2)\\)), il n’y a plus de chemin entre A2 et Y. Il peut par contre y avoir une modification de l’effet \\(\\small A1 \\rightarrow Y\\) par A2 (\\(\\small do(A1)\\)). Dans ce cas, pour estimer cet effet, L1 et L2 seront considérés comme des facteurs de confusion, mais pas L3. Références "],["la-question-des-échelles.html", "Chapter 5 La question des échelles 5.1 Mesures des interactions 5.2 Lien entre les deux échelles 5.3 Synthèse", " Chapter 5 La question des échelles 5.1 Mesures des interactions Echelle additive Une façon simple de mesurer l’interaction est de mesurer à quel point l’effet conjoint de deux facteurs est différents de la somme de leurs effets individuels [2] : \\(\\small DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small p_{11} - p_{10} - p_{01} + p_{00}\\) Exemple Mesure de l’interaction dans l’exemple 1 \\(\\small DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0)) = 0.8 - (0,3 + 0,1) = +0,4\\) soit \\(\\small p_{11} - p_{10} - p_{01} + p_{00} = 0,9 - 0,4 - 0,2 + 0,1 = +0,4\\) ou \\(\\small (p_{11} - p_{01}) - (p_{10} - p_{00}) = (0,9 - 0,2) - (0,4 - 0,1) = 0,7 - 0,3 = +0,4\\) ou \\(\\small (p_{11} - p_{10}) - (p_{01} - p_{00}) = (0,9 - 0,4) - (0,2 - 0,1) = 0,5 - 0,1 = +0,4\\) soit : Echelle multiplicative En cas d’outcome binaire, c’est souvent le RR ou l’OR qui est utilisé pour mesurer les effets. La mesure de l’interaction sur une échelle multiplicative serait donc [2] : \\(\\small \\frac{RR11}{RR10 \\times RR01}\\) soit \\(\\small \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Exemple Mesure de l’nteraction dans l’exemple 1 \\(\\small \\frac{RR(X\\cap V)}{RR(X| V=0)*RR(V|X=0)} = 9/(4 \\times 2) = \\times 1,1\\) soit \\(\\small\\frac{p_{11} / p_{00}}{(p_{10} + p_{01}) / p_{00}} = \\frac{0,9 / 0,1}{(0,4 \\times 0,2) / 0,1} = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{01}}{p_{10} / p_{00}} = \\frac{0,9 / 0,2}{0,4 / 0,1} = \\times 4,5 / \\times 4 = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{10}}{p_{01} / p_{00}} = \\frac{0,9 / 0,4}{0,2 / 0,1} = \\times 2,25 - \\times 2 = \\times 1,1\\) ou : 5.2 Lien entre les deux échelles Un apparent paradoxe Mesurer l’interaction sur une seule échelle peut être trompeur [6]. On peut fréquemment observer une interaction positive dans une échelle (par exemple \\(\\small p11 - p10 - p01 + p00 &gt; 0\\)) et négative dans l’autre (par exemple \\(\\small p11.p00 / p10.p01 &lt;1\\)). Exemple Dans cet exemple (on a juste modifié la probabilité \\(p_{11}\\), on observe une interaction additive positive (l’effet de X augmente de +20% quand V=1 par rapport à V=0) mais une interaction multiplicative négative (l’effet de X est multiplié par 0,9 - donc diminue - quand V=1 par rapport à V=0). Il a même été démontré que si on n’observe pas d’interaction sur une échelle, alors on en observera obligatoirement sur l’autre échelle… [2]. Exemple Dans cet exemple, il n’y a pas d’interaction multiplicative (effet de X identique quelque soit V), mais sur l’echelle additive, on observe une interaction positive. et dans cet autre exemple, il n’y a pas d’interaction additive (effet de X identique quelque soit V), mais sur l’echelle multiplicative, on observe une interaction négative. Le continuum Dans un article de 2019 [7], Vanderweele décrit le continuum existant entre les 2 échelles. Par exemple, avec deux expositions ayant un effet positif (qui augmentent le risque) sur l’outcome en l’absence de l’autre exposition, lorsque l’effet joint est très important, l’interaction est positive sur les 2 échelles. Mais lorsque la taille de l’effet joint diminue, l’interaction multiplicative devient négative alors que l’interaction additive reste positive. Puis, lorsque la taille de l’effet joint diminue encore, l’interaction devient négative sur les deux échelles. Interaction pure et qualitative Dans ce continuum, deux cas particuliers d’interaction peuvent être retrouvées : Interaction pure de X en fonction de V, si X n’a un effet que dans une strate de V. Par exemple, \\(\\small p_{10} = p_{00}\\) et \\(\\small p_{11} ≠ p_{01}\\) Par exemple ici, V a un effet si X=0 mais pas si X=1 : Interaction qualitative de X1 en fonction de X2, , si l’effet de X1 dans une strate de X2 va dans la direction opposée de l’autre strate de X2 Par exemple ici, V a un effet positif si X=0 mais négatif si X=1 : 5.3 Synthèse Quelle échelle choisir pour mesurer un effet d’interaction ? Même si en pratique l’échelle multiplicative est plus utilisée en raison de l’utilisation des modèles logistiques [8], il semble y avoir un consensus pour privilégier l’échelle additive, plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Si on reprend l’exemple ci dessous : X représente un traitement dont on ne dispose que de 100 doses et Y un outcome de santé favorable (guérison). Il faut choisir si on donne 100 doses au groupe V = 0 ou au groupe V = 1. Si on donne 100 doses au groupe V = 0, 30 personnes seront guéries grace au traitement (30 personnes de plus que l’évolution naturelle, X=0) contre 50 personnes si on les donne au groupe V = 1. Donc il est préférable d’allouer les doses au groupe V=1. Pourtant si on avait réfléchi à partir de l’échelle multiplicative, on aurait choisi le groupe V=0 car l’effet du traitement est de RR=4 dans le groupe V = 0 et RR=3,5 dans le groupe v = 1… On peut donc conclure à un effet multiplicatif plus fort d’un traitement dans un groupe alors qu’en terme d’utilité (nombre de personnes favorablement impactées), l’échelle additive nous conduirait à choisir l’autre groupe… Idéalement, les interactions devraient cependant être reportées sur les 2 échelles [8] [2]. Références "],["types-de-paramètres.html", "Chapter 6 Types de paramètres 6.1 Avec les différences de risques (DR) 6.2 Avec les risques relatifs (RR) 6.3 Avec les Odds Ration (OR) 6.4 Excès de risque à partir des RR (RERI) 6.5 Autres", " Chapter 6 Types de paramètres 6.1 Avec les différences de risques (DR) On a déjà défini un paramètre d’interaction sur l’échelle additive (AI) à partir des différences d’effets [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) 6.2 Avec les risques relatifs (RR) On a aussi défini un paramètre d’interaction sur l’échelle multiplicative (MI) à partir des risques relatifs [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) 6.3 Avec les Odds Ration (OR) Souvent en épidémiologie, lorsque l’outcome Y est binaire, les effets sont mesurés par des odds ratio estimé à partir de modèle de régression logistique. Un paramètre d’interaction sur l’echelle multiplicative (MI_{OR}) peut être estimé à partir de ces OR [2] : \\(\\small MI_{OR} = \\frac{OR_{11}}{OR_{10} \\times OR_{01}}\\) En général, la mesure \\(MI_{OR}\\) et \\(MI_{RR}\\) seront proches si l’outcome est rare [2]. 6.4 Excès de risque à partir des RR (RERI) Lorsque seulement les risques relatifs sont donnés mais que l’on souhaite évaluer l’interaction sur l’échelle additive, “l’excès de risque du à l’interaction” (RERI) ou “interaction contrast ratio” (ICR), peut être estimé à partir des risques relatifs [2] : \\(\\small RERI = RR_{11} - RR_{10} - RR_{01} + 1\\) Il faut noter que, bien que le RERI donne la direction direction (positive, négative ou nulle) de l’interaction additive, nous ne pouvons pas utiliser le RERI pour évaluer l’ampleur de l’interaction additive, à moins de connaître au moins \\(\\small p_{00}\\). Si l’on a seulement l’OR et que l’outcome est rare, les OR peuvent approximé les RR, on a donc : \\(\\small RERI_{OR} = OR_{11} - OR_{10} - OR_{01} + 1 \\approx RERI_{RR}\\) 6.5 Autres D’autres paramètres ont aussi été proposé [2], tels que : Le “Synergie index” (SI) Il s’agit d’un paramètre explorant l’interaction additive : \\(\\small S = \\frac{RR_{11} - 1}{(RR_{10} - 1) + (RR_{01}-1)}\\). Il mesure à quel point le rapport de risque joint dépasse 1, et si cette mesure est supérieure à la somme de “à quel point” les rapports de risque de chaque exposition dépasse 1. Si le dénominateur est positif: si S &gt; 1, alors \\(\\small RERI_{RR}\\) &gt; 0 si S &lt; 1, alors \\(\\small RERI_{RR}\\) &lt; 0 L’interprétation de l’indice de synergie devient difficile dans les cas où l’effet de l’une des expositions est négatif et que le dénominateur de S est donc inférieur à 1. 6.5.1 Proportion attribuable (AP) Il s’agit aussi d’un paramètre explorant l’interaction additive : \\(\\small AP = \\frac{RR_{11} - RR_{10} - RR_{01} + 1}{RR_{11}}\\). Ce paramètre mesure la proportion du risque dans le groupe doublement exposé qui est due à l’interaction. L’AP est en lien avec le \\(\\small RERI_{RR}\\) : AP &gt; 0 si et seulement si \\(\\small RERI_{RR}\\) &gt; 0 AP &lt; 0 si et seulement si \\(\\small RERI_{RR}\\) &lt; 0. En fait \\(\\small AP = \\frac{RERI_{RR}}{RR_{11}-1}\\). Références "],["estimations.html", "Chapter 7 Estimations 7.1 Simulations 7.2 Estimation par régression 7.3 Estimation par G-computation 7.4 Estimation par Modèle Structurel Marginal 7.5 Estimation avec TMLE", " Chapter 7 Estimations 7.1 Simulations On simule des données selon le DAG suivant (toutes les variables sont binaires): rm(list=ls()) param.causal.model &lt;- function(p_L1 = 0.50, p_L2 = 0.20, p_L3 = 0.70, # baseline confounders b_A1 = 0.10, b_L1_A1 = 0.15, b_L2_A1 = 0.25, # modèle de A1 b_A2 = 0.15, b_L1_A2 = 0.20, b_L3_A2 = 0.20, # modèle de A2 b_Y = 0.10, # modèle de Y b_L1_Y = 0.02, b_L2_Y = 0.02, b_L3_Y = -0.02, b_A1_Y = 0.3, b_A2_Y = 0.1, b_A1A2_Y = 0.4 ) { # &lt;- effet d&#39;interaction Delta) # coefficients pour simuler l&#39;exposition # exposition A1 # vérif try(if(b_A1 + b_L1_A1 + b_L1_A1 &gt; 1) stop(&quot;la somme des coefficient du modèle A1 dépasse 100%&quot;)) # exposition A2 # vérif try(if(b_A2 + b_L1_A2 + b_L3_A2 &gt; 1) stop(&quot;la somme des coefficients du modèle A2 dépasse 100%&quot;)) # coefficients pour simuler l&#39;outcome, vérif try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &gt; 1) stop(&quot;la somme des coefficients du modèle Y dépasse 100%&quot;)) try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &lt; 0) stop(&quot;la somme des coefficients du modèle Y est inférieure à 0%&quot;)) coef &lt;- list(c(p_L1 = p_L1, p_L2 = p_L2, p_L3 = p_L3), c(b_A1 = b_A1, b_L1_A1 = b_L1_A1, b_L2_A1 = b_L2_A1), c(b_A2 = b_A2, b_L1_A2 = b_L1_A2, b_L3_A2 = b_L3_A2), c(b_Y = b_Y, b_L1_Y = b_L1_Y, b_L2_Y = b_L2_Y, b_L3_Y = b_L3_Y, b_A1_Y = b_A1_Y, b_A2_Y = b_A2_Y, b_A1A2_Y = b_A1A2_Y)) return(coef) } generate.data &lt;- function(N, b = param.causal.model()) { L1 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L1&quot;]) L2 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L2&quot;]) L3 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L3&quot;]) A1 &lt;- rbinom(N, size = 1, prob = b[[2]][&quot;b_A1&quot;] + (b[[2]][&quot;b_L1_A1&quot;] * L1) + (b[[2]][&quot;b_L2_A1&quot;] * L2)) A2 &lt;- rbinom(N, size = 1, prob = b[[3]][&quot;b_A2&quot;] + (b[[3]][&quot;b_L1_A2&quot;] * L1) + (b[[3]][&quot;b_L3_A2&quot;] * L3)) Y &lt;- rbinom(N, size = 1, prob = (b[[4]][&quot;b_Y&quot;] + (b[[4]][&quot;b_L1_Y&quot;] * L1) + (b[[4]][&quot;b_L2_Y&quot;] * L2) + (b[[4]][&quot;b_L3_Y&quot;] * L3) + (b[[4]][&quot;b_A1_Y&quot;] * A1) + (b[[4]][&quot;b_A2_Y&quot;] * A2) + (b[[4]][&quot;b_A1A2_Y&quot;] * A1 * A2)) ) data.sim &lt;- data.frame(L1, L2, L3, A1, A2, Y) return(data.sim) } #### On simule une base de données set.seed(12345) # b = param.causal.model(b_A1A2_Y = -0.45) b = param.causal.model() df &lt;- generate.data(N = 10000, b = b) summary(df) prop.table(table(df$Y, df$A1, df$A2, deparse.level = 2)) Au final, on a comme P(Y==1) dans chaque catégorie : A2 label levels value 0 A1 0 0.10 (0.30) 0 1 0.41 (0.49) 1 A1 0 0.20 (0.40) 1 1 0.90 (0.30) 7.2 Estimation par régression 7.2.1 Multiplicatif (régression logistique) names DR A1|A2=0 5.79 (4.99-6.72, p&lt;0.001) A2|A1=0 2.12 (1.86-2.43, p&lt;0.001) Interaction 5.96 (4.54-7.90, p&lt;0.001) Attention, les modèles de régressions logistiques sont biaisés car les données sont générées à partir de modèles additifs. 7.2.2 Additif (régression lineaire) names OR A1|A2=0 0.30 (0.28 to 0.32, p&lt;0.001) A2|A1=0 0.09 (0.08 to 0.11, p&lt;0.001) Interaction 0.39 (0.36 to 0.43, p&lt;0.001) 7.3 Estimation par G-computation Il s’agit d’une “G-methods” aussi appelée “standardisation” par Hernàn. ## 1.a) on crée 4 tables correspondant aux 4 interventions contrefactuelles df.A1_0.A2_0 &lt;- df.A1_1.A2_0 &lt;- df.A1_0.A2_1 &lt;- df.A1_1.A2_1 &lt;- df df.A1_0.A2_0$A1 &lt;- df.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) df.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) df.A1_1.A2_1$A1 &lt;- df.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) ## 1.b) on modélise le critère de jugement # model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;binomial&quot;) # modèle logistique biaisé (il y a des interactions avec les baseline) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;gaussian&quot;) # modèle non biaisé # en pratique la régression logistique n&#39;est pas tellement biaisée, # mais peut être car il n&#39;y a pas la place de mettre beaucoup de confusion # par rapport aux effets importants de A1 et A2 ? (10 fois plus grands) ## 1.c) on prédit le critère de jugement sous les interventions contrefactuelles Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = df.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = df.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = df.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = df.A1_1.A2_1, type = &quot;response&quot;) ## 1.d) on va enregistrer l&#39;ensemble des résultats pertinents dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 26, nrow = nlevels(as.factor(df$A1)) * nlevels(as.factor(df$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;,&quot;RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;,&quot;RR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_0.A2_0) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) # risk difference # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) # relative risk # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1)) ## 1.e) Intervalles de confiance par bootstrap set.seed(5678) B &lt;- 2000 bootstrap.est &lt;- data.frame(matrix(NA, nrow = B, ncol = 15)) colnames(bootstrap.est) &lt;- c(&quot;p.A1is0.A2is0&quot;, &quot;p.A1is1.A2is0&quot;, &quot;p.A1is0.A2is1&quot;, &quot;p.A1is1.A2is1&quot;, &quot;RD.A1.A2is0&quot;, &quot;RD.A1.A2is1&quot;, &quot;RD.A2.A1is0&quot;, &quot;RD.A2.A1is1&quot;, &quot;lnRR.A1.A2is0&quot;, &quot;lnRR.A1.A2is1&quot;, &quot;lnRR.A2.A1is0&quot;, &quot;lnRR.A2.A1is1&quot;, &quot;INT.a&quot;, &quot;lnRERI&quot;, &quot;lnINT.m&quot;) for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(df), replace=T) bootData &lt;- df[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) # model (unbiased in this case) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = bootData, # use BootData here +++ family = &quot;gaussian&quot;) # conterfactual data sets boot.A1_0.A2_0 &lt;- boot.A1_1.A2_0 &lt;- boot.A1_0.A2_1 &lt;- boot.A1_1.A2_1 &lt;- bootData boot.A1_0.A2_0$A1 &lt;- boot.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) boot.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) boot.A1_1.A2_1$A1 &lt;- boot.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) # predict potential outcomes under counterfactual scenarios Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = boot.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = boot.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = boot.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = boot.A1_1.A2_1, type = &quot;response&quot;) # save results in the bootstrap table bootstrap.est[b,&quot;p.A1is0.A2is0&quot;] &lt;- mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;p.A1is1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;p.A1is0.A2is1&quot;] &lt;- mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;p.A1is1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) bootstrap.est[b,&quot;RD.A1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;RD.A2.A1is0&quot;] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A2.A1is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;lnRR.A1.A2is0&quot;] &lt;- log(mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A1.A2is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1)) bootstrap.est[b,&quot;lnRR.A2.A1is0&quot;] &lt;- log(mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A2.A1is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0)) bootstrap.est[b,&quot;INT.a&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;lnRERI&quot;] &lt;- log((mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnINT.m&quot;] &lt;- log( (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1))) } # head(bootstrap.est) # summary(bootstrap.est) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * # exp(-1/2 * ((x-mean(bootstrap.est[,c])) / sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # ok, on a des belles lois normales dans les distributions bootstrap, tout va bien ! # pour les IC95%, je peux utiliser la déviation standard des distributions # pour des distributions plus asymétriques, on utiliserait plutôt les percentiles 2.5% et 97.5% # } # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) # A1 = 1 et A2 = 0 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) # A1 = 0 et A2 = 1 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) # A1 = 1 et A2 = 1 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) # risk difference # RD.A1.A2is0 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) # RD.A1.A2is1 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) # RD.A2.A1is0 int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) # RD.A2.A1is1 int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) # relative risk # RR.A1.A2is0 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) # RR.A1.A2is1 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) # RR.A2.A1is0 int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) # RR.A2.A1is1 int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) # additive interaction int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$INT.a) int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$INT.a) # RERI int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRERI)) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRERI)) # multiplicative interaction int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnINT.m)) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnINT.m)) Au final, on a : A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.197 [0.183,0.211] 0.092 [0.076,0.109] 1.89 [1.68,2.11] A1=1 \\(p_{10}\\)=0.405 [0.379,0.431] \\(p_{11}\\)=0.891 [0.87,0.912] 0.486 [0.453,0.519] 2.2 [2.06,2.36] RD.A1|A2 0.301 [0.273,0.329] 0.695 [0.67,0.72] RR.A1|A2 3.89 [3.48,4.34] 4.54 [4.21,4.89] Note: additive Interaction = 0.394 [0.358;0.43] RERI = 3.78 [3.38;4.23] multiplicative Interaction = 1.17 [1.02;1.33] 7.4 Estimation par Modèle Structurel Marginal # On récupère les Y prédit précédents, que l&#39;on fusionne Y &lt;- c(Y.A1_0.A2_0, Y.A1_1.A2_0, Y.A1_0.A2_1, Y.A1_1.A2_1) length(Y) # on aura une base de données de 40000 lignes # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(df.A1_0.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_0.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;))) # dim(X) ## Modèle structurel marginal msm.RD &lt;- glm(Y ~ A1 + A2 + A1:A2, data = data.frame(Y,X), family = &quot;gaussian&quot;) # ne pas ajuster sur les facteurs de confusion msm.RD ## tableau des effets marignaux results.MSM &lt;- matrix(NA, ncol = 4, nrow = 4) colnames(results.MSM) &lt;- c(&quot;A2 = 0&quot;, &quot;A2 = 1&quot;, &quot;RD within strata of A1&quot;, &quot;RR within strata of A1&quot;) rownames(results.MSM) &lt;- c(&quot;A1 = 0&quot;, &quot;A1 = 1&quot;, &quot;RD within strata of A2&quot;, &quot;RR within strata of A2&quot;) # 4 risques marginaux results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A1&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] + msm.RD$coefficients[&quot;A1&quot;] + msm.RD$coefficients[&quot;A1:A2&quot;] # within strata of A2 results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] # within strata of A1 results.MSM[&quot;A1 = 0&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 0&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM &lt;- round(results.MSM,3) RD.interaction &lt;- msm.RD$coefficients[&quot;A1:A2&quot;] RR.interaction &lt;- (results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;]) / ( results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] ) Au final, on a (sans les IC): A2 = 0 A2 = 1 RD within strata of A1 RR within strata of A1 A1 = 0 0.099 0.198 0.099 2.008 A1 = 1 0.409 0.904 0.494 2.208 RD within strata of A2 0.311 0.705 NA NA RR within strata of A2 4.146 4.560 NA NA Note: additive Interaction = 0.395 multiplicative Interaction = 1.11 7.5 Estimation avec TMLE ## 3- int.ltmleMSM() pour estimer les différentes quantités d&#39;intérêt, ### par gcomputation, IPTW ou tmle int.ltmleMSM &lt;- function(data = data, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, final.Ynodes = final.Ynodes, SL.library = list(Q=&quot;SL.glm&quot;, g=&quot;SL.glm&quot;), gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 2000, boot.seed = 12345) { # regime= # binary array: n x numAnodes x numRegimes of counterfactual treatment or a list of &#39;rule&#39; functions regimes.MSM &lt;- array(NA, dim = c(nrow(data), 2, 4)) # 2 variables d&#39;exposition (A1, A2), 4 régimes d&#39;exposition (0,0) (1,0) (0,1) (1,1) regimes.MSM[,,1] &lt;- matrix(c(0,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé ni à A1, ni à A2 regimes.MSM[,,2] &lt;- matrix(c(1,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 uniquement regimes.MSM[,,3] &lt;- matrix(c(0,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A2 uniquement regimes.MSM[,,4] &lt;- matrix(c(1,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 et à A2 # summary.measures = valeurs des coefficients du MSM associés à chaque régime # array: num.regimes x num.summary.measures x num.final.Ynodes - # measures summarizing the regimes that will be used on the right hand side of working.msm # (baseline covariates may also be used in the right hand side of working.msm and do not need to be included in summary.measures) summary.measures.reg &lt;- array(NA, dim = c(4, 3, 1)) summary.measures.reg[,,1] &lt;- matrix(c(0, 0, 0, # aucun effet ni de A1, ni de A2 1, 0, 0, # effet de A1 isolé 0, 1, 0, # effet de A2 isolé 1, 1, 1), # effet de A1 + A2 + A1:A2 ncol = 3, nrow = 4, byrow = TRUE) colnames(summary.measures.reg) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A1:A2&quot;) if(gcomp == TRUE) { # test length SL.library$Q SL.library$Q &lt;- ifelse(length(SL.library$Q) &gt; 1, &quot;SL.glm&quot;, SL.library$Q) # simplify SL.library$g because g functions are useless with g-computation SL.library$g &lt;- &quot;SL.mean&quot; iptw.only &lt;- FALSE } ltmle_MSM &lt;- ltmleMSM(data = data, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res &lt;- data.frame(&quot;beta.Intercept&quot; = rep(NA, B), &quot;beta.A1&quot; = rep(NA, B), &quot;beta.A2&quot; = rep(NA, B), &quot;beta.A1A2&quot; = rep(NA, B)) if(gcomp == TRUE) { set.seed &lt;- boot.seed for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) bootData &lt;- data[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) boot_ltmle_MSM &lt;- ltmleMSM(data = bootData, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res$beta.Intercept[b] &lt;- boot_ltmle_MSM$beta[&quot;(Intercept)&quot;] bootstrap.res$beta.A1[b] &lt;- boot_ltmle_MSM$beta[&quot;A1&quot;] bootstrap.res$beta.A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A2&quot;] bootstrap.res$beta.A1A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A1:A2&quot;] } } return(list(ltmle_MSM = ltmle_MSM, bootstrap.res = bootstrap.res)) } ### 4- summary.int() pour enregistrer l&#39;ensemble des estimations summary.int &lt;- function(data = data, ltmle_MSM = ltmle_MSM, estimator = c(&quot;gcomp&quot;, &quot;iptw&quot;, &quot;tmle&quot;)) { if(estimator == &quot;gcomp&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == FALSE) stop(&quot;The ltmle function did not use the gcomp estimator, but the iptw +/- tmle estimator&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta } if(estimator == &quot;iptw&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, iptw is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta.iptw IC &lt;- ltmle_MSM$ltmle_MSM$IC.iptw } if(estimator == &quot;tmle&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, tmle is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta IC &lt;- ltmle_MSM$ltmle_MSM$IC } # on va enregitrer l&#39;ensemble des résultats pertinent dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 34, nrow = nlevels(as.factor(data$A1)) * nlevels(as.factor(data$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;sd.p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;sd.RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;, &quot;RD.A2&quot;,&quot;sd.RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;sd.lnRR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;, &quot;RR.A2&quot;,&quot;sd.lnRR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;sd.a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;sd.lnRERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;sd.ln.m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # on peut retrouver les IC95% par delta method # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;]) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;]) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A2&quot;]) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;] + beta[&quot;A2&quot;] + beta[&quot;A1:A2&quot;]) # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) ## IC95% if(estimator == &quot;iptw&quot; | estimator == &quot;tmle&quot;) { # A1 = 0 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]),0,0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]),0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 grad &lt;- rep(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), 4) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0, 1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI grad &lt;- c((int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } if(estimator == &quot;gcomp&quot;) { ltmle_MSM$bootstrap.res$p.A1_0.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept) ltmle_MSM$bootstrap.res$p.A1_1.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1) ltmle_MSM$bootstrap.res$p.A1_0.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A2) ltmle_MSM$bootstrap.res$p.A1_1.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1 + ltmle_MSM$bootstrap.res$beta.A2 + ltmle_MSM$bootstrap.res$beta.A1A2) ltmle_MSM$bootstrap.res$RD.A1.A2_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A1.A2_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 ltmle_MSM$bootstrap.res$RD.A2.A1_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_0.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A2.A1_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 ltmle_MSM$bootstrap.res$lnRR.A1.A2_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_0 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A1.A2_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_1) ltmle_MSM$bootstrap.res$lnRR.A2.A1_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_0.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A2.A1_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_1.A2_0) ltmle_MSM$bootstrap.res$a.INT &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$lnRERI &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$ln.m.INT &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 * ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / (ltmle_MSM$bootstrap.res$p.A1_1.A2_0 * ltmle_MSM$bootstrap.res$p.A1_0.A2_1)) # A1 = 0 et A2 = 0 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_0) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_0) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_1) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_1) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_0) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_1) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_0) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_1) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_0) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_1) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_0) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_1) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$a.INT) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRERI) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$ln.m.INT) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } return(list(int.r = int.r, bootstrap.res = bootstrap.res)) } ### Obtention du MSM par la fonction ltmle, estimation par gcomp, iptw ou tmle # avec la fonction int.ltmleMSM() # on définit les arguments de la fonction ltmleMSM du package ltmle library(ltmle) library(SuperLearner) ## arguments à renseigner Q_formulas = c(Y=&quot;Q.kplus1 ~ L1 + L2 + L3 + A1 * A2&quot;) # useful to add A1 * A2 interaction here g_formulas = c(&quot;A1 ~ L1 + L2&quot;, &quot;A2 ~ L1 + L3&quot;) SL.library = list(Q=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;), g=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;)) ### estimation par IPTW et TMLE interaction.ltmle &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, SL.library = SL.library, gcomp = FALSE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;) ### estimation par g-computation # par défaut, il fait une régression logistique à partir de la formule Q_formulas # si on veut faire un régression linéaire pour le modèle additif, on peut créer une fonction de SuperLearner # à partir de la fonction SL.glm SL.glm.gaussian &lt;- function (Y, X, newX, family = &quot;gaussian&quot;, # tout est comme SL.glm, sauf cette famille &quot;gaussian&quot; obsWeights, model = TRUE, ...) { if (is.matrix(X)) { X = as.data.frame(X) } fit.glm &lt;- glm(Y ~ ., data = X, family = family, weights = obsWeights, model = model) if (is.matrix(newX)) { newX = as.data.frame(newX) } pred &lt;- predict(fit.glm, newdata = newX, type = &quot;response&quot;) fit &lt;- list(object = fit.glm) class(fit) &lt;- &quot;SL.glm&quot; out &lt;- list(pred = pred, fit = fit) return(out) } environment(SL.glm.gaussian) &lt;-asNamespace(&quot;SuperLearner&quot;) interaction.gcomp &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, # SL.library = SL.library, SL.library = list(Q=&quot;SL.glm.gaussian&quot;, # g=&quot;SL.mean&quot;), gcomp = TRUE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 1000, # nombre d&#39;échantillons bootstrap boot.seed = 54321) # seed pour l&#39;échantillonnage bootstrap ### 3) Calcul des paramètres utiles pour l&#39;analyse de l&#39;interaction # avec la fonction summary.int() ### récupération des résultats tmle summary.tmle &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;tmle&quot;)) # summary.tmle$int.r ### récupération des résultats iptw summary.iptw &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;iptw&quot;)) # summary.iptw$int.r ### récupération des résultats gcomputation summary.gcomp &lt;- summary.int(data = df, ltmle_MSM = interaction.gcomp, estimator = c(&quot;gcomp&quot;)) # summary.gcomp$int.r # head(summary.gcomp$bootstrap.res) # # vérifier la normalité des estimations bootstrap # bootstrap.est &lt;- subset(summary.gcomp$bootstrap.res, # select = # c(&quot;p.A1_0.A2_0&quot;, # &quot;p.A1_1.A2_0&quot;, # &quot;p.A1_0.A2_1&quot;, # &quot;p.A1_1.A2_1&quot;, # &quot;RD.A1.A2_0&quot;, # &quot;RD.A1.A2_1&quot;, # &quot;RD.A2.A1_0&quot;, # &quot;RD.A2.A1_1&quot;, # &quot;lnRR.A1.A2_0&quot;, # &quot;lnRR.A1.A2_1&quot;, # &quot;lnRR.A2.A1_0&quot;, # &quot;lnRR.A2.A1_1&quot;, # &quot;a.INT&quot;, # &quot;lnRERI&quot;, # &quot;ln.m.INT&quot;)) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * exp(-1/2*((x-mean(bootstrap.est[,c]))/sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } Au final, on a (présentation selon recommandation Knol et al. [8]): 7.5.1 TMLE ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] ## A1=1 $p_{10}$=0.408 [0.378,0.439] $p_{11}$=0.903 [0.88,0.927] ## RD.A1|A2 0.304 [0.272,0.336] 0.708 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.091 [0.073,0.109] 1.88 [1.67,2.11] ## A1=1 0.495 [0.457,0.534] 2.21 [2.04,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.404 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.89 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; 7.5.2 IPTW ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] ## A1=1 $p_{10}$=0.408 [0.377,0.439] $p_{11}$=0.904 [0.88,0.927] ## RD.A1|A2 0.304 [0.272,0.336] 0.709 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.091 [0.073,0.109] 1.88 [1.67,2.11] ## A1=1 0.496 [0.457,0.535] 2.22 [2.05,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.405 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.9 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; 7.5.3 G-computation ## $out.table ## A2=0 A2=1 ## A1=0 $p_{00}$=0.104 [0.095,0.112] $p_{01}$=0.197 [0.183,0.211] ## A1=1 $p_{10}$=0.4 [0.373,0.427] $p_{11}$=0.893 [0.872,0.915] ## RD.A1|A2 0.296 [0.267,0.325] 0.697 [0.671,0.722] ## RR.A1|A2 3.86 [3.45,4.32] 4.54 [4.46,4.61] ## RD.A2|A1 RR.A2|A1 ## A1=0 0.093 [0.076,0.11] 1.9 [1.69,2.13] ## A1=1 0.494 [0.459,0.528] 2.23 [2.08,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.4 [0.362;0.439]&quot; ## [2] &quot;RERI = 3.86 [3.46;4.31]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.35]&quot; Références "],["représentations-graphiques.html", "Chapter 8 Représentations graphiques", " Chapter 8 Représentations graphiques "],["présentation-des-résultats.html", "Chapter 9 Présentation des résultats 9.1 Modification d’effet 9.2 Interaction 9.3 Proposition", " Chapter 9 Présentation des résultats Les recommandation de Knol et al. [8] sont : 9.1 Modification d’effet Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC pour A1 dans les strates de A2. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y a été ajustée. 9.2 Interaction Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC de l’effet de A1 sur Y dans les strates de A2 et de A2 sur Y dans les strates de A1. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y et la relation entre A2 et Y ont été ajustées. 9.3 Proposition Présenter les effets marginaux ou les proportions dans chaque strate présenter les effets dans chaque strate dans une échelle multiplicative et additive Références "],["steps.html", "Chapter 10 Steps", " Chapter 10 Steps formuler l’objectif predictif ou explicatif interaction ou modification d’effet? DAG, estimand, estimateur Description tableau croisé Analyses exploratoires régressions avec terme d’interaction analyses stratifiées marge Analyses confirmatoire g computation MSM "],["exemple-1---y-binaire.html", "Chapter 11 Exemple 1 - Y binaire", " Chapter 11 Exemple 1 - Y binaire "],["exemple-2---y-quantitatif.html", "Chapter 12 Exemple 2 - Y quantitatif", " Chapter 12 Exemple 2 - Y quantitatif 1. Objectifs Dans cette étude [ref], on s’est intéressé, de façon explicative, à l’effet de la défavorisation sociale précoce sur le taux de cholestérol LDL vers 45 ans; mais aussi à l’effet du sexe sur ce taux de cholestérol en fonction de la défavorisation sociale précoce. Ici on s’intéresse donc à deux modifications d’effet. 2. DAG, estimand, estimateur Le DAG (sans les médiateurs) était : Les estimands étaient définis sur l’échelle additive par : \\(\\small (Y_{s=1|d=0} - Y_{s=0|d=0}) - (Y_{s=1|d=1} - Y_{s=0|d=1})\\) \\(\\small (Y_{d=1|s=0} - Y_{d=0|s=0}) - (Y_{d=1|s=1} - Y_{d=0|s=1})\\) Ils sont ici équivalents car il n’y pas de facteurs de confusion, donc, par exemple, \\(\\small Y_{d=1|s=0} = Y_{s=0|d=1} = Y_{d=1,s=0}\\) Les effets ont été estimés par g-computation (standardisation). 3. Résultats "],["synthèse-générale.html", "Chapter 13 Synthèse générale", " Chapter 13 Synthèse générale La première étape importantes consiste à définir précisément l’objectif. Et, si l’on est dans une démarche explicative, d’inférence causale, il s’agit de définir si la mesure d’un effet d’interaction est nécessaire pour y répondre (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Le fait de choisir une démarche d’analyse d’interaction ou de modification d’effet repose sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Concernant le choix de l’échelle, idéalement, les interactions devraient être reportées sur les 2 échelles [8] [2]. Cependant, l’échelle additive est plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Concernant les paramètres, Références "],["pour-aller-plus-loin.html", "Chapter 14 Pour aller plus loin… 14.1 Ajouter de la complexité 14.2 Interaction avec confusion intermédiaire 14.3 Interaction et médiation", " Chapter 14 Pour aller plus loin… 14.1 Ajouter de la complexité A1 et A2 sont rarement indépendants. Scénario plus probable : 14.2 Interaction avec confusion intermédiaire 14.3 Interaction et médiation [9] [10] Références "],["références.html", "Chapter 15 Références", " Chapter 15 Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["index.html", "Interactions et modifications d’effet en Epidémiologie Chapter 1 Présentation", " Interactions et modifications d’effet en Epidémiologie CERPOP, INSERM, EQUITY Team Last compiled on 04 May, 2023 Chapter 1 Présentation Ce document a été rédigé en tant que document de synthèse du travail du groupe “Interaction” de l’équipe EQUITY, CERPOP. Ce travail a consisté en une revue de la littérature et en une application détaillée des méthodes sur des analyses illustratives, dans un but d’auto-formation et pédagogique. Les participant.e.s du groupe de travail sont : Hélène COLINEAUX Léna BONIN Camille JOANNES Benoit LEPAGE Lola NEUFCOURT Ainhoa UGARTECHE The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapter 2 Introduction 2.1 Quand étudier les interactions ? 2.2 Les points les plus importants", " Chapter 2 Introduction Comment telle prédisposition génétique et telle exposition environnementale inter-agissent-elles ? L’effet de tel traitement varie-t-il selon les circonstances ? Selon les caractéristiques du patient ? Telle intervention peut-elle être bénéfique pour un groupe social et délétère pour un autre ? De nombreuses questions épidémiologiques impliquent des mécanismes d’interactions ou de modifications d’effet. Pourtant, étudier ces mécanismes restent encore complexe aujourd’hui sur le plan méthodologique : quelle démarche adopter ? sur quelle échelle mesurer cette interaction ? comment interpréter les coefficients ? et cetera. Dans ce document, nous proposons une synthèse de la littérature et une démarche progressive et appliquée pour explorer ces questions. 2.1 Quand étudier les interactions ? 2.1.1 Prediction versus causalité La science des données cherche à répondre à 3 types d’objectifs [1] : Selon le type d’objectif, la démarche d’analyse et les enjeux méthodologiques ne vont pas être les mêmes. Si l’objectif est prédictif, la démarche va être centrée sur la prédiction de l’outcome, à partir de covariables sélectionnées afin d’optimiser la précision de l’estimation, tout en prenant en compte leur disponibilité en pratique et la parcimonie du modèle. Dans une démarche explicative, ou étiologique, au contraire, la démarche va être centré sur l’estimation d’un effet causal, en prenant en compte les covariables en fonction de leur rôle vis-à-vis de l’effet d’intérêt (facteurs de confusion, colliders, médiateurs…). En épidémiologie, à l’exception des cas où l’on souhaite développer un test ou score diagnostic ou pronostic, les objectifs sont le plus souvent explicatifs. On cherche en effet, la plupart du temps, à identifier des liens de cause à effet, afin de pouvoir agir sur les causes pour modifier les effets. Finalement, pour répondre à la question “quand doit-on prendre en compte les interactions ?”, il est d’abord nécessaire d’identifier dans quel type de démarche l’on s’inscrit : Démarche prédictive : on ajoutera alors les interactions dans le modèle de prédiction, pour le rendre plus flexible, si cela améliore la précision de l’estimation [2]. Démarche explicative/étiologique : on étudiera les interactions ou modifications d’effet, si cela répond directement à l’objectif. Par exemple : Si l’objectif est du type “l’effet de X sur Y varie-t-il en fonction de V ?”, on prendra en compte l’interactions entre X et V. Les objectifs qui nécessitent la prise en compte de l’interaction peuvent aussi être du type : “Quel est l’effet conjoint de X et V sur Y ?” ou “Quel part de l’effet de X sur Y disparaît quand V est modifié ?”, etc. Par contre, si l’objectif est simplement d’estimer l’effet de X sur Y, ou l’effet médié par M, la prise en compte des interactions entre X et des covariables (facteurs de confusion ou médiateurs) n’est pas indispensable. C’est l’effet “moyen” qui sera estimé. Des termes d’interactions peuvent cependant être ajoutés (mais non interprétés), si cela améliore la précision de l’estimation (enjeu d’optimisation du modèle). 2.1.2 Types d’objectifs Dans ce document, nous nous intéresserons principalement aux interactions et modifications d’effet dans une démarche étiologique/ explicative. Les objectifs pouvant nécessiter l’étude de l’interaction/modification d’effet sont [2] : Cibler des sous-groupes. Par exemple, identifier des sous-groupes pour lesquels l’intervention aura le plus d’effet afin de pouvoir cibler l’intervention en cas de ressources limitées, ou s’assurer que l’intervention est bénéfice pour tous les groupes et pas délétères pour certains groupes. Explorer les mécanismes d’un effet. Par exemple, en cas d’intervention qui n’a d’effet qu’en présence ou absence d’une caractéristiques particulière (définition mécanistique de l’interaction) ou seulement conjointement à une autre intervention. Etudier l’effet d’une intervention pour éliminer une partie de l’effet d’une exposition non modifiable. Par exemple, quelle part de l’effet du niveau d’éducation des parents sur la mortalité disparaîtrait si on intervenait sur le tabagisme à l’adolescence ? 2.2 Les points les plus importants La première étape importante consiste à définir précisément l’objectif : L’objectif est-il de type descriptif, prédictif ou explicatif ? Si l’on est dans une démarche explicative, d’inférence causale, est-ce que la mesure d’un effet d’interaction est nécessaire pour y répondre ? (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Ensuite, de nombreuses questions se posent pour réaliser une analyse d’interaction, auxquelles nous tentons de répondre dans ce document : S’agit-il d’une interaction ou une modification d’effet ? Sur quelle échelle la mesure-t-on ? Un effet d’interaction peut en effet être défini sur une échelle multiplicative ou additive, et les résultats entre ces échelles peuvent être contradictoires. Comment estimer cette interaction ? Quels paramètres présenter et comment les interpréter ? Comment la représenter graphiquement ? Références "],["notations.html", "Chapter 3 Notations 3.1 Variables et probabilités 3.2 Mesures d’effets", " Chapter 3 Notations 3.1 Variables et probabilités On note : un outcome : \\(\\small Y\\), deux expositions : \\(\\small X\\) et \\(\\small V\\) La probabilité de l’outcome Y dans chaque strate définie par les 2 expositions est notée : \\(\\small p_{xv} = P(Y = 1|X = x,V = v)\\) Exemple On a deux exposition \\(\\small X\\), le tabagisme actif à 20 ans, et \\(\\small V\\), le fait d’avoir vécu un évènement traumatique pendant l’enfance. L’outcome \\(\\small Y\\) est binaire et représente le fait d’avoir au moins une pathologie chronique à 60 ans \\(\\small Y=1\\) ou aucune \\(\\small Y=0\\). On décrit (données complètement fictives) : Interprétation : La probabilité d’avoir au moins une pathologie chronique à 60 ans quand on n’a pas vécu d’événement traumatique pendant l’enfance et pas fumé à 20 ans est de 10%, tandis qu’elle est de 90% quand on a vécu un événement traumatique et fumé. 3.2 Mesures d’effets L’effet d’une variable \\(\\small X\\) sur \\(\\small Y\\) peut être mesuré sur deux échelles : additive (différence de risque/probabilité) ou multiplicative (rapport de risque/probabilité). Concernant les différences de risques (DR, effets additifs) On a donc : L’effet d’un X binaire sur Y est : \\(\\small DR(X) = P(Y = 1|do(X = 1)) - P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1|X = 1) - P(Y = 1|X = 0) = p_1-p_0\\) L’effet conjoint de X et V est : \\(\\small DR(X,V) = p_{11}-p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small DR(X|V=0) = p_{10}-p_{00}\\) et \\(\\small DR(X|V=1) = p_{11}-p_{01}\\) Exemple Différences de risques pour l’exemple 1 \\(\\small DR(X \\cap V) = p_{11}-p_{00} = 0,9 - 0,1 = +0,8\\) \\(\\small DR (X | V=0) = p_{10}-p_{00} = 0,4 - 0,1 = +0,3\\) \\(\\small DR (X | V=1) = p_{11}-p_{01} = 0,9 - 0,2 = +0,7\\) Le fait d’être doublement exposé par rapport à pas du tout augmente le risque de +80%. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 augmente le risque de +30%, alors que parmi les personnes ayant vécu un événement traumatique, il est augmenté de +70%. Concernant, les rapports de risque (effets multiplicatifs) on peut notamment utiliser les risques relatifs (RR). On donc : L’effet d’un X binaire sur Y est : \\(\\small RR(X) = P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))= p_1 / p_0\\) L’effet conjoint de X et V est : \\(\\small RR(X,V) = p_{11}/p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small RR(X|V=0) = p_{10}/p_{00}\\) et \\(\\small RR(X|V=1) =p_{11}/p_{01}\\) Exemple Risques relatifs pour l’exemple 1 \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) Le risque quand on est doublement exposé par rapport à pas du tout est multiplié par 9. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 multiplie le risque par 4, alors que parmi les personnes ayant vécu un événement traumatique, il est multiplié par 4,5. "],["interaction-vs-modification-deffets.html", "Chapter 4 Interaction vs modification d’effets 4.1 Modification d’effets 4.2 Interaction 4.3 Synthèse", " Chapter 4 Interaction vs modification d’effets Dans le champ des analyses d’interaction, deux termes peuvent être rencontrés : “interaction” et “modification d’effet”. Quel est la différence entre ces deux termes ? 4.1 Modification d’effets La question de la modification d’effet consiste à d’identifier si l’effet du traitement ou de l’exposition est différent dans différents groupes de patients ayant des caractéristiques différentes (estimer l’effet d’une exposition séparément en fonction d’une autre variable) [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait 1 seule intervention mais que l’analyse est stratifiée sur V. On analyse donc l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de \\(\\small V\\). En observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X\\) mais pas celui de \\(\\small V\\). On ajustera sur les facteurs de confusion de \\(\\small X \\rightarrow Y\\). On ne fait pas d’hypothèse sur les mécanismes de la modification d’effet, qui peut être causale, de façon directe ou indirecte, ou pas du tout (par proxy ou cause commune) [4]. Exemples d’objectifs : identifier des groupes pour lesquels le traitement ne serait pas utile, ou si l’effet du traitement est homogène/hétérogène en fonction de l’âge, du sexe, etc. On a une modification de l’effet de X par V si l’effet de X est différent dans chaque strate définie par V: en additif : \\(\\small DR(X | V=0) ≠ DR(X | V=1)\\) soit \\(\\small p_{10}-p_{00} ≠ p_{11}-p_{01}\\) en multiplicatif : \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) soit \\(\\small p_{10}/p_{00} ≠ p_{11}/p_{01}1\\) Exemple Modification d’effet dans l’exemple 1 En additif : effet quand V=0 : \\(\\small DR (X | V=0) = 0,4 - 0,1 = +0,3\\) effet quand V=1 : \\(\\small DR (X | V=1) = 0,9 - 0,2 = +0,7\\) donc \\(\\small DR (X | V=0) ≠ DR (X | V=1)\\) En multiplicatif : effet quand V=0 : \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) effet quand V=1 : \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) donc \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) Ici l’effet du tabagisme est différent selon que les personnes ont vécu un événement traumatique ou non, sur l’échelle additive et multiplicative. On peut donc dire que le fait d’avoir vécu un événement traumatique modifie l’effet du tabac. Attention, on fait l’hypothèse de l’absence de facteurs de confusion entre le tabagisme et l’outcome, ce qui est en réalité peu probable. 4.2 Interaction Quand on s’intéresse à l’interaction, on s’intéresse plutôt à l’effet conjoints de 2 expositions (ou plus) sur un outcome. Il y a une interaction synergique si l’effet conjoint est supérieur à l’effet de la somme des individuels. Il y a une interaction antagoniste lorsque l’effet conjoint est inférieur à la somme des effets individuels [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y a plusieurs interventions selon le nombre de combinaison. On analyse donc l’effet du scénario \\(\\small do(X, V)\\). Ici l’effet causal d’interêt est vraiment l’effet conjoint des deux variables. Dans un schéma observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X*V\\). On ajustera sur les facteurs de confusion de \\(\\small X.V \\rightarrow Y\\). On fait l’hypothèse que les mécanismes de l’effet conjoint de X et V sont causaux. On a une interaction si : en additif : \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) \\(\\small p_{11}-p_{00} ≠ (p_{10}-p_{00})+(p_{01}-p_{00})\\) \\(\\small p_{11} ≠ p_{10} + p_{01} - p_{00}\\) en multiplicatif \\(\\small RR(X \\cap V) ≠ RR(X| V=0) + RR(V| X=0)\\) \\(\\small p_{11}/p_{00} ≠ (p_{10}/p_{00})+(p_{01}/p_{00})\\) \\(\\small p_{11} ≠ (p_{10} + p_{01}) / p_{00}\\) Exemple Interaction dans l’exemple 1 En additif : effet joint : \\(\\small DR(X \\cap V) = 0,9 - 0,1 = +0.8\\) somme des effets individuel : \\(\\small DR(X| V=0) + DR(V| X=0) = +0,3 +0,1 = +0,4\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) En multiplicatif : effet joint : \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) produit des effets individuel : \\(\\small RR(X | V=0) \\times RR(V | X=0) = 4 \\times 2 = \\times 8\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) \\times DR(V| X=0)\\) Ici l’effet joint des 2 expositions est supérieur à la somme ou au produit des effets individuels, il y a donc une interaction synergique entre les deux expositions. 4.3 Synthèse Mathématiquement, les formulations sont équivalentes : échelle additive: \\(\\small p_{10} -p_{00} ≠ p_{11}- p_{01} ⇔ p_{11}≠(p_{10}+p_{01})- p_{00}\\) échelle multiplicative : \\(\\small p_{10} /p_{00} ≠ p_{11}/ p_{01} ⇔ p_{11}≠(p_{10} \\times p_{01})/p_{00}\\) La différence se joue plutôt sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Il existe des cas où l’identification d’une interaction ou d’une modification d’effet ne conduira pas à la même démarche et donc au même résultat [5]. Prenons le DAG suivant : Dans ce cas, il n’y a pas d’interaction entre A1 et A2, car si on intervient sur les 2 (\\(\\small do(A1, A2)\\)), il n’y a plus de chemin entre A2 et Y. Il peut par contre y avoir une modification de l’effet \\(\\small A1 \\rightarrow Y\\) par A2 (\\(\\small do(A1)\\)). Dans ce cas, pour estimer cet effet, L1 et L2 seront considérés comme des facteurs de confusion, mais pas L3. Références "],["la-question-des-échelles.html", "Chapter 5 La question des échelles 5.1 Mesures des interactions 5.2 Lien entre les deux échelles 5.3 Synthèse", " Chapter 5 La question des échelles 5.1 Mesures des interactions Echelle additive Une façon simple de mesurer l’interaction est de mesurer à quel point l’effet conjoint de deux facteurs est différents de la somme de leurs effets individuels [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) Exemple Mesure de l’interaction dans l’exemple 1 \\(\\small DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0)) = 0.8 - (0,3 + 0,1) = +0,4\\) soit \\(\\small p_{11} - p_{10} - p_{01} + p_{00} = 0,9 - 0,4 - 0,2 + 0,1 = +0,4\\) ou \\(\\small (p_{11} - p_{01}) - (p_{10} - p_{00}) = (0,9 - 0,2) - (0,4 - 0,1) = 0,7 - 0,3 = +0,4\\) ou \\(\\small (p_{11} - p_{10}) - (p_{01} - p_{00}) = (0,9 - 0,4) - (0,2 - 0,1) = 0,5 - 0,1 = +0,4\\) soit : Echelle multiplicative En cas d’outcome binaire, c’est souvent le RR ou l’OR qui est utilisé pour mesurer les effets. La mesure de l’interaction sur une échelle multiplicative serait donc [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Exemple Mesure de l’nteraction dans l’exemple 1 \\(\\small \\frac{RR(X\\cap V)}{RR(X| V=0)*RR(V|X=0)} = 9/(4 \\times 2) = \\times 1,1\\) soit \\(\\small\\frac{p_{11} / p_{00}}{(p_{10} + p_{01}) / p_{00}} = \\frac{0,9 / 0,1}{(0,4 \\times 0,2) / 0,1} = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{01}}{p_{10} / p_{00}} = \\frac{0,9 / 0,2}{0,4 / 0,1} = \\times 4,5 / \\times 4 = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{10}}{p_{01} / p_{00}} = \\frac{0,9 / 0,4}{0,2 / 0,1} = \\times 2,25 - \\times 2 = \\times 1,1\\) ou : 5.2 Lien entre les deux échelles Un apparent paradoxe Mesurer l’interaction sur une seule échelle peut être trompeur [6]. On peut fréquemment observer une interaction positive dans une échelle (par exemple \\(\\small p11 - p10 - p01 + p00 &gt; 0\\)) et négative dans l’autre (par exemple \\(\\small p11.p00 / p10.p01 &lt;1\\)). Exemple Dans cet exemple (on a juste modifié la probabilité \\(p_{11}\\), on observe une interaction additive positive (l’effet de X augmente de +20% quand V=1 par rapport à V=0) mais une interaction multiplicative négative (l’effet de X est multiplié par 0,9 - donc diminue - quand V=1 par rapport à V=0). Il a même été démontré que si on n’observe pas d’interaction sur une échelle, alors on en observera obligatoirement sur l’autre échelle… [2]. Exemple Dans cet exemple, il n’y a pas d’interaction multiplicative (effet de X identique quelque soit V), mais sur l’echelle additive, on observe une interaction positive. et dans cet autre exemple, il n’y a pas d’interaction additive (effet de X identique quelque soit V), mais sur l’echelle multiplicative, on observe une interaction négative. Le continuum Dans un article de 2019 [7], Vanderweele décrit le continuum existant entre les 2 échelles. Par exemple, avec deux expositions ayant un effet positif (qui augmentent le risque) sur l’outcome en l’absence de l’autre exposition, lorsque l’effet joint est très important, l’interaction est positive sur les 2 échelles. Mais lorsque la taille de l’effet joint diminue, l’interaction multiplicative devient négative alors que l’interaction additive reste positive. Puis, lorsque la taille de l’effet joint diminue encore, l’interaction devient négative sur les deux échelles. Interaction pure et qualitative Dans ce continuum, deux cas particuliers d’interaction peuvent être retrouvées : Interaction pure de X en fonction de V, si X n’a un effet que dans une strate de V. Par exemple, \\(\\small p_{10} = p_{00}\\) et \\(\\small p_{11} ≠ p_{01}\\) Par exemple ici, V a un effet si X=0 mais pas si X=1 : Interaction qualitative de X1 en fonction de X2, , si l’effet de X1 dans une strate de X2 va dans la direction opposée de l’autre strate de X2 Par exemple ici, V a un effet positif si X=0 mais négatif si X=1 : 5.3 Synthèse Quelle échelle choisir pour mesurer un effet d’interaction ? Même si en pratique l’échelle multiplicative est plus utilisée en raison de l’utilisation des modèles logistiques [8], il semble y avoir un consensus pour privilégier l’échelle additive, plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Si on reprend l’exemple ci dessous : X représente un traitement dont on ne dispose que de 100 doses et Y un outcome de santé favorable (guérison). Il faut choisir si on donne 100 doses au groupe V = 0 ou au groupe V = 1. Si on donne 100 doses au groupe V = 0, 30 personnes seront guéries grace au traitement (30 personnes de plus que l’évolution naturelle, X=0) contre 50 personnes si on les donne au groupe V = 1. Donc il est préférable d’allouer les doses au groupe V=1. Pourtant si on avait réfléchi à partir de l’échelle multiplicative, on aurait choisi le groupe V=0 car l’effet du traitement est de RR=4 dans le groupe V = 0 et RR=3,5 dans le groupe v = 1… On peut donc conclure à un effet multiplicatif plus fort d’un traitement dans un groupe alors qu’en terme d’utilité (nombre de personnes favorablement impactées), l’échelle additive nous conduirait à choisir l’autre groupe… Idéalement, les interactions devraient cependant être reportées sur les 2 échelles [8] [2]. Références "],["types-de-paramètres.html", "Chapter 6 Types de paramètres 6.1 Avec les différences de risques (DR) 6.2 Avec les risques relatifs (RR) 6.3 Avec les Odds Ration (OR) 6.4 Excès de risque à partir des RR (RERI) 6.5 Autres", " Chapter 6 Types de paramètres Plusieurs paramètres peuvent être utilisés pour décrire une interaction, sur l’échelle additive ou multiplicative. 6.1 Avec les différences de risques (DR) On a déjà défini un paramètre d’interaction sur l’échelle additive (AI) à partir des différences d’effets [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) 6.2 Avec les risques relatifs (RR) On a aussi défini un paramètre d’interaction sur l’échelle multiplicative (MI) à partir des risques relatifs [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) 6.3 Avec les Odds Ration (OR) Souvent en épidémiologie, lorsque l’outcome Y est binaire, les effets sont mesurés par des odds ratio estimé à partir de modèle de régression logistique. Un paramètre d’interaction sur l’echelle multiplicative (MI_{OR}) peut être estimé à partir de ces OR [2] : \\(\\small MI_{OR} = \\frac{OR_{11}}{OR_{10} \\times OR_{01}}\\) En général, la mesure \\(\\small MI_{OR}\\) et \\(\\small MI_{RR}\\) seront proches si l’outcome est rare [2]. 6.4 Excès de risque à partir des RR (RERI) Lorsque seulement les risques relatifs sont donnés mais que l’on souhaite évaluer l’interaction sur l’échelle additive, “l’excès de risque du à l’interaction” (RERI) ou “interaction contrast ratio” (ICR), peut être estimé à partir des risques relatifs [2] : \\(\\small RERI = RR_{11} - RR_{10} - RR_{01} + 1\\) Il faut noter que, bien que le RERI donne la direction direction (positive, négative ou nulle) de l’interaction additive, nous ne pouvons pas utiliser le RERI pour évaluer l’ampleur de l’interaction additive, à moins de connaître au moins \\(\\small p_{00}\\). Si l’on a seulement l’OR et que l’outcome est rare, les OR peuvent approximé les RR, on a donc : \\(\\small RERI_{OR} = OR_{11} - OR_{10} - OR_{01} + 1 \\approx RERI_{RR}\\) 6.5 Autres D’autres paramètres ont aussi été proposé [2], tels que : Le “Synergie index” (SI) Il s’agit d’un paramètre explorant l’interaction additive : \\(\\small S = \\frac{RR_{11} - 1}{(RR_{10} - 1) + (RR_{01}-1)}\\). Il mesure à quel point le rapport de risque joint dépasse 1, et si cette mesure est supérieure à la somme de “à quel point” les rapports de risque de chaque exposition dépasse 1. Si le dénominateur est positif: si S &gt; 1, alors \\(\\small RERI_{RR}\\) &gt; 0 si S &lt; 1, alors \\(\\small RERI_{RR}\\) &lt; 0 L’interprétation de l’indice de synergie devient difficile dans les cas où l’effet de l’une des expositions est négatif et que le dénominateur de S est donc inférieur à 1. 6.5.1 Proportion attribuable (AP) Il s’agit aussi d’un paramètre explorant l’interaction additive : \\(\\small AP = \\frac{RR_{11} - RR_{10} - RR_{01} + 1}{RR_{11}}\\). Ce paramètre mesure la proportion du risque dans le groupe doublement exposé qui est due à l’interaction. L’AP est en lien avec le \\(\\small RERI_{RR}\\) : AP &gt; 0 si et seulement si \\(\\small RERI_{RR}\\) &gt; 0 AP &lt; 0 si et seulement si \\(\\small RERI_{RR}\\) &lt; 0. En fait \\(\\small AP = \\frac{RERI_{RR}}{RR_{11}-1}\\). Références "],["simulations.html", "Chapter 7 Simulations", " Chapter 7 Simulations Pour la description des différents types d’estimation, on a simulé des données selon le DAG suivant (toutes les variables sont binaires): Le code ayant permis de simuler les données est le suivant : rm(list=ls()) param.causal.model &lt;- function(p_L1 = 0.50, p_L2 = 0.20, p_L3 = 0.70, # baseline confounders b_A1 = 0.10, b_L1_A1 = 0.15, b_L2_A1 = 0.25, # modèle de A1 b_A2 = 0.15, b_L1_A2 = 0.20, b_L3_A2 = 0.20, # modèle de A2 b_Y = 0.10, # modèle de Y b_L1_Y = 0.02, b_L2_Y = 0.02, b_L3_Y = -0.02, b_A1_Y = 0.3, b_A2_Y = 0.1, b_A1A2_Y = 0.4 ) { # &lt;- effet d&#39;interaction Delta) # coefficients pour simuler l&#39;exposition # exposition A1 # vérif try(if(b_A1 + b_L1_A1 + b_L1_A1 &gt; 1) stop(&quot;la somme des coefficient du modèle A1 dépasse 100%&quot;)) # exposition A2 # vérif try(if(b_A2 + b_L1_A2 + b_L3_A2 &gt; 1) stop(&quot;la somme des coefficients du modèle A2 dépasse 100%&quot;)) # coefficients pour simuler l&#39;outcome, vérif try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &gt; 1) stop(&quot;la somme des coefficients du modèle Y dépasse 100%&quot;)) try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &lt; 0) stop(&quot;la somme des coefficients du modèle Y est inférieure à 0%&quot;)) coef &lt;- list(c(p_L1 = p_L1, p_L2 = p_L2, p_L3 = p_L3), c(b_A1 = b_A1, b_L1_A1 = b_L1_A1, b_L2_A1 = b_L2_A1), c(b_A2 = b_A2, b_L1_A2 = b_L1_A2, b_L3_A2 = b_L3_A2), c(b_Y = b_Y, b_L1_Y = b_L1_Y, b_L2_Y = b_L2_Y, b_L3_Y = b_L3_Y, b_A1_Y = b_A1_Y, b_A2_Y = b_A2_Y, b_A1A2_Y = b_A1A2_Y)) return(coef) } generate.data &lt;- function(N, b = param.causal.model()) { L1 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L1&quot;]) L2 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L2&quot;]) L3 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L3&quot;]) A1 &lt;- rbinom(N, size = 1, prob = b[[2]][&quot;b_A1&quot;] + (b[[2]][&quot;b_L1_A1&quot;] * L1) + (b[[2]][&quot;b_L2_A1&quot;] * L2)) A2 &lt;- rbinom(N, size = 1, prob = b[[3]][&quot;b_A2&quot;] + (b[[3]][&quot;b_L1_A2&quot;] * L1) + (b[[3]][&quot;b_L3_A2&quot;] * L3)) Y &lt;- rbinom(N, size = 1, prob = (b[[4]][&quot;b_Y&quot;] + (b[[4]][&quot;b_L1_Y&quot;] * L1) + (b[[4]][&quot;b_L2_Y&quot;] * L2) + (b[[4]][&quot;b_L3_Y&quot;] * L3) + (b[[4]][&quot;b_A1_Y&quot;] * A1) + (b[[4]][&quot;b_A2_Y&quot;] * A2) + (b[[4]][&quot;b_A1A2_Y&quot;] * A1 * A2)) ) data.sim &lt;- data.frame(L1, L2, L3, A1, A2, Y) return(data.sim) } #### On simule une base de données set.seed(12345) # b = param.causal.model(b_A1A2_Y = -0.45) b = param.causal.model() df &lt;- generate.data(N = 10000, b = b) summary(df) prop.table(table(df$Y, df$A1, df$A2, deparse.level = 2)) Au final, les probabilités de l’outcome P(Y=1), dans chaque catégorie sont : A2 label levels value 0 A1 0 0.10 (0.30) 0 1 0.41 (0.49) 1 A1 0 0.20 (0.40) 1 1 0.90 (0.30) "],["a-partir-de-modèles-de-régression.html", "Chapter 8 A partir de modèles de régression 8.1 Multiplicatif (régression logistique) 8.2 Additif (régression lineaire)", " Chapter 8 A partir de modèles de régression 8.1 Multiplicatif (régression logistique) Lorsque l’on étudie un outcome binaire, on utilise souvent les modèles de régression logistique ## ## Call: ## glm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## family = binomial, data = df_f) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2419 -0.6580 -0.4678 -0.4341 2.1949 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.16540 0.06708 -32.281 &lt; 2e-16 *** ## as.factor(A1)1 1.75607 0.07604 23.093 &lt; 2e-16 *** ## as.factor(A2)1 0.75332 0.06831 11.028 &lt; 2e-16 *** ## as.factor(L1)1 0.15753 0.05702 2.763 0.00573 ** ## as.factor(L2)1 0.14128 0.06878 2.054 0.03996 * ## as.factor(L3)1 -0.14926 0.06141 -2.431 0.01507 * ## as.factor(A1)1:as.factor(A2)1 1.78587 0.14131 12.638 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 11037.7 on 9999 degrees of freedom ## Residual deviance: 8460.4 on 9993 degrees of freedom ## AIC: 8474.4 ## ## Number of Fisher Scoring iterations: 4 Le coefficient as.factor(A1)1 correspond à l’effet de A1 dans la catégorie de référence de A2, soit \\(\\small OR = exp(1.756) =\\) 5.789. Le coefficient as.factor(A1)1:as.factor(A2)1 correspond à la différence d’effet de A1 quand on passe dans l’autre catégorie de A2. L’effet de A1 dans la catégorie A2=1 est donc \\(\\small OR = exp(1.756+1.786) =\\) 34.536. L’interaction multiplicative peut donc être estimée par \\(\\small IM = exp(1.786) =\\) 5.966, soit \\(\\small OR(A1|A2=0) - OR(A1|A2=1)\\) On aurait aussi pu décrire cette interaction à partir de l’effet d’A2 dans chaque strate de A1. En résumé : explanatory = c(&quot;as.factor(A1)&quot;, &quot;as.factor(A2)&quot;, &quot;as.factor(A1)*as.factor(A2)&quot;, &quot;as.factor(L1)&quot;, &quot;as.factor(L2)&quot;, &quot;as.factor(L3)&quot;) dependent = &quot;Y&quot; df_f %&gt;% finalfit(dependent, explanatory)-&gt; t cbind(names = c(&quot;A1|A2=0&quot;, &quot;A2|A1=0&quot;, &quot;Interaction&quot;), DR = t[c(12,14,13),6]) %&gt;% as.data.frame %&gt;% kbl() %&gt;% kable_classic() names DR A1|A2=0 5.79 (4.99-6.72, p&lt;0.001) A2|A1=0 2.12 (1.86-2.43, p&lt;0.001) Interaction 5.96 (4.54-7.90, p&lt;0.001) Attention, les modèles de régressions logistiques sont biaisés car les données sont générées à partir de modèles additifs. 8.2 Additif (régression lineaire) partie à compléter names OR A1|A2=0 0.30 (0.28 to 0.32, p&lt;0.001) A2|A1=0 0.09 (0.08 to 0.11, p&lt;0.001) Interaction 0.39 (0.36 to 0.43, p&lt;0.001) "],["pour-aller-plus-loin.html", "Chapter 9 Pour aller plus loin 9.1 Estimation par G-computation 9.2 Estimation par Modèle Structurel Marginal 9.3 Estimation avec TMLE", " Chapter 9 Pour aller plus loin 9.1 Estimation par G-computation Il s’agit d’une “G-methods” aussi appelée “standardisation” par Hernàn. ## 1.a) on crée 4 tables correspondant aux 4 interventions contrefactuelles df.A1_0.A2_0 &lt;- df.A1_1.A2_0 &lt;- df.A1_0.A2_1 &lt;- df.A1_1.A2_1 &lt;- df df.A1_0.A2_0$A1 &lt;- df.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) df.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) df.A1_1.A2_1$A1 &lt;- df.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) ## 1.b) on modélise le critère de jugement # model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;binomial&quot;) # modèle logistique biaisé (il y a des interactions avec les baseline) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;gaussian&quot;) # modèle non biaisé # en pratique la régression logistique n&#39;est pas tellement biaisée, # mais peut être car il n&#39;y a pas la place de mettre beaucoup de confusion # par rapport aux effets importants de A1 et A2 ? (10 fois plus grands) ## 1.c) on prédit le critère de jugement sous les interventions contrefactuelles Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = df.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = df.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = df.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = df.A1_1.A2_1, type = &quot;response&quot;) ## 1.d) on va enregistrer l&#39;ensemble des résultats pertinents dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 26, nrow = nlevels(as.factor(df$A1)) * nlevels(as.factor(df$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;,&quot;RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;,&quot;RR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_0.A2_0) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) # risk difference # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) # relative risk # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1)) ## 1.e) Intervalles de confiance par bootstrap set.seed(5678) B &lt;- 2000 bootstrap.est &lt;- data.frame(matrix(NA, nrow = B, ncol = 15)) colnames(bootstrap.est) &lt;- c(&quot;p.A1is0.A2is0&quot;, &quot;p.A1is1.A2is0&quot;, &quot;p.A1is0.A2is1&quot;, &quot;p.A1is1.A2is1&quot;, &quot;RD.A1.A2is0&quot;, &quot;RD.A1.A2is1&quot;, &quot;RD.A2.A1is0&quot;, &quot;RD.A2.A1is1&quot;, &quot;lnRR.A1.A2is0&quot;, &quot;lnRR.A1.A2is1&quot;, &quot;lnRR.A2.A1is0&quot;, &quot;lnRR.A2.A1is1&quot;, &quot;INT.a&quot;, &quot;lnRERI&quot;, &quot;lnINT.m&quot;) for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(df), replace=T) bootData &lt;- df[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) # model (unbiased in this case) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = bootData, # use BootData here +++ family = &quot;gaussian&quot;) # conterfactual data sets boot.A1_0.A2_0 &lt;- boot.A1_1.A2_0 &lt;- boot.A1_0.A2_1 &lt;- boot.A1_1.A2_1 &lt;- bootData boot.A1_0.A2_0$A1 &lt;- boot.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) boot.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) boot.A1_1.A2_1$A1 &lt;- boot.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) # predict potential outcomes under counterfactual scenarios Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = boot.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = boot.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = boot.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = boot.A1_1.A2_1, type = &quot;response&quot;) # save results in the bootstrap table bootstrap.est[b,&quot;p.A1is0.A2is0&quot;] &lt;- mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;p.A1is1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;p.A1is0.A2is1&quot;] &lt;- mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;p.A1is1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) bootstrap.est[b,&quot;RD.A1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;RD.A2.A1is0&quot;] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A2.A1is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;lnRR.A1.A2is0&quot;] &lt;- log(mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A1.A2is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1)) bootstrap.est[b,&quot;lnRR.A2.A1is0&quot;] &lt;- log(mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A2.A1is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0)) bootstrap.est[b,&quot;INT.a&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;lnRERI&quot;] &lt;- log((mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnINT.m&quot;] &lt;- log( (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1))) } # head(bootstrap.est) # summary(bootstrap.est) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * # exp(-1/2 * ((x-mean(bootstrap.est[,c])) / sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # ok, on a des belles lois normales dans les distributions bootstrap, tout va bien ! # pour les IC95%, je peux utiliser la déviation standard des distributions # pour des distributions plus asymétriques, on utiliserait plutôt les percentiles 2.5% et 97.5% # } # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) # A1 = 1 et A2 = 0 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) # A1 = 0 et A2 = 1 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) # A1 = 1 et A2 = 1 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) # risk difference # RD.A1.A2is0 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) # RD.A1.A2is1 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) # RD.A2.A1is0 int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) # RD.A2.A1is1 int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) # relative risk # RR.A1.A2is0 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) # RR.A1.A2is1 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) # RR.A2.A1is0 int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) # RR.A2.A1is1 int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) # additive interaction int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$INT.a) int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$INT.a) # RERI int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRERI)) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRERI)) # multiplicative interaction int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnINT.m)) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnINT.m)) Au final, on a : A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.197 [0.183,0.211] 0.092 [0.076,0.109] 1.89 [1.68,2.11] A1=1 \\(p_{10}\\)=0.405 [0.379,0.431] \\(p_{11}\\)=0.891 [0.87,0.912] 0.486 [0.453,0.519] 2.2 [2.06,2.36] RD.A1|A2 0.301 [0.273,0.329] 0.695 [0.67,0.72] RR.A1|A2 3.89 [3.48,4.34] 4.54 [4.21,4.89] Note: additive Interaction = 0.394 [0.358;0.43] RERI = 3.78 [3.38;4.23] multiplicative Interaction = 1.17 [1.02;1.33] 9.2 Estimation par Modèle Structurel Marginal # On récupère les Y prédit précédents, que l&#39;on fusionne Y &lt;- c(Y.A1_0.A2_0, Y.A1_1.A2_0, Y.A1_0.A2_1, Y.A1_1.A2_1) length(Y) # on aura une base de données de 40000 lignes # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(df.A1_0.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_0.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;))) # dim(X) ## Modèle structurel marginal msm.RD &lt;- glm(Y ~ A1 + A2 + A1:A2, data = data.frame(Y,X), family = &quot;gaussian&quot;) # ne pas ajuster sur les facteurs de confusion msm.RD ## tableau des effets marignaux results.MSM &lt;- matrix(NA, ncol = 4, nrow = 4) colnames(results.MSM) &lt;- c(&quot;A2 = 0&quot;, &quot;A2 = 1&quot;, &quot;RD within strata of A1&quot;, &quot;RR within strata of A1&quot;) rownames(results.MSM) &lt;- c(&quot;A1 = 0&quot;, &quot;A1 = 1&quot;, &quot;RD within strata of A2&quot;, &quot;RR within strata of A2&quot;) # 4 risques marginaux results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A1&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] + msm.RD$coefficients[&quot;A1&quot;] + msm.RD$coefficients[&quot;A1:A2&quot;] # within strata of A2 results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] # within strata of A1 results.MSM[&quot;A1 = 0&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 0&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM &lt;- round(results.MSM,3) RD.interaction &lt;- msm.RD$coefficients[&quot;A1:A2&quot;] RR.interaction &lt;- (results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;]) / ( results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] ) Au final, on a (sans les IC): A2 = 0 A2 = 1 RD within strata of A1 RR within strata of A1 A1 = 0 0.099 0.198 0.099 2.008 A1 = 1 0.409 0.904 0.494 2.208 RD within strata of A2 0.311 0.705 NA NA RR within strata of A2 4.146 4.560 NA NA Note: additive Interaction = 0.395 multiplicative Interaction = 1.11 9.3 Estimation avec TMLE ## 3- int.ltmleMSM() pour estimer les différentes quantités d&#39;intérêt, ### par gcomputation, IPTW ou tmle int.ltmleMSM &lt;- function(data = data, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, final.Ynodes = final.Ynodes, SL.library = list(Q=&quot;SL.glm&quot;, g=&quot;SL.glm&quot;), gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 2000, boot.seed = 12345) { # regime= # binary array: n x numAnodes x numRegimes of counterfactual treatment or a list of &#39;rule&#39; functions regimes.MSM &lt;- array(NA, dim = c(nrow(data), 2, 4)) # 2 variables d&#39;exposition (A1, A2), 4 régimes d&#39;exposition (0,0) (1,0) (0,1) (1,1) regimes.MSM[,,1] &lt;- matrix(c(0,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé ni à A1, ni à A2 regimes.MSM[,,2] &lt;- matrix(c(1,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 uniquement regimes.MSM[,,3] &lt;- matrix(c(0,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A2 uniquement regimes.MSM[,,4] &lt;- matrix(c(1,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 et à A2 # summary.measures = valeurs des coefficients du MSM associés à chaque régime # array: num.regimes x num.summary.measures x num.final.Ynodes - # measures summarizing the regimes that will be used on the right hand side of working.msm # (baseline covariates may also be used in the right hand side of working.msm and do not need to be included in summary.measures) summary.measures.reg &lt;- array(NA, dim = c(4, 3, 1)) summary.measures.reg[,,1] &lt;- matrix(c(0, 0, 0, # aucun effet ni de A1, ni de A2 1, 0, 0, # effet de A1 isolé 0, 1, 0, # effet de A2 isolé 1, 1, 1), # effet de A1 + A2 + A1:A2 ncol = 3, nrow = 4, byrow = TRUE) colnames(summary.measures.reg) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A1:A2&quot;) if(gcomp == TRUE) { # test length SL.library$Q SL.library$Q &lt;- ifelse(length(SL.library$Q) &gt; 1, &quot;SL.glm&quot;, SL.library$Q) # simplify SL.library$g because g functions are useless with g-computation SL.library$g &lt;- &quot;SL.mean&quot; iptw.only &lt;- FALSE } ltmle_MSM &lt;- ltmleMSM(data = data, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res &lt;- data.frame(&quot;beta.Intercept&quot; = rep(NA, B), &quot;beta.A1&quot; = rep(NA, B), &quot;beta.A2&quot; = rep(NA, B), &quot;beta.A1A2&quot; = rep(NA, B)) if(gcomp == TRUE) { set.seed &lt;- boot.seed for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) bootData &lt;- data[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) boot_ltmle_MSM &lt;- ltmleMSM(data = bootData, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res$beta.Intercept[b] &lt;- boot_ltmle_MSM$beta[&quot;(Intercept)&quot;] bootstrap.res$beta.A1[b] &lt;- boot_ltmle_MSM$beta[&quot;A1&quot;] bootstrap.res$beta.A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A2&quot;] bootstrap.res$beta.A1A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A1:A2&quot;] } } return(list(ltmle_MSM = ltmle_MSM, bootstrap.res = bootstrap.res)) } ### 4- summary.int() pour enregistrer l&#39;ensemble des estimations summary.int &lt;- function(data = data, ltmle_MSM = ltmle_MSM, estimator = c(&quot;gcomp&quot;, &quot;iptw&quot;, &quot;tmle&quot;)) { if(estimator == &quot;gcomp&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == FALSE) stop(&quot;The ltmle function did not use the gcomp estimator, but the iptw +/- tmle estimator&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta } if(estimator == &quot;iptw&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, iptw is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta.iptw IC &lt;- ltmle_MSM$ltmle_MSM$IC.iptw } if(estimator == &quot;tmle&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, tmle is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta IC &lt;- ltmle_MSM$ltmle_MSM$IC } # on va enregitrer l&#39;ensemble des résultats pertinent dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 34, nrow = nlevels(as.factor(data$A1)) * nlevels(as.factor(data$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;sd.p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;sd.RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;, &quot;RD.A2&quot;,&quot;sd.RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;sd.lnRR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;, &quot;RR.A2&quot;,&quot;sd.lnRR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;sd.a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;sd.lnRERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;sd.ln.m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # on peut retrouver les IC95% par delta method # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;]) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;]) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A2&quot;]) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;] + beta[&quot;A2&quot;] + beta[&quot;A1:A2&quot;]) # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) ## IC95% if(estimator == &quot;iptw&quot; | estimator == &quot;tmle&quot;) { # A1 = 0 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]),0,0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]),0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 grad &lt;- rep(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), 4) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0, 1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI grad &lt;- c((int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } if(estimator == &quot;gcomp&quot;) { ltmle_MSM$bootstrap.res$p.A1_0.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept) ltmle_MSM$bootstrap.res$p.A1_1.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1) ltmle_MSM$bootstrap.res$p.A1_0.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A2) ltmle_MSM$bootstrap.res$p.A1_1.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1 + ltmle_MSM$bootstrap.res$beta.A2 + ltmle_MSM$bootstrap.res$beta.A1A2) ltmle_MSM$bootstrap.res$RD.A1.A2_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A1.A2_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 ltmle_MSM$bootstrap.res$RD.A2.A1_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_0.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A2.A1_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 ltmle_MSM$bootstrap.res$lnRR.A1.A2_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_0 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A1.A2_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_1) ltmle_MSM$bootstrap.res$lnRR.A2.A1_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_0.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A2.A1_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_1.A2_0) ltmle_MSM$bootstrap.res$a.INT &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$lnRERI &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$ln.m.INT &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 * ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / (ltmle_MSM$bootstrap.res$p.A1_1.A2_0 * ltmle_MSM$bootstrap.res$p.A1_0.A2_1)) # A1 = 0 et A2 = 0 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_0) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_0) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_1) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_1) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_0) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_1) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_0) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_1) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_0) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_1) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_0) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_1) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$a.INT) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRERI) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$ln.m.INT) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } return(list(int.r = int.r, bootstrap.res = bootstrap.res)) } ### Obtention du MSM par la fonction ltmle, estimation par gcomp, iptw ou tmle # avec la fonction int.ltmleMSM() # on définit les arguments de la fonction ltmleMSM du package ltmle library(ltmle) library(SuperLearner) ## arguments à renseigner Q_formulas = c(Y=&quot;Q.kplus1 ~ L1 + L2 + L3 + A1 * A2&quot;) # useful to add A1 * A2 interaction here g_formulas = c(&quot;A1 ~ L1 + L2&quot;, &quot;A2 ~ L1 + L3&quot;) SL.library = list(Q=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;), g=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;)) ### estimation par IPTW et TMLE interaction.ltmle &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, SL.library = SL.library, gcomp = FALSE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;) ### estimation par g-computation # par défaut, il fait une régression logistique à partir de la formule Q_formulas # si on veut faire un régression linéaire pour le modèle additif, on peut créer une fonction de SuperLearner # à partir de la fonction SL.glm SL.glm.gaussian &lt;- function (Y, X, newX, family = &quot;gaussian&quot;, # tout est comme SL.glm, sauf cette famille &quot;gaussian&quot; obsWeights, model = TRUE, ...) { if (is.matrix(X)) { X = as.data.frame(X) } fit.glm &lt;- glm(Y ~ ., data = X, family = family, weights = obsWeights, model = model) if (is.matrix(newX)) { newX = as.data.frame(newX) } pred &lt;- predict(fit.glm, newdata = newX, type = &quot;response&quot;) fit &lt;- list(object = fit.glm) class(fit) &lt;- &quot;SL.glm&quot; out &lt;- list(pred = pred, fit = fit) return(out) } environment(SL.glm.gaussian) &lt;-asNamespace(&quot;SuperLearner&quot;) interaction.gcomp &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, # SL.library = SL.library, SL.library = list(Q=&quot;SL.glm.gaussian&quot;, # g=&quot;SL.mean&quot;), gcomp = TRUE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 1000, # nombre d&#39;échantillons bootstrap boot.seed = 54321) # seed pour l&#39;échantillonnage bootstrap ### 3) Calcul des paramètres utiles pour l&#39;analyse de l&#39;interaction # avec la fonction summary.int() ### récupération des résultats tmle summary.tmle &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;tmle&quot;)) # summary.tmle$int.r ### récupération des résultats iptw summary.iptw &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;iptw&quot;)) # summary.iptw$int.r ### récupération des résultats gcomputation summary.gcomp &lt;- summary.int(data = df, ltmle_MSM = interaction.gcomp, estimator = c(&quot;gcomp&quot;)) # summary.gcomp$int.r # head(summary.gcomp$bootstrap.res) # # vérifier la normalité des estimations bootstrap # bootstrap.est &lt;- subset(summary.gcomp$bootstrap.res, # select = # c(&quot;p.A1_0.A2_0&quot;, # &quot;p.A1_1.A2_0&quot;, # &quot;p.A1_0.A2_1&quot;, # &quot;p.A1_1.A2_1&quot;, # &quot;RD.A1.A2_0&quot;, # &quot;RD.A1.A2_1&quot;, # &quot;RD.A2.A1_0&quot;, # &quot;RD.A2.A1_1&quot;, # &quot;lnRR.A1.A2_0&quot;, # &quot;lnRR.A1.A2_1&quot;, # &quot;lnRR.A2.A1_0&quot;, # &quot;lnRR.A2.A1_1&quot;, # &quot;a.INT&quot;, # &quot;lnRERI&quot;, # &quot;ln.m.INT&quot;)) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * exp(-1/2*((x-mean(bootstrap.est[,c]))/sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } Au final, on a (présentation selon recommandation Knol et al. [8]): 9.3.1 TMLE (#tab:t_tmle)Interaction effects estimated by TMLE A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.195 [0.18,0.21] 0.091 [0.073,0.109] 1.88 [1.67,2.11] A1=1 \\(p_{10}\\)=0.408 [0.378,0.439] \\(p_{11}\\)=0.903 [0.88,0.927] 0.495 [0.457,0.534] 2.21 [2.04,2.4] RD.A1|A2 0.304 [0.272,0.336] 0.708 [0.68,0.737] RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] Note: additive Interaction = 0.404 [0.362;0.447] RERI = 3.89 [3.45;4.4] multiplicative Interaction = 1.18 [1.02;1.36] 9.3.2 IPTW (#tab:t_iptw)Interaction effects estimated by IPTW A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.195 [0.18,0.21] 0.091 [0.073,0.109] 1.88 [1.67,2.11] A1=1 \\(p_{10}\\)=0.408 [0.377,0.439] \\(p_{11}\\)=0.904 [0.88,0.927] 0.496 [0.457,0.535] 2.22 [2.05,2.4] RD.A1|A2 0.304 [0.272,0.336] 0.709 [0.68,0.737] RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] Note: additive Interaction = 0.405 [0.362;0.447] RERI = 3.9 [3.45;4.4] multiplicative Interaction = 1.18 [1.02;1.36] 9.3.3 G-computation (#tab:t_ggcomp)Interaction effects estimated by G-computation A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.112] \\(p_{01}\\)=0.197 [0.183,0.211] 0.093 [0.076,0.11] 1.9 [1.69,2.13] A1=1 \\(p_{10}\\)=0.4 [0.373,0.427] \\(p_{11}\\)=0.893 [0.872,0.915] 0.494 [0.459,0.528] 2.23 [2.08,2.4] RD.A1|A2 0.296 [0.267,0.325] 0.697 [0.671,0.722] RR.A1|A2 3.86 [3.45,4.32] 4.54 [4.46,4.61] Note: additive Interaction = 0.4 [0.362;0.439] RERI = 3.86 [3.46;4.31] multiplicative Interaction = 1.18 [1.02;1.35] Références "],["représentations-graphiques.html", "Chapter 10 Représentations graphiques", " Chapter 10 Représentations graphiques "],["présentation-des-résultats.html", "Chapter 11 Présentation des résultats 11.1 Modification d’effet 11.2 Interaction 11.3 Proposition", " Chapter 11 Présentation des résultats Les recommandation de Knol et al. [8] sont : 11.1 Modification d’effet Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC pour A1 dans les strates de A2. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y a été ajustée. 11.2 Interaction Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC de l’effet de A1 sur Y dans les strates de A2 et de A2 sur Y dans les strates de A1. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y et la relation entre A2 et Y ont été ajustées. 11.3 Proposition Présenter les effets marginaux ou les proportions dans chaque strate présenter les effets dans chaque strate dans une échelle multiplicative et additive Références "],["proposition-détapes.html", "Chapter 12 Proposition d’étapes", " Chapter 12 Proposition d’étapes formuler l’objectif predictif ou explicatif interaction ou modification d’effet? DAG, estimand, estimateur Description tableau croisé Analyses exploratoires régressions avec terme d’interaction analyses stratifiées marge Analyses confirmatoire g computation MSM "],["exemple-1---y-binaire.html", "Chapter 13 Exemple 1 - Y binaire", " Chapter 13 Exemple 1 - Y binaire "],["exemple-2---y-quantitatif.html", "Chapter 14 Exemple 2 - Y quantitatif", " Chapter 14 Exemple 2 - Y quantitatif 1. Objectifs Dans cette étude [ref], on s’est intéressé, de façon explicative, à l’effet de la défavorisation sociale précoce sur le taux de cholestérol LDL vers 45 ans; mais aussi à l’effet du sexe sur ce taux de cholestérol en fonction de la défavorisation sociale précoce. Ici on s’intéresse donc à deux modifications d’effet. 2. DAG, estimand, estimateur Le DAG (sans les médiateurs) était : Les estimands étaient définis sur l’échelle additive par : \\(\\small (Y_{s=1|d=0} - Y_{s=0|d=0}) - (Y_{s=1|d=1} - Y_{s=0|d=1})\\) \\(\\small (Y_{d=1|s=0} - Y_{d=0|s=0}) - (Y_{d=1|s=1} - Y_{d=0|s=1})\\) Ils sont ici équivalents car il n’y pas de facteurs de confusion, donc, par exemple, \\(\\small Y_{d=1|s=0} = Y_{s=0|d=1} = Y_{d=1,s=0}\\) Les effets ont été estimés par g-computation (standardisation). 3. Résultats "],["exemple-3---y-multinomial.html", "Chapter 15 Exemple 3 - Y multinomial", " Chapter 15 Exemple 3 - Y multinomial "],["exemple-4---x-quantitatif.html", "Chapter 16 Exemple 4 - X quantitatif", " Chapter 16 Exemple 4 - X quantitatif "],["synthèse-générale.html", "Chapter 17 Synthèse générale", " Chapter 17 Synthèse générale La première étape importantes consiste à définir précisément l’objectif. Et, si l’on est dans une démarche explicative, d’inférence causale, il s’agit de définir si la mesure d’un effet d’interaction est nécessaire pour y répondre (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Le fait de choisir une démarche d’analyse d’interaction ou de modification d’effet repose sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Concernant le choix de l’échelle, idéalement, les interactions devraient être reportées sur les 2 échelles [8] [2]. Cependant, l’échelle additive est plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Concernant les paramètres, Références "],["pour-aller-plus-loin-1.html", "Chapter 18 Pour aller plus loin… 18.1 Ajouter de la complexité 18.2 Interaction avec confusion intermédiaire 18.3 Interaction et médiation", " Chapter 18 Pour aller plus loin… 18.1 Ajouter de la complexité A1 et A2 sont rarement indépendants. Scénario plus probable : 18.2 Interaction avec confusion intermédiaire 18.3 Interaction et médiation [9] [10] Références "],["références.html", "Chapter 19 Références", " Chapter 19 Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> e225b751959377b69a975cacad5b91fa4793254f
=======
[["index.html", "Interactions et modifications d’effet en Epidémiologie Chapter 1 Présentation", " Interactions et modifications d’effet en Epidémiologie CERPOP, INSERM, EQUITY Team Last compiled on 04 May, 2023 Chapter 1 Présentation Ce document a été rédigé en tant que document de synthèse du travail du groupe “Interaction” de l’équipe EQUITY, CERPOP. Ce travail a consisté en une revue de la littérature et en une application détaillée des méthodes sur des analyses illustratives, dans un but d’auto-formation et pédagogique. Les participant.e.s du groupe de travail sont : Hélène COLINEAUX Léna BONIN Camille JOANNES Benoit LEPAGE Lola NEUFCOURT Ainhoa UGARTECHE The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapter 2 Introduction 2.1 Quand étudier les interactions ? 2.2 Les points les plus importants", " Chapter 2 Introduction Comment telle prédisposition génétique et telle exposition environnementale inter-agissent-elles ? L’effet de tel traitement varie-t-il selon les circonstances ? Selon les caractéristiques du patient ? Telle intervention peut-elle être bénéfique pour un groupe social et délétère pour un autre ? De nombreuses questions épidémiologiques impliquent des mécanismes d’interactions ou de modifications d’effet. Pourtant, étudier ces mécanismes restent encore complexe aujourd’hui sur le plan méthodologique : quelle démarche adopter ? sur quelle échelle mesurer cette interaction ? comment interpréter les coefficients ? et cetera. Dans ce document, nous proposons une synthèse de la littérature et une démarche progressive et appliquée pour explorer ces questions. 2.1 Quand étudier les interactions ? 2.1.1 Prediction versus causalité La science des données cherche à répondre à 3 types d’objectifs [1] : Selon le type d’objectif, la démarche d’analyse et les enjeux méthodologiques ne vont pas être les mêmes. Si l’objectif est prédictif, la démarche va être centrée sur la prédiction de l’outcome, à partir de covariables sélectionnées afin d’optimiser la précision de l’estimation, tout en prenant en compte leur disponibilité en pratique et la parcimonie du modèle. Dans une démarche explicative, ou étiologique, au contraire, la démarche va être centré sur l’estimation d’un effet causal, en prenant en compte les covariables en fonction de leur rôle vis-à-vis de l’effet d’intérêt (facteurs de confusion, colliders, médiateurs…). En épidémiologie, à l’exception des cas où l’on souhaite développer un test ou score diagnostic ou pronostic, les objectifs sont le plus souvent explicatifs. On cherche en effet, la plupart du temps, à identifier des liens de cause à effet, afin de pouvoir agir sur les causes pour modifier les effets. Finalement, pour répondre à la question “quand doit-on prendre en compte les interactions ?”, il est d’abord nécessaire d’identifier dans quel type de démarche l’on s’inscrit : Démarche prédictive : on ajoutera alors les interactions dans le modèle de prédiction, pour le rendre plus flexible, si cela améliore la précision de l’estimation [2]. Démarche explicative/étiologique : on étudiera les interactions ou modifications d’effet, si cela répond directement à l’objectif. Par exemple : Si l’objectif est du type “l’effet de X sur Y varie-t-il en fonction de V ?”, on prendra en compte l’interactions entre X et V. Les objectifs qui nécessitent la prise en compte de l’interaction peuvent aussi être du type : “Quel est l’effet conjoint de X et V sur Y ?” ou “Quel part de l’effet de X sur Y disparaît quand V est modifié ?”, etc. Par contre, si l’objectif est simplement d’estimer l’effet de X sur Y, ou l’effet médié par M, la prise en compte des interactions entre X et des covariables (facteurs de confusion ou médiateurs) n’est pas indispensable. C’est l’effet “moyen” qui sera estimé. Des termes d’interactions peuvent cependant être ajoutés (mais non interprétés), si cela améliore la précision de l’estimation (enjeu d’optimisation du modèle). 2.1.2 Types d’objectifs Dans ce document, nous nous intéresserons principalement aux interactions et modifications d’effet dans une démarche étiologique/ explicative. Les objectifs pouvant nécessiter l’étude de l’interaction/modification d’effet sont [2] : Cibler des sous-groupes. Par exemple, identifier des sous-groupes pour lesquels l’intervention aura le plus d’effet afin de pouvoir cibler l’intervention en cas de ressources limitées, ou s’assurer que l’intervention est bénéfice pour tous les groupes et pas délétères pour certains groupes. Explorer les mécanismes d’un effet. Par exemple, en cas d’intervention qui n’a d’effet qu’en présence ou absence d’une caractéristiques particulière (définition mécanistique de l’interaction) ou seulement conjointement à une autre intervention. Etudier l’effet d’une intervention pour éliminer une partie de l’effet d’une exposition non modifiable. Par exemple, quelle part de l’effet du niveau d’éducation des parents sur la mortalité disparaîtrait si on intervenait sur le tabagisme à l’adolescence ? 2.2 Les points les plus importants La première étape importante consiste à définir précisément l’objectif : L’objectif est-il de type descriptif, prédictif ou explicatif ? Si l’on est dans une démarche explicative, d’inférence causale, est-ce que la mesure d’un effet d’interaction est nécessaire pour y répondre ? (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Ensuite, de nombreuses questions se posent pour réaliser une analyse d’interaction, auxquelles nous tentons de répondre dans ce document : S’agit-il d’une interaction ou une modification d’effet ? Sur quelle échelle la mesure-t-on ? Un effet d’interaction peut en effet être défini sur une échelle multiplicative ou additive, et les résultats entre ces échelles peuvent être contradictoires. Comment estimer cette interaction ? Quels paramètres présenter et comment les interpréter ? Comment la représenter graphiquement ? Références "],["notations.html", "Chapter 3 Notations 3.1 Variables et probabilités 3.2 Mesures d’effets", " Chapter 3 Notations 3.1 Variables et probabilités On note : un outcome : \\(\\small Y\\), deux expositions : \\(\\small X\\) et \\(\\small V\\) La probabilité de l’outcome Y dans chaque strate définie par les 2 expositions est notée : \\(\\small p_{xv} = P(Y = 1|X = x,V = v)\\) Exemple On a deux exposition \\(\\small X\\), le tabagisme actif à 20 ans, et \\(\\small V\\), le fait d’avoir vécu un évènement traumatique pendant l’enfance. L’outcome \\(\\small Y\\) est binaire et représente le fait d’avoir au moins une pathologie chronique à 60 ans \\(\\small Y=1\\) ou aucune \\(\\small Y=0\\). On décrit (données complètement fictives) : Interprétation : La probabilité d’avoir au moins une pathologie chronique à 60 ans quand on n’a pas vécu d’événement traumatique pendant l’enfance et pas fumé à 20 ans est de 10%, tandis qu’elle est de 90% quand on a vécu un événement traumatique et fumé. 3.2 Mesures d’effets L’effet d’une variable \\(\\small X\\) sur \\(\\small Y\\) peut être mesuré sur deux échelles : additive (différence de risque/probabilité) ou multiplicative (rapport de risque/probabilité). Concernant les différences de risques (DR, effets additifs) On a donc : L’effet d’un X binaire sur Y est : \\(\\small DR(X) = P(Y = 1|do(X = 1)) - P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1|X = 1) - P(Y = 1|X = 0) = p_1-p_0\\) L’effet conjoint de X et V est : \\(\\small DR(X,V) = p_{11}-p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small DR(X|V=0) = p_{10}-p_{00}\\) et \\(\\small DR(X|V=1) = p_{11}-p_{01}\\) Exemple Différences de risques pour l’exemple 1 \\(\\small DR(X \\cap V) = p_{11}-p_{00} = 0,9 - 0,1 = +0,8\\) \\(\\small DR (X | V=0) = p_{10}-p_{00} = 0,4 - 0,1 = +0,3\\) \\(\\small DR (X | V=1) = p_{11}-p_{01} = 0,9 - 0,2 = +0,7\\) Le fait d’être doublement exposé par rapport à pas du tout augmente le risque de +80%. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 augmente le risque de +30%, alors que parmi les personnes ayant vécu un événement traumatique, il est augmenté de +70%. Concernant, les rapports de risque (effets multiplicatifs) on peut notamment utiliser les risques relatifs (RR). On donc : L’effet d’un X binaire sur Y est : \\(\\small RR(X) = P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))= p_1 / p_0\\) L’effet conjoint de X et V est : \\(\\small RR(X,V) = p_{11}/p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small RR(X|V=0) = p_{10}/p_{00}\\) et \\(\\small RR(X|V=1) =p_{11}/p_{01}\\) Exemple Risques relatifs pour l’exemple 1 \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) Le risque quand on est doublement exposé par rapport à pas du tout est multiplié par 9. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 multiplie le risque par 4, alors que parmi les personnes ayant vécu un événement traumatique, il est multiplié par 4,5. "],["interaction-vs-modification-deffets.html", "Chapter 4 Interaction vs modification d’effets 4.1 Modification d’effets 4.2 Interaction 4.3 Synthèse", " Chapter 4 Interaction vs modification d’effets Dans le champ des analyses d’interaction, deux termes peuvent être rencontrés : “interaction” et “modification d’effet”. Quel est la différence entre ces deux termes ? 4.1 Modification d’effets La question de la modification d’effet consiste à d’identifier si l’effet du traitement ou de l’exposition est différent dans différents groupes de patients ayant des caractéristiques différentes (estimer l’effet d’une exposition séparément en fonction d’une autre variable) [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait 1 seule intervention mais que l’analyse est stratifiée sur V. On analyse donc l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de \\(\\small V\\). En observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X\\) mais pas celui de \\(\\small V\\). On ajustera sur les facteurs de confusion de \\(\\small X \\rightarrow Y\\). On ne fait pas d’hypothèse sur les mécanismes de la modification d’effet, qui peut être causale, de façon directe ou indirecte, ou pas du tout (par proxy ou cause commune) [4]. Exemples d’objectifs : identifier des groupes pour lesquels le traitement ne serait pas utile, ou si l’effet du traitement est homogène/hétérogène en fonction de l’âge, du sexe, etc. On a une modification de l’effet de X par V si l’effet de X est différent dans chaque strate définie par V: en additif : \\(\\small DR(X | V=0) ≠ DR(X | V=1)\\) soit \\(\\small p_{10}-p_{00} ≠ p_{11}-p_{01}\\) en multiplicatif : \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) soit \\(\\small p_{10}/p_{00} ≠ p_{11}/p_{01}1\\) Exemple Modification d’effet dans l’exemple 1 En additif : effet quand V=0 : \\(\\small DR (X | V=0) = 0,4 - 0,1 = +0,3\\) effet quand V=1 : \\(\\small DR (X | V=1) = 0,9 - 0,2 = +0,7\\) donc \\(\\small DR (X | V=0) ≠ DR (X | V=1)\\) En multiplicatif : effet quand V=0 : \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) effet quand V=1 : \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) donc \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) Ici l’effet du tabagisme est différent selon que les personnes ont vécu un événement traumatique ou non, sur l’échelle additive et multiplicative. On peut donc dire que le fait d’avoir vécu un événement traumatique modifie l’effet du tabac. Attention, on fait l’hypothèse de l’absence de facteurs de confusion entre le tabagisme et l’outcome, ce qui est en réalité peu probable. 4.2 Interaction Quand on s’intéresse à l’interaction, on s’intéresse plutôt à l’effet conjoints de 2 expositions (ou plus) sur un outcome. Il y a une interaction synergique si l’effet conjoint est supérieur à l’effet de la somme des individuels. Il y a une interaction antagoniste lorsque l’effet conjoint est inférieur à la somme des effets individuels [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y a plusieurs interventions selon le nombre de combinaison. On analyse donc l’effet du scénario \\(\\small do(X, V)\\). Ici l’effet causal d’interêt est vraiment l’effet conjoint des deux variables. Dans un schéma observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X*V\\). On ajustera sur les facteurs de confusion de \\(\\small X.V \\rightarrow Y\\). On fait l’hypothèse que les mécanismes de l’effet conjoint de X et V sont causaux. On a une interaction si : en additif : \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) \\(\\small p_{11}-p_{00} ≠ (p_{10}-p_{00})+(p_{01}-p_{00})\\) \\(\\small p_{11} ≠ p_{10} + p_{01} - p_{00}\\) en multiplicatif \\(\\small RR(X \\cap V) ≠ RR(X| V=0) + RR(V| X=0)\\) \\(\\small p_{11}/p_{00} ≠ (p_{10}/p_{00})+(p_{01}/p_{00})\\) \\(\\small p_{11} ≠ (p_{10} + p_{01}) / p_{00}\\) Exemple Interaction dans l’exemple 1 En additif : effet joint : \\(\\small DR(X \\cap V) = 0,9 - 0,1 = +0.8\\) somme des effets individuel : \\(\\small DR(X| V=0) + DR(V| X=0) = +0,3 +0,1 = +0,4\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) En multiplicatif : effet joint : \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) produit des effets individuel : \\(\\small RR(X | V=0) \\times RR(V | X=0) = 4 \\times 2 = \\times 8\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) \\times DR(V| X=0)\\) Ici l’effet joint des 2 expositions est supérieur à la somme ou au produit des effets individuels, il y a donc une interaction synergique entre les deux expositions. 4.3 Synthèse Mathématiquement, les formulations sont équivalentes : échelle additive: \\(\\small p_{10} -p_{00} ≠ p_{11}- p_{01} ⇔ p_{11}≠(p_{10}+p_{01})- p_{00}\\) échelle multiplicative : \\(\\small p_{10} /p_{00} ≠ p_{11}/ p_{01} ⇔ p_{11}≠(p_{10} \\times p_{01})/p_{00}\\) La différence se joue plutôt sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Il existe des cas où l’identification d’une interaction ou d’une modification d’effet ne conduira pas à la même démarche et donc au même résultat [5]. Prenons le DAG suivant : Dans ce cas, il n’y a pas d’interaction entre A1 et A2, car si on intervient sur les 2 (\\(\\small do(A1, A2)\\)), il n’y a plus de chemin entre A2 et Y. Il peut par contre y avoir une modification de l’effet \\(\\small A1 \\rightarrow Y\\) par A2 (\\(\\small do(A1)\\)). Dans ce cas, pour estimer cet effet, L1 et L2 seront considérés comme des facteurs de confusion, mais pas L3. Références "],["la-question-des-échelles.html", "Chapter 5 La question des échelles 5.1 Mesures des interactions 5.2 Lien entre les deux échelles 5.3 Synthèse", " Chapter 5 La question des échelles 5.1 Mesures des interactions Echelle additive Une façon simple de mesurer l’interaction est de mesurer à quel point l’effet conjoint de deux facteurs est différents de la somme de leurs effets individuels [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) Exemple Mesure de l’interaction dans l’exemple 1 \\(\\small DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0)) = 0.8 - (0,3 + 0,1) = +0,4\\) soit \\(\\small p_{11} - p_{10} - p_{01} + p_{00} = 0,9 - 0,4 - 0,2 + 0,1 = +0,4\\) ou \\(\\small (p_{11} - p_{01}) - (p_{10} - p_{00}) = (0,9 - 0,2) - (0,4 - 0,1) = 0,7 - 0,3 = +0,4\\) ou \\(\\small (p_{11} - p_{10}) - (p_{01} - p_{00}) = (0,9 - 0,4) - (0,2 - 0,1) = 0,5 - 0,1 = +0,4\\) soit : Echelle multiplicative En cas d’outcome binaire, c’est souvent le RR ou l’OR qui est utilisé pour mesurer les effets. La mesure de l’interaction sur une échelle multiplicative serait donc [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Exemple Mesure de l’nteraction dans l’exemple 1 \\(\\small \\frac{RR(X\\cap V)}{RR(X| V=0)*RR(V|X=0)} = 9/(4 \\times 2) = \\times 1,1\\) soit \\(\\small\\frac{p_{11} / p_{00}}{(p_{10} + p_{01}) / p_{00}} = \\frac{0,9 / 0,1}{(0,4 \\times 0,2) / 0,1} = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{01}}{p_{10} / p_{00}} = \\frac{0,9 / 0,2}{0,4 / 0,1} = \\times 4,5 / \\times 4 = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{10}}{p_{01} / p_{00}} = \\frac{0,9 / 0,4}{0,2 / 0,1} = \\times 2,25 - \\times 2 = \\times 1,1\\) ou : 5.2 Lien entre les deux échelles Un apparent paradoxe Mesurer l’interaction sur une seule échelle peut être trompeur [6]. On peut fréquemment observer une interaction positive dans une échelle (par exemple \\(\\small p11 - p10 - p01 + p00 &gt; 0\\)) et négative dans l’autre (par exemple \\(\\small p11.p00 / p10.p01 &lt;1\\)). Exemple Dans cet exemple (on a juste modifié la probabilité \\(p_{11}\\), on observe une interaction additive positive (l’effet de X augmente de +20% quand V=1 par rapport à V=0) mais une interaction multiplicative négative (l’effet de X est multiplié par 0,9 - donc diminue - quand V=1 par rapport à V=0). Il a même été démontré que si on n’observe pas d’interaction sur une échelle, alors on en observera obligatoirement sur l’autre échelle… [2]. Exemple Dans cet exemple, il n’y a pas d’interaction multiplicative (effet de X identique quelque soit V), mais sur l’echelle additive, on observe une interaction positive. et dans cet autre exemple, il n’y a pas d’interaction additive (effet de X identique quelque soit V), mais sur l’echelle multiplicative, on observe une interaction négative. Le continuum Dans un article de 2019 [7], Vanderweele décrit le continuum existant entre les 2 échelles. Par exemple, avec deux expositions ayant un effet positif (qui augmentent le risque) sur l’outcome en l’absence de l’autre exposition, lorsque l’effet joint est très important, l’interaction est positive sur les 2 échelles. Mais lorsque la taille de l’effet joint diminue, l’interaction multiplicative devient négative alors que l’interaction additive reste positive. Puis, lorsque la taille de l’effet joint diminue encore, l’interaction devient négative sur les deux échelles. Interaction pure et qualitative Dans ce continuum, deux cas particuliers d’interaction peuvent être retrouvées : Interaction pure de X en fonction de V, si X n’a un effet que dans une strate de V. Par exemple, \\(\\small p_{10} = p_{00}\\) et \\(\\small p_{11} ≠ p_{01}\\) Par exemple ici, V a un effet si X=0 mais pas si X=1 : Interaction qualitative de X1 en fonction de X2, , si l’effet de X1 dans une strate de X2 va dans la direction opposée de l’autre strate de X2 Par exemple ici, V a un effet positif si X=0 mais négatif si X=1 : 5.3 Synthèse Quelle échelle choisir pour mesurer un effet d’interaction ? Même si en pratique l’échelle multiplicative est plus utilisée en raison de l’utilisation des modèles logistiques [8], il semble y avoir un consensus pour privilégier l’échelle additive, plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Si on reprend l’exemple ci dessous : X représente un traitement dont on ne dispose que de 100 doses et Y un outcome de santé favorable (guérison). Il faut choisir si on donne 100 doses au groupe V = 0 ou au groupe V = 1. Si on donne 100 doses au groupe V = 0, 30 personnes seront guéries grace au traitement (30 personnes de plus que l’évolution naturelle, X=0) contre 50 personnes si on les donne au groupe V = 1. Donc il est préférable d’allouer les doses au groupe V=1. Pourtant si on avait réfléchi à partir de l’échelle multiplicative, on aurait choisi le groupe V=0 car l’effet du traitement est de RR=4 dans le groupe V = 0 et RR=3,5 dans le groupe v = 1… On peut donc conclure à un effet multiplicatif plus fort d’un traitement dans un groupe alors qu’en terme d’utilité (nombre de personnes favorablement impactées), l’échelle additive nous conduirait à choisir l’autre groupe… Idéalement, les interactions devraient cependant être reportées sur les 2 échelles [8] [2]. Références "],["types-de-paramètres.html", "Chapter 6 Types de paramètres 6.1 Avec les différences de risques (DR) 6.2 Avec les risques relatifs (RR) 6.3 Avec les Odds Ration (OR) 6.4 Excès de risque à partir des RR (RERI) 6.5 Autres", " Chapter 6 Types de paramètres Plusieurs paramètres peuvent être utilisés pour décrire une interaction, sur l’échelle additive ou multiplicative. 6.1 Avec les différences de risques (DR) On a déjà défini un paramètre d’interaction sur l’échelle additive (AI) à partir des différences d’effets [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) 6.2 Avec les risques relatifs (RR) On a aussi défini un paramètre d’interaction sur l’échelle multiplicative (MI) à partir des risques relatifs [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) 6.3 Avec les Odds Ration (OR) Souvent en épidémiologie, lorsque l’outcome Y est binaire, les effets sont mesurés par des odds ratio estimé à partir de modèle de régression logistique. Un paramètre d’interaction sur l’echelle multiplicative (MI_{OR}) peut être estimé à partir de ces OR [2] : \\(\\small MI_{OR} = \\frac{OR_{11}}{OR_{10} \\times OR_{01}}\\) En général, la mesure \\(\\small MI_{OR}\\) et \\(\\small MI_{RR}\\) seront proches si l’outcome est rare [2]. 6.4 Excès de risque à partir des RR (RERI) Lorsque seulement les risques relatifs sont donnés mais que l’on souhaite évaluer l’interaction sur l’échelle additive, “l’excès de risque du à l’interaction” (RERI) ou “interaction contrast ratio” (ICR), peut être estimé à partir des risques relatifs [2] : \\(\\small RERI = RR_{11} - RR_{10} - RR_{01} + 1\\) Il faut noter que, bien que le RERI donne la direction direction (positive, négative ou nulle) de l’interaction additive, nous ne pouvons pas utiliser le RERI pour évaluer l’ampleur de l’interaction additive, à moins de connaître au moins \\(\\small p_{00}\\). Si l’on a seulement l’OR et que l’outcome est rare, les OR peuvent approximé les RR, on a donc : \\(\\small RERI_{OR} = OR_{11} - OR_{10} - OR_{01} + 1 \\approx RERI_{RR}\\) 6.5 Autres D’autres paramètres ont aussi été proposé [2], tels que : Le “Synergie index” (SI) Il s’agit d’un paramètre explorant l’interaction additive : \\(\\small S = \\frac{RR_{11} - 1}{(RR_{10} - 1) + (RR_{01}-1)}\\). Il mesure à quel point le rapport de risque joint dépasse 1, et si cette mesure est supérieure à la somme de “à quel point” les rapports de risque de chaque exposition dépasse 1. Si le dénominateur est positif: si S &gt; 1, alors \\(\\small RERI_{RR}\\) &gt; 0 si S &lt; 1, alors \\(\\small RERI_{RR}\\) &lt; 0 L’interprétation de l’indice de synergie devient difficile dans les cas où l’effet de l’une des expositions est négatif et que le dénominateur de S est donc inférieur à 1. 6.5.1 Proportion attribuable (AP) Il s’agit aussi d’un paramètre explorant l’interaction additive : \\(\\small AP = \\frac{RR_{11} - RR_{10} - RR_{01} + 1}{RR_{11}}\\). Ce paramètre mesure la proportion du risque dans le groupe doublement exposé qui est due à l’interaction. L’AP est en lien avec le \\(\\small RERI_{RR}\\) : AP &gt; 0 si et seulement si \\(\\small RERI_{RR}\\) &gt; 0 AP &lt; 0 si et seulement si \\(\\small RERI_{RR}\\) &lt; 0. En fait \\(\\small AP = \\frac{RERI_{RR}}{RR_{11}-1}\\). Références "],["simulations.html", "Chapter 7 Simulations", " Chapter 7 Simulations Pour la description des différents types d’estimation, on a simulé des données selon le DAG suivant (toutes les variables sont binaires): Le code ayant permis de simuler les données est le suivant : rm(list=ls()) param.causal.model &lt;- function(p_L1 = 0.50, p_L2 = 0.20, p_L3 = 0.70, # baseline confounders b_A1 = 0.10, b_L1_A1 = 0.15, b_L2_A1 = 0.25, # modèle de A1 b_A2 = 0.15, b_L1_A2 = 0.20, b_L3_A2 = 0.20, # modèle de A2 b_Y = 0.10, # modèle de Y b_L1_Y = 0.02, b_L2_Y = 0.02, b_L3_Y = -0.02, b_A1_Y = 0.3, b_A2_Y = 0.1, b_A1A2_Y = 0.4 ) { # &lt;- effet d&#39;interaction Delta) # coefficients pour simuler l&#39;exposition # exposition A1 # vérif try(if(b_A1 + b_L1_A1 + b_L1_A1 &gt; 1) stop(&quot;la somme des coefficient du modèle A1 dépasse 100%&quot;)) # exposition A2 # vérif try(if(b_A2 + b_L1_A2 + b_L3_A2 &gt; 1) stop(&quot;la somme des coefficients du modèle A2 dépasse 100%&quot;)) # coefficients pour simuler l&#39;outcome, vérif try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &gt; 1) stop(&quot;la somme des coefficients du modèle Y dépasse 100%&quot;)) try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &lt; 0) stop(&quot;la somme des coefficients du modèle Y est inférieure à 0%&quot;)) coef &lt;- list(c(p_L1 = p_L1, p_L2 = p_L2, p_L3 = p_L3), c(b_A1 = b_A1, b_L1_A1 = b_L1_A1, b_L2_A1 = b_L2_A1), c(b_A2 = b_A2, b_L1_A2 = b_L1_A2, b_L3_A2 = b_L3_A2), c(b_Y = b_Y, b_L1_Y = b_L1_Y, b_L2_Y = b_L2_Y, b_L3_Y = b_L3_Y, b_A1_Y = b_A1_Y, b_A2_Y = b_A2_Y, b_A1A2_Y = b_A1A2_Y)) return(coef) } generate.data &lt;- function(N, b = param.causal.model()) { L1 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L1&quot;]) L2 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L2&quot;]) L3 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L3&quot;]) A1 &lt;- rbinom(N, size = 1, prob = b[[2]][&quot;b_A1&quot;] + (b[[2]][&quot;b_L1_A1&quot;] * L1) + (b[[2]][&quot;b_L2_A1&quot;] * L2)) A2 &lt;- rbinom(N, size = 1, prob = b[[3]][&quot;b_A2&quot;] + (b[[3]][&quot;b_L1_A2&quot;] * L1) + (b[[3]][&quot;b_L3_A2&quot;] * L3)) Y &lt;- rbinom(N, size = 1, prob = (b[[4]][&quot;b_Y&quot;] + (b[[4]][&quot;b_L1_Y&quot;] * L1) + (b[[4]][&quot;b_L2_Y&quot;] * L2) + (b[[4]][&quot;b_L3_Y&quot;] * L3) + (b[[4]][&quot;b_A1_Y&quot;] * A1) + (b[[4]][&quot;b_A2_Y&quot;] * A2) + (b[[4]][&quot;b_A1A2_Y&quot;] * A1 * A2)) ) data.sim &lt;- data.frame(L1, L2, L3, A1, A2, Y) return(data.sim) } #### On simule une base de données set.seed(12345) # b = param.causal.model(b_A1A2_Y = -0.45) b = param.causal.model() df &lt;- generate.data(N = 10000, b = b) summary(df) prop.table(table(df$Y, df$A1, df$A2, deparse.level = 2)) Au final, les probabilités de l’outcome P(Y=1), dans chaque catégorie sont : A2 label levels value 0 A1 0 0.10 (0.30) 0 1 0.41 (0.49) 1 A1 0 0.20 (0.40) 1 1 0.90 (0.30) "],["a-partir-de-modèles-de-régression.html", "Chapter 8 A partir de modèles de régression 8.1 Régression logistique 8.2 Régression lineaire", " Chapter 8 A partir de modèles de régression Dans une première étape exploratoire, on peut simplement utiliser les modèles de régression habituels. 8.1 Régression logistique Lorsque l’on étudie un outcome binaire, on utilise souvent les modèles de régression logistique. ## ## Call: ## glm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## family = binomial, data = df_f) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2419 -0.6580 -0.4678 -0.4341 2.1949 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.16540 0.06708 -32.281 &lt; 2e-16 *** ## as.factor(A1)1 1.75607 0.07604 23.093 &lt; 2e-16 *** ## as.factor(A2)1 0.75332 0.06831 11.028 &lt; 2e-16 *** ## as.factor(L1)1 0.15753 0.05702 2.763 0.00573 ** ## as.factor(L2)1 0.14128 0.06878 2.054 0.03996 * ## as.factor(L3)1 -0.14926 0.06141 -2.431 0.01507 * ## as.factor(A1)1:as.factor(A2)1 1.78587 0.14131 12.638 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 11037.7 on 9999 degrees of freedom ## Residual deviance: 8460.4 on 9993 degrees of freedom ## AIC: 8474.4 ## ## Number of Fisher Scoring iterations: 4 Le coefficient as.factor(A1)1 correspond à l’effet de A1 dans la catégorie de référence de A2, soit \\(\\small OR = exp(1.756) =\\) 5.789. Le coefficient as.factor(A1)1:as.factor(A2)1 correspond à la différence d’effet de A1 quand on passe dans l’autre catégorie de A2. L’effet de A1 dans la catégorie A2=1 est donc \\(\\small OR = exp(1.756+1.786) =\\) 34.536. L’interaction multiplicative peut donc être estimée par \\(\\small IM = exp(1.786) =\\) 5.966, soit \\(\\small OR(A1|A2=0) - OR(A1|A2=1)\\). Ici l’interaction est significative. On aurait aussi pu décrire cette interaction à partir de l’effet d’A2 dans chaque strate de A1. On peut explorer l’interaction sur l’échelle additive en estimant le RERI par \\(\\small RERI \\approx OR_{11} - OR_{10} - OR_{01} + 1 =\\) 28.499, soit \\(\\small OR(A1|A2=0) - OR(A1|A2=1)\\). Ici l’interaction est significative. En résumé (le package finalfit permet de sortir les résultats proprement) : explanatory = c(&quot;as.factor(A1)&quot;, &quot;as.factor(A2)&quot;, &quot;as.factor(A1)*as.factor(A2)&quot;, &quot;as.factor(L1)&quot;, &quot;as.factor(L2)&quot;, &quot;as.factor(L3)&quot;) dependent = &quot;Y&quot; df_f %&gt;% finalfit(dependent, explanatory)-&gt; t cbind(names = c(&quot;A1|A2=0&quot;, &quot;A2|A1=0&quot;, &quot;Interaction&quot;), OR = t[c(12,14,13),6]) %&gt;% as.data.frame %&gt;% kbl() %&gt;% kable_classic() names OR A1|A2=0 5.79 (4.99-6.72, p&lt;0.001) A2|A1=0 2.12 (1.86-2.43, p&lt;0.001) Interaction 5.96 (4.54-7.90, p&lt;0.001) Attention, les modèles de régressions logistiques sont biaisés car les données sont générées à partir de modèles additifs. 8.2 Régression lineaire Même si l’outcome binaire, on peut en théorie utiliser un modèle de régression linéaire et explorer les effets sur une échelle additive. Si l’outcome est quantitatif, on utilise aussi, en général, les modèles de régression linéaire. ## ## Call: ## lm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93110 -0.19602 -0.10494 -0.08426 0.91574 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.103835 0.008146 12.746 &lt; 2e-16 *** ## as.factor(A1)1 0.300796 0.011592 25.948 &lt; 2e-16 *** ## as.factor(A2)1 0.092280 0.008671 10.642 &lt; 2e-16 *** ## as.factor(L1)1 0.020677 0.007495 2.759 0.00581 ** ## as.factor(L2)1 0.019476 0.009410 2.070 0.03851 * ## as.factor(L3)1 -0.019574 0.008085 -2.421 0.01549 * ## as.factor(A1)1:as.factor(A2)1 0.394034 0.017854 22.070 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3615 on 9993 degrees of freedom ## Multiple R-squared: 0.2856, Adjusted R-squared: 0.2852 ## F-statistic: 665.8 on 6 and 9993 DF, p-value: &lt; 2.2e-16 partie à compléter names OR A1|A2=0 0.30 (0.28 to 0.32, p&lt;0.001) A2|A1=0 0.09 (0.08 to 0.11, p&lt;0.001) Interaction 0.39 (0.36 to 0.43, p&lt;0.001) "],["pour-aller-plus-loin.html", "Chapter 9 Pour aller plus loin 9.1 Estimation par G-computation 9.2 Estimation par Modèle Structurel Marginal 9.3 Estimation avec TMLE", " Chapter 9 Pour aller plus loin 9.1 Estimation par G-computation Il s’agit d’une “G-methods” aussi appelée “standardisation” par Hernàn. ## 1.a) on crée 4 tables correspondant aux 4 interventions contrefactuelles df.A1_0.A2_0 &lt;- df.A1_1.A2_0 &lt;- df.A1_0.A2_1 &lt;- df.A1_1.A2_1 &lt;- df df.A1_0.A2_0$A1 &lt;- df.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) df.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) df.A1_1.A2_1$A1 &lt;- df.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) ## 1.b) on modélise le critère de jugement # model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;binomial&quot;) # modèle logistique biaisé (il y a des interactions avec les baseline) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;gaussian&quot;) # modèle non biaisé # en pratique la régression logistique n&#39;est pas tellement biaisée, # mais peut être car il n&#39;y a pas la place de mettre beaucoup de confusion # par rapport aux effets importants de A1 et A2 ? (10 fois plus grands) ## 1.c) on prédit le critère de jugement sous les interventions contrefactuelles Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = df.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = df.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = df.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = df.A1_1.A2_1, type = &quot;response&quot;) ## 1.d) on va enregistrer l&#39;ensemble des résultats pertinents dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 26, nrow = nlevels(as.factor(df$A1)) * nlevels(as.factor(df$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;,&quot;RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;,&quot;RR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_0.A2_0) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) # risk difference # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) # relative risk # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1)) ## 1.e) Intervalles de confiance par bootstrap set.seed(5678) B &lt;- 2000 bootstrap.est &lt;- data.frame(matrix(NA, nrow = B, ncol = 15)) colnames(bootstrap.est) &lt;- c(&quot;p.A1is0.A2is0&quot;, &quot;p.A1is1.A2is0&quot;, &quot;p.A1is0.A2is1&quot;, &quot;p.A1is1.A2is1&quot;, &quot;RD.A1.A2is0&quot;, &quot;RD.A1.A2is1&quot;, &quot;RD.A2.A1is0&quot;, &quot;RD.A2.A1is1&quot;, &quot;lnRR.A1.A2is0&quot;, &quot;lnRR.A1.A2is1&quot;, &quot;lnRR.A2.A1is0&quot;, &quot;lnRR.A2.A1is1&quot;, &quot;INT.a&quot;, &quot;lnRERI&quot;, &quot;lnINT.m&quot;) for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(df), replace=T) bootData &lt;- df[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) # model (unbiased in this case) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = bootData, # use BootData here +++ family = &quot;gaussian&quot;) # conterfactual data sets boot.A1_0.A2_0 &lt;- boot.A1_1.A2_0 &lt;- boot.A1_0.A2_1 &lt;- boot.A1_1.A2_1 &lt;- bootData boot.A1_0.A2_0$A1 &lt;- boot.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) boot.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) boot.A1_1.A2_1$A1 &lt;- boot.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) # predict potential outcomes under counterfactual scenarios Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = boot.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = boot.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = boot.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = boot.A1_1.A2_1, type = &quot;response&quot;) # save results in the bootstrap table bootstrap.est[b,&quot;p.A1is0.A2is0&quot;] &lt;- mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;p.A1is1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;p.A1is0.A2is1&quot;] &lt;- mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;p.A1is1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) bootstrap.est[b,&quot;RD.A1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;RD.A2.A1is0&quot;] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A2.A1is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;lnRR.A1.A2is0&quot;] &lt;- log(mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A1.A2is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1)) bootstrap.est[b,&quot;lnRR.A2.A1is0&quot;] &lt;- log(mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A2.A1is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0)) bootstrap.est[b,&quot;INT.a&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;lnRERI&quot;] &lt;- log((mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnINT.m&quot;] &lt;- log( (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1))) } # head(bootstrap.est) # summary(bootstrap.est) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * # exp(-1/2 * ((x-mean(bootstrap.est[,c])) / sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # ok, on a des belles lois normales dans les distributions bootstrap, tout va bien ! # pour les IC95%, je peux utiliser la déviation standard des distributions # pour des distributions plus asymétriques, on utiliserait plutôt les percentiles 2.5% et 97.5% # } # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) # A1 = 1 et A2 = 0 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) # A1 = 0 et A2 = 1 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) # A1 = 1 et A2 = 1 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) # risk difference # RD.A1.A2is0 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) # RD.A1.A2is1 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) # RD.A2.A1is0 int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) # RD.A2.A1is1 int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) # relative risk # RR.A1.A2is0 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) # RR.A1.A2is1 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) # RR.A2.A1is0 int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) # RR.A2.A1is1 int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) # additive interaction int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$INT.a) int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$INT.a) # RERI int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRERI)) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRERI)) # multiplicative interaction int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnINT.m)) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnINT.m)) Au final, on a : A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.197 [0.183,0.211] 0.092 [0.076,0.109] 1.89 [1.68,2.11] A1=1 \\(p_{10}\\)=0.405 [0.379,0.431] \\(p_{11}\\)=0.891 [0.87,0.912] 0.486 [0.453,0.519] 2.2 [2.06,2.36] RD.A1|A2 0.301 [0.273,0.329] 0.695 [0.67,0.72] RR.A1|A2 3.89 [3.48,4.34] 4.54 [4.21,4.89] Note: additive Interaction = 0.394 [0.358;0.43] RERI = 3.78 [3.38;4.23] multiplicative Interaction = 1.17 [1.02;1.33] 9.2 Estimation par Modèle Structurel Marginal # On récupère les Y prédit précédents, que l&#39;on fusionne Y &lt;- c(Y.A1_0.A2_0, Y.A1_1.A2_0, Y.A1_0.A2_1, Y.A1_1.A2_1) length(Y) # on aura une base de données de 40000 lignes # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(df.A1_0.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_0.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;))) # dim(X) ## Modèle structurel marginal msm.RD &lt;- glm(Y ~ A1 + A2 + A1:A2, data = data.frame(Y,X), family = &quot;gaussian&quot;) # ne pas ajuster sur les facteurs de confusion msm.RD ## tableau des effets marignaux results.MSM &lt;- matrix(NA, ncol = 4, nrow = 4) colnames(results.MSM) &lt;- c(&quot;A2 = 0&quot;, &quot;A2 = 1&quot;, &quot;RD within strata of A1&quot;, &quot;RR within strata of A1&quot;) rownames(results.MSM) &lt;- c(&quot;A1 = 0&quot;, &quot;A1 = 1&quot;, &quot;RD within strata of A2&quot;, &quot;RR within strata of A2&quot;) # 4 risques marginaux results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A1&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] + msm.RD$coefficients[&quot;A1&quot;] + msm.RD$coefficients[&quot;A1:A2&quot;] # within strata of A2 results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] # within strata of A1 results.MSM[&quot;A1 = 0&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 0&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM &lt;- round(results.MSM,3) RD.interaction &lt;- msm.RD$coefficients[&quot;A1:A2&quot;] RR.interaction &lt;- (results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;]) / ( results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] ) Au final, on a (sans les IC): A2 = 0 A2 = 1 RD within strata of A1 RR within strata of A1 A1 = 0 0.099 0.198 0.099 2.008 A1 = 1 0.409 0.904 0.494 2.208 RD within strata of A2 0.311 0.705 NA NA RR within strata of A2 4.146 4.560 NA NA Note: additive Interaction = 0.395 multiplicative Interaction = 1.11 9.3 Estimation avec TMLE ## 3- int.ltmleMSM() pour estimer les différentes quantités d&#39;intérêt, ### par gcomputation, IPTW ou tmle int.ltmleMSM &lt;- function(data = data, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, final.Ynodes = final.Ynodes, SL.library = list(Q=&quot;SL.glm&quot;, g=&quot;SL.glm&quot;), gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 2000, boot.seed = 12345) { # regime= # binary array: n x numAnodes x numRegimes of counterfactual treatment or a list of &#39;rule&#39; functions regimes.MSM &lt;- array(NA, dim = c(nrow(data), 2, 4)) # 2 variables d&#39;exposition (A1, A2), 4 régimes d&#39;exposition (0,0) (1,0) (0,1) (1,1) regimes.MSM[,,1] &lt;- matrix(c(0,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé ni à A1, ni à A2 regimes.MSM[,,2] &lt;- matrix(c(1,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 uniquement regimes.MSM[,,3] &lt;- matrix(c(0,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A2 uniquement regimes.MSM[,,4] &lt;- matrix(c(1,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 et à A2 # summary.measures = valeurs des coefficients du MSM associés à chaque régime # array: num.regimes x num.summary.measures x num.final.Ynodes - # measures summarizing the regimes that will be used on the right hand side of working.msm # (baseline covariates may also be used in the right hand side of working.msm and do not need to be included in summary.measures) summary.measures.reg &lt;- array(NA, dim = c(4, 3, 1)) summary.measures.reg[,,1] &lt;- matrix(c(0, 0, 0, # aucun effet ni de A1, ni de A2 1, 0, 0, # effet de A1 isolé 0, 1, 0, # effet de A2 isolé 1, 1, 1), # effet de A1 + A2 + A1:A2 ncol = 3, nrow = 4, byrow = TRUE) colnames(summary.measures.reg) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A1:A2&quot;) if(gcomp == TRUE) { # test length SL.library$Q SL.library$Q &lt;- ifelse(length(SL.library$Q) &gt; 1, &quot;SL.glm&quot;, SL.library$Q) # simplify SL.library$g because g functions are useless with g-computation SL.library$g &lt;- &quot;SL.mean&quot; iptw.only &lt;- FALSE } ltmle_MSM &lt;- ltmleMSM(data = data, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res &lt;- data.frame(&quot;beta.Intercept&quot; = rep(NA, B), &quot;beta.A1&quot; = rep(NA, B), &quot;beta.A2&quot; = rep(NA, B), &quot;beta.A1A2&quot; = rep(NA, B)) if(gcomp == TRUE) { set.seed &lt;- boot.seed for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) bootData &lt;- data[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) boot_ltmle_MSM &lt;- ltmleMSM(data = bootData, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res$beta.Intercept[b] &lt;- boot_ltmle_MSM$beta[&quot;(Intercept)&quot;] bootstrap.res$beta.A1[b] &lt;- boot_ltmle_MSM$beta[&quot;A1&quot;] bootstrap.res$beta.A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A2&quot;] bootstrap.res$beta.A1A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A1:A2&quot;] } } return(list(ltmle_MSM = ltmle_MSM, bootstrap.res = bootstrap.res)) } ### 4- summary.int() pour enregistrer l&#39;ensemble des estimations summary.int &lt;- function(data = data, ltmle_MSM = ltmle_MSM, estimator = c(&quot;gcomp&quot;, &quot;iptw&quot;, &quot;tmle&quot;)) { if(estimator == &quot;gcomp&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == FALSE) stop(&quot;The ltmle function did not use the gcomp estimator, but the iptw +/- tmle estimator&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta } if(estimator == &quot;iptw&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, iptw is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta.iptw IC &lt;- ltmle_MSM$ltmle_MSM$IC.iptw } if(estimator == &quot;tmle&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, tmle is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta IC &lt;- ltmle_MSM$ltmle_MSM$IC } # on va enregitrer l&#39;ensemble des résultats pertinent dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 34, nrow = nlevels(as.factor(data$A1)) * nlevels(as.factor(data$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;sd.p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;sd.RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;, &quot;RD.A2&quot;,&quot;sd.RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;sd.lnRR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;, &quot;RR.A2&quot;,&quot;sd.lnRR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;sd.a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;sd.lnRERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;sd.ln.m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # on peut retrouver les IC95% par delta method # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;]) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;]) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A2&quot;]) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;] + beta[&quot;A2&quot;] + beta[&quot;A1:A2&quot;]) # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) ## IC95% if(estimator == &quot;iptw&quot; | estimator == &quot;tmle&quot;) { # A1 = 0 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]),0,0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]),0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 grad &lt;- rep(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), 4) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0, 1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI grad &lt;- c((int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } if(estimator == &quot;gcomp&quot;) { ltmle_MSM$bootstrap.res$p.A1_0.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept) ltmle_MSM$bootstrap.res$p.A1_1.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1) ltmle_MSM$bootstrap.res$p.A1_0.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A2) ltmle_MSM$bootstrap.res$p.A1_1.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1 + ltmle_MSM$bootstrap.res$beta.A2 + ltmle_MSM$bootstrap.res$beta.A1A2) ltmle_MSM$bootstrap.res$RD.A1.A2_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A1.A2_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 ltmle_MSM$bootstrap.res$RD.A2.A1_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_0.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A2.A1_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 ltmle_MSM$bootstrap.res$lnRR.A1.A2_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_0 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A1.A2_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_1) ltmle_MSM$bootstrap.res$lnRR.A2.A1_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_0.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A2.A1_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_1.A2_0) ltmle_MSM$bootstrap.res$a.INT &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$lnRERI &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$ln.m.INT &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 * ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / (ltmle_MSM$bootstrap.res$p.A1_1.A2_0 * ltmle_MSM$bootstrap.res$p.A1_0.A2_1)) # A1 = 0 et A2 = 0 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_0) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_0) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_1) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_1) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_0) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_1) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_0) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_1) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_0) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_1) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_0) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_1) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$a.INT) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRERI) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$ln.m.INT) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } return(list(int.r = int.r, bootstrap.res = bootstrap.res)) } ### Obtention du MSM par la fonction ltmle, estimation par gcomp, iptw ou tmle # avec la fonction int.ltmleMSM() # on définit les arguments de la fonction ltmleMSM du package ltmle library(ltmle) library(SuperLearner) ## arguments à renseigner Q_formulas = c(Y=&quot;Q.kplus1 ~ L1 + L2 + L3 + A1 * A2&quot;) # useful to add A1 * A2 interaction here g_formulas = c(&quot;A1 ~ L1 + L2&quot;, &quot;A2 ~ L1 + L3&quot;) SL.library = list(Q=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;), g=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;)) ### estimation par IPTW et TMLE interaction.ltmle &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, SL.library = SL.library, gcomp = FALSE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;) ### estimation par g-computation # par défaut, il fait une régression logistique à partir de la formule Q_formulas # si on veut faire un régression linéaire pour le modèle additif, on peut créer une fonction de SuperLearner # à partir de la fonction SL.glm SL.glm.gaussian &lt;- function (Y, X, newX, family = &quot;gaussian&quot;, # tout est comme SL.glm, sauf cette famille &quot;gaussian&quot; obsWeights, model = TRUE, ...) { if (is.matrix(X)) { X = as.data.frame(X) } fit.glm &lt;- glm(Y ~ ., data = X, family = family, weights = obsWeights, model = model) if (is.matrix(newX)) { newX = as.data.frame(newX) } pred &lt;- predict(fit.glm, newdata = newX, type = &quot;response&quot;) fit &lt;- list(object = fit.glm) class(fit) &lt;- &quot;SL.glm&quot; out &lt;- list(pred = pred, fit = fit) return(out) } environment(SL.glm.gaussian) &lt;-asNamespace(&quot;SuperLearner&quot;) interaction.gcomp &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, # SL.library = SL.library, SL.library = list(Q=&quot;SL.glm.gaussian&quot;, # g=&quot;SL.mean&quot;), gcomp = TRUE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 1000, # nombre d&#39;échantillons bootstrap boot.seed = 54321) # seed pour l&#39;échantillonnage bootstrap ### 3) Calcul des paramètres utiles pour l&#39;analyse de l&#39;interaction # avec la fonction summary.int() ### récupération des résultats tmle summary.tmle &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;tmle&quot;)) # summary.tmle$int.r ### récupération des résultats iptw summary.iptw &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;iptw&quot;)) # summary.iptw$int.r ### récupération des résultats gcomputation summary.gcomp &lt;- summary.int(data = df, ltmle_MSM = interaction.gcomp, estimator = c(&quot;gcomp&quot;)) # summary.gcomp$int.r # head(summary.gcomp$bootstrap.res) # # vérifier la normalité des estimations bootstrap # bootstrap.est &lt;- subset(summary.gcomp$bootstrap.res, # select = # c(&quot;p.A1_0.A2_0&quot;, # &quot;p.A1_1.A2_0&quot;, # &quot;p.A1_0.A2_1&quot;, # &quot;p.A1_1.A2_1&quot;, # &quot;RD.A1.A2_0&quot;, # &quot;RD.A1.A2_1&quot;, # &quot;RD.A2.A1_0&quot;, # &quot;RD.A2.A1_1&quot;, # &quot;lnRR.A1.A2_0&quot;, # &quot;lnRR.A1.A2_1&quot;, # &quot;lnRR.A2.A1_0&quot;, # &quot;lnRR.A2.A1_1&quot;, # &quot;a.INT&quot;, # &quot;lnRERI&quot;, # &quot;ln.m.INT&quot;)) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * exp(-1/2*((x-mean(bootstrap.est[,c]))/sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } Au final, on a (présentation selon recommandation Knol et al. [8]): 9.3.1 TMLE (#tab:t_tmle)Interaction effects estimated by TMLE A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.195 [0.18,0.21] 0.091 [0.073,0.109] 1.88 [1.67,2.11] A1=1 \\(p_{10}\\)=0.408 [0.378,0.439] \\(p_{11}\\)=0.903 [0.88,0.927] 0.495 [0.457,0.534] 2.21 [2.04,2.4] RD.A1|A2 0.304 [0.272,0.336] 0.708 [0.68,0.737] RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] Note: additive Interaction = 0.404 [0.362;0.447] RERI = 3.89 [3.45;4.4] multiplicative Interaction = 1.18 [1.02;1.36] 9.3.2 IPTW (#tab:t_iptw)Interaction effects estimated by IPTW A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.195 [0.18,0.21] 0.091 [0.073,0.109] 1.88 [1.67,2.11] A1=1 \\(p_{10}\\)=0.408 [0.377,0.439] \\(p_{11}\\)=0.904 [0.88,0.927] 0.496 [0.457,0.535] 2.22 [2.05,2.4] RD.A1|A2 0.304 [0.272,0.336] 0.709 [0.68,0.737] RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] Note: additive Interaction = 0.405 [0.362;0.447] RERI = 3.9 [3.45;4.4] multiplicative Interaction = 1.18 [1.02;1.36] 9.3.3 G-computation (#tab:t_ggcomp)Interaction effects estimated by G-computation A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.112] \\(p_{01}\\)=0.197 [0.183,0.211] 0.093 [0.076,0.11] 1.9 [1.69,2.13] A1=1 \\(p_{10}\\)=0.4 [0.373,0.427] \\(p_{11}\\)=0.893 [0.872,0.915] 0.494 [0.459,0.528] 2.23 [2.08,2.4] RD.A1|A2 0.296 [0.267,0.325] 0.697 [0.671,0.722] RR.A1|A2 3.86 [3.45,4.32] 4.54 [4.46,4.61] Note: additive Interaction = 0.4 [0.362;0.439] RERI = 3.86 [3.46;4.31] multiplicative Interaction = 1.18 [1.02;1.35] Références "],["représentations-graphiques.html", "Chapter 10 Représentations graphiques", " Chapter 10 Représentations graphiques "],["présentation-des-résultats.html", "Chapter 11 Présentation des résultats 11.1 Modification d’effet 11.2 Interaction 11.3 Proposition", " Chapter 11 Présentation des résultats Les recommandation de Knol et al. [8] sont : 11.1 Modification d’effet Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC pour A1 dans les strates de A2. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y a été ajustée. 11.2 Interaction Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC de l’effet de A1 sur Y dans les strates de A2 et de A2 sur Y dans les strates de A1. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y et la relation entre A2 et Y ont été ajustées. 11.3 Proposition Présenter les effets marginaux ou les proportions dans chaque strate présenter les effets dans chaque strate dans une échelle multiplicative et additive Références "],["proposition-détapes.html", "Chapter 12 Proposition d’étapes", " Chapter 12 Proposition d’étapes formuler l’objectif predictif ou explicatif interaction ou modification d’effet? DAG, estimand, estimateur Description tableau croisé Analyses exploratoires régressions avec terme d’interaction analyses stratifiées marge Analyses confirmatoire g computation MSM "],["exemple-1---y-binaire.html", "Chapter 13 Exemple 1 - Y binaire", " Chapter 13 Exemple 1 - Y binaire "],["exemple-2---y-quantitatif.html", "Chapter 14 Exemple 2 - Y quantitatif", " Chapter 14 Exemple 2 - Y quantitatif 1. Objectifs Dans cette étude [ref], on s’est intéressé, de façon explicative, à l’effet de la défavorisation sociale précoce sur le taux de cholestérol LDL vers 45 ans; mais aussi à l’effet du sexe sur ce taux de cholestérol en fonction de la défavorisation sociale précoce. Ici on s’intéresse donc à deux modifications d’effet. 2. DAG, estimand, estimateur Le DAG (sans les médiateurs) était : Les estimands étaient définis sur l’échelle additive par : \\(\\small (Y_{s=1|d=0} - Y_{s=0|d=0}) - (Y_{s=1|d=1} - Y_{s=0|d=1})\\) \\(\\small (Y_{d=1|s=0} - Y_{d=0|s=0}) - (Y_{d=1|s=1} - Y_{d=0|s=1})\\) Ils sont ici équivalents car il n’y pas de facteurs de confusion, donc, par exemple, \\(\\small Y_{d=1|s=0} = Y_{s=0|d=1} = Y_{d=1,s=0}\\) Les effets ont été estimés par g-computation (standardisation). 3. Résultats "],["exemple-3---y-multinomial.html", "Chapter 15 Exemple 3 - Y multinomial", " Chapter 15 Exemple 3 - Y multinomial "],["exemple-4---x-quantitatif.html", "Chapter 16 Exemple 4 - X quantitatif", " Chapter 16 Exemple 4 - X quantitatif "],["synthèse-générale.html", "Chapter 17 Synthèse générale", " Chapter 17 Synthèse générale La première étape importantes consiste à définir précisément l’objectif. Et, si l’on est dans une démarche explicative, d’inférence causale, il s’agit de définir si la mesure d’un effet d’interaction est nécessaire pour y répondre (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Le fait de choisir une démarche d’analyse d’interaction ou de modification d’effet repose sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Concernant le choix de l’échelle, idéalement, les interactions devraient être reportées sur les 2 échelles [8] [2]. Cependant, l’échelle additive est plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Concernant les paramètres, Références "],["pour-aller-plus-loin-1.html", "Chapter 18 Pour aller plus loin… 18.1 Ajouter de la complexité 18.2 Interaction avec confusion intermédiaire 18.3 Interaction et médiation", " Chapter 18 Pour aller plus loin… 18.1 Ajouter de la complexité A1 et A2 sont rarement indépendants. Scénario plus probable : 18.2 Interaction avec confusion intermédiaire 18.3 Interaction et médiation [9] [10] Références "],["références.html", "Chapter 19 Références", " Chapter 19 Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> e66f18f4651d4d14c4c9899bcd440649c836a733
=======
[["index.html", "Interactions et modifications d’effet en Epidémiologie Chapter 1 Présentation", " Interactions et modifications d’effet en Epidémiologie CERPOP, INSERM, EQUITY Team Last compiled on 04 May, 2023 Chapter 1 Présentation Ce document a été rédigé en tant que document de synthèse du travail du groupe “Interaction” de l’équipe EQUITY, CERPOP. Ce travail a consisté en une revue de la littérature et en une application détaillée des méthodes sur des analyses illustratives, dans un but d’auto-formation et pédagogique. Les participant.e.s du groupe de travail sont : Hélène COLINEAUX Léna BONIN Camille JOANNES Benoit LEPAGE Lola NEUFCOURT Ainhoa UGARTECHE The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapter 2 Introduction 2.1 Quand étudier les interactions ? 2.2 Les points les plus importants", " Chapter 2 Introduction Comment telle prédisposition génétique et telle exposition environnementale inter-agissent-elles ? L’effet de tel traitement varie-t-il selon les circonstances ? Selon les caractéristiques du patient ? Telle intervention peut-elle être bénéfique pour un groupe social et délétère pour un autre ? De nombreuses questions épidémiologiques impliquent des mécanismes d’interactions ou de modifications d’effet. Pourtant, étudier ces mécanismes restent encore complexe aujourd’hui sur le plan méthodologique : quelle démarche adopter ? sur quelle échelle mesurer cette interaction ? comment interpréter les coefficients ? et cetera. Dans ce document, nous proposons une synthèse de la littérature et une démarche progressive et appliquée pour explorer ces questions. 2.1 Quand étudier les interactions ? 2.1.1 Prediction versus causalité La science des données cherche à répondre à 3 types d’objectifs [1] : Selon le type d’objectif, la démarche d’analyse et les enjeux méthodologiques ne vont pas être les mêmes. Si l’objectif est prédictif, la démarche va être centrée sur la prédiction de l’outcome, à partir de covariables sélectionnées afin d’optimiser la précision de l’estimation, tout en prenant en compte leur disponibilité en pratique et la parcimonie du modèle. Dans une démarche explicative, ou étiologique, au contraire, la démarche va être centré sur l’estimation d’un effet causal, en prenant en compte les covariables en fonction de leur rôle vis-à-vis de l’effet d’intérêt (facteurs de confusion, colliders, médiateurs…). En épidémiologie, à l’exception des cas où l’on souhaite développer un test ou score diagnostic ou pronostic, les objectifs sont le plus souvent explicatifs. On cherche en effet, la plupart du temps, à identifier des liens de cause à effet, afin de pouvoir agir sur les causes pour modifier les effets. Finalement, pour répondre à la question “quand doit-on prendre en compte les interactions ?”, il est d’abord nécessaire d’identifier dans quel type de démarche l’on s’inscrit : Démarche prédictive : on ajoutera alors les interactions dans le modèle de prédiction, pour le rendre plus flexible, si cela améliore la précision de l’estimation [2]. Démarche explicative/étiologique : on étudiera les interactions ou modifications d’effet, si cela répond directement à l’objectif. Par exemple : Si l’objectif est du type “l’effet de X sur Y varie-t-il en fonction de V ?”, on prendra en compte l’interactions entre X et V. Les objectifs qui nécessitent la prise en compte de l’interaction peuvent aussi être du type : “Quel est l’effet conjoint de X et V sur Y ?” ou “Quel part de l’effet de X sur Y disparaît quand V est modifié ?”, etc. Par contre, si l’objectif est simplement d’estimer l’effet de X sur Y, ou l’effet médié par M, la prise en compte des interactions entre X et des covariables (facteurs de confusion ou médiateurs) n’est pas indispensable. C’est l’effet “moyen” qui sera estimé. Des termes d’interactions peuvent cependant être ajoutés (mais non interprétés), si cela améliore la précision de l’estimation (enjeu d’optimisation du modèle). 2.1.2 Types d’objectifs Dans ce document, nous nous intéresserons principalement aux interactions et modifications d’effet dans une démarche étiologique/ explicative. Les objectifs pouvant nécessiter l’étude de l’interaction/modification d’effet sont [2] : Cibler des sous-groupes. Par exemple, identifier des sous-groupes pour lesquels l’intervention aura le plus d’effet afin de pouvoir cibler l’intervention en cas de ressources limitées, ou s’assurer que l’intervention est bénéfice pour tous les groupes et pas délétères pour certains groupes. Explorer les mécanismes d’un effet. Par exemple, en cas d’intervention qui n’a d’effet qu’en présence ou absence d’une caractéristiques particulière (définition mécanistique de l’interaction) ou seulement conjointement à une autre intervention. Etudier l’effet d’une intervention pour éliminer une partie de l’effet d’une exposition non modifiable. Par exemple, quelle part de l’effet du niveau d’éducation des parents sur la mortalité disparaîtrait si on intervenait sur le tabagisme à l’adolescence ? 2.2 Les points les plus importants La première étape importante consiste à définir précisément l’objectif : L’objectif est-il de type descriptif, prédictif ou explicatif ? Si l’on est dans une démarche explicative, d’inférence causale, est-ce que la mesure d’un effet d’interaction est nécessaire pour y répondre ? (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Ensuite, de nombreuses questions se posent pour réaliser une analyse d’interaction, auxquelles nous tentons de répondre dans ce document : S’agit-il d’une interaction ou une modification d’effet ? Sur quelle échelle la mesure-t-on ? Un effet d’interaction peut en effet être défini sur une échelle multiplicative ou additive, et les résultats entre ces échelles peuvent être contradictoires. Comment estimer cette interaction ? Quels paramètres présenter et comment les interpréter ? Comment la représenter graphiquement ? Références "],["notations.html", "Chapter 3 Notations 3.1 Variables et probabilités 3.2 Mesures d’effets", " Chapter 3 Notations 3.1 Variables et probabilités On note : un outcome : \\(\\small Y\\), deux expositions : \\(\\small X\\) et \\(\\small V\\) La probabilité de l’outcome Y dans chaque strate définie par les 2 expositions est notée : \\(\\small p_{xv} = P(Y = 1|X = x,V = v)\\) Exemple On a deux exposition \\(\\small X\\), le tabagisme actif à 20 ans, et \\(\\small V\\), le fait d’avoir vécu un évènement traumatique pendant l’enfance. L’outcome \\(\\small Y\\) est binaire et représente le fait d’avoir au moins une pathologie chronique à 60 ans \\(\\small Y=1\\) ou aucune \\(\\small Y=0\\). On décrit (données complètement fictives) : Interprétation : La probabilité d’avoir au moins une pathologie chronique à 60 ans quand on n’a pas vécu d’événement traumatique pendant l’enfance et pas fumé à 20 ans est de 10%, tandis qu’elle est de 90% quand on a vécu un événement traumatique et fumé. 3.2 Mesures d’effets L’effet d’une variable \\(\\small X\\) sur \\(\\small Y\\) peut être mesuré sur deux échelles : additive (différence de risque/probabilité) ou multiplicative (rapport de risque/probabilité). Concernant les différences de risques (DR, effets additifs) On a donc : L’effet d’un X binaire sur Y est : \\(\\small DR(X) = P(Y = 1|do(X = 1)) - P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1|X = 1) - P(Y = 1|X = 0) = p_1-p_0\\) L’effet conjoint de X et V est : \\(\\small DR(X,V) = p_{11}-p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small DR(X|V=0) = p_{10}-p_{00}\\) et \\(\\small DR(X|V=1) = p_{11}-p_{01}\\) Exemple Différences de risques pour l’exemple 1 \\(\\small DR(X \\cap V) = p_{11}-p_{00} = 0,9 - 0,1 = +0,8\\) \\(\\small DR (X | V=0) = p_{10}-p_{00} = 0,4 - 0,1 = +0,3\\) \\(\\small DR (X | V=1) = p_{11}-p_{01} = 0,9 - 0,2 = +0,7\\) Le fait d’être doublement exposé par rapport à pas du tout augmente le risque de +80%. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 augmente le risque de +30%, alors que parmi les personnes ayant vécu un événement traumatique, il est augmenté de +70%. Concernant, les rapports de risque (effets multiplicatifs) on peut notamment utiliser les risques relatifs (RR). On donc : L’effet d’un X binaire sur Y est : \\(\\small RR(X) = P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))\\) qu’on peut estimer, si les conditions d’identifiabilité sont réunies, par \\(\\small P(Y = 1| do(X = 1)) / P(Y = 1|do(X = 0))= p_1 / p_0\\) L’effet conjoint de X et V est : \\(\\small RR(X,V) = p_{11}/p_{00}\\) L’effet de X sur Y dans chaque strate de V est : \\(\\small RR(X|V=0) = p_{10}/p_{00}\\) et \\(\\small RR(X|V=1) =p_{11}/p_{01}\\) Exemple Risques relatifs pour l’exemple 1 \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) Le risque quand on est doublement exposé par rapport à pas du tout est multiplié par 9. Parmi les personnes n’ayant pas vécu d’événement traumatique, le fait de fumer à 20 multiplie le risque par 4, alors que parmi les personnes ayant vécu un événement traumatique, il est multiplié par 4,5. "],["interaction-vs-modification-deffets.html", "Chapter 4 Interaction vs modification d’effets 4.1 Modification d’effets 4.2 Interaction 4.3 Synthèse", " Chapter 4 Interaction vs modification d’effets Dans le champ des analyses d’interaction, deux termes peuvent être rencontrés : “interaction” et “modification d’effet”. Quel est la différence entre ces deux termes ? 4.1 Modification d’effets La question de la modification d’effet consiste à d’identifier si l’effet du traitement ou de l’exposition est différent dans différents groupes de patients ayant des caractéristiques différentes (estimer l’effet d’une exposition séparément en fonction d’une autre variable) [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y avait 1 seule intervention mais que l’analyse est stratifiée sur V. On analyse donc l’effet du scénario \\(\\small do(X)\\) dans chaque groupe de \\(\\small V\\). En observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X\\) mais pas celui de \\(\\small V\\). On ajustera sur les facteurs de confusion de \\(\\small X \\rightarrow Y\\). On ne fait pas d’hypothèse sur les mécanismes de la modification d’effet, qui peut être causale, de façon directe ou indirecte, ou pas du tout (par proxy ou cause commune) [4]. Exemples d’objectifs : identifier des groupes pour lesquels le traitement ne serait pas utile, ou si l’effet du traitement est homogène/hétérogène en fonction de l’âge, du sexe, etc. On a une modification de l’effet de X par V si l’effet de X est différent dans chaque strate définie par V: en additif : \\(\\small DR(X | V=0) ≠ DR(X | V=1)\\) soit \\(\\small p_{10}-p_{00} ≠ p_{11}-p_{01}\\) en multiplicatif : \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) soit \\(\\small p_{10}/p_{00} ≠ p_{11}/p_{01}1\\) Exemple Modification d’effet dans l’exemple 1 En additif : effet quand V=0 : \\(\\small DR (X | V=0) = 0,4 - 0,1 = +0,3\\) effet quand V=1 : \\(\\small DR (X | V=1) = 0,9 - 0,2 = +0,7\\) donc \\(\\small DR (X | V=0) ≠ DR (X | V=1)\\) En multiplicatif : effet quand V=0 : \\(\\small RR(X | V=0) = 0,4/0,1 = \\times 4\\) effet quand V=1 : \\(\\small RR(X | V=1) = 0,9/0,2 = \\times 4,5\\) donc \\(\\small RR(X | V=0) ≠ RR(X | V=1)\\) Ici l’effet du tabagisme est différent selon que les personnes ont vécu un événement traumatique ou non, sur l’échelle additive et multiplicative. On peut donc dire que le fait d’avoir vécu un événement traumatique modifie l’effet du tabac. Attention, on fait l’hypothèse de l’absence de facteurs de confusion entre le tabagisme et l’outcome, ce qui est en réalité peu probable. 4.2 Interaction Quand on s’intéresse à l’interaction, on s’intéresse plutôt à l’effet conjoints de 2 expositions (ou plus) sur un outcome. Il y a une interaction synergique si l’effet conjoint est supérieur à l’effet de la somme des individuels. Il y a une interaction antagoniste lorsque l’effet conjoint est inférieur à la somme des effets individuels [3]. Si l’on compare avec un essai d’intervention, c’est comme s’il y a plusieurs interventions selon le nombre de combinaison. On analyse donc l’effet du scénario \\(\\small do(X, V)\\). Ici l’effet causal d’interêt est vraiment l’effet conjoint des deux variables. Dans un schéma observationnel, l’effet causal qui nous intéresse est donc celui de \\(\\small X*V\\). On ajustera sur les facteurs de confusion de \\(\\small X.V \\rightarrow Y\\). On fait l’hypothèse que les mécanismes de l’effet conjoint de X et V sont causaux. On a une interaction si : en additif : \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) \\(\\small p_{11}-p_{00} ≠ (p_{10}-p_{00})+(p_{01}-p_{00})\\) \\(\\small p_{11} ≠ p_{10} + p_{01} - p_{00}\\) en multiplicatif \\(\\small RR(X \\cap V) ≠ RR(X| V=0) + RR(V| X=0)\\) \\(\\small p_{11}/p_{00} ≠ (p_{10}/p_{00})+(p_{01}/p_{00})\\) \\(\\small p_{11} ≠ (p_{10} + p_{01}) / p_{00}\\) Exemple Interaction dans l’exemple 1 En additif : effet joint : \\(\\small DR(X \\cap V) = 0,9 - 0,1 = +0.8\\) somme des effets individuel : \\(\\small DR(X| V=0) + DR(V| X=0) = +0,3 +0,1 = +0,4\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) + DR(V| X=0)\\) En multiplicatif : effet joint : \\(\\small RR(X \\cap V) = 0,9/0,1 = \\times 9\\) produit des effets individuel : \\(\\small RR(X | V=0) \\times RR(V | X=0) = 4 \\times 2 = \\times 8\\) donc \\(\\small DR(X \\cap V) ≠ DR(X| V=0) \\times DR(V| X=0)\\) Ici l’effet joint des 2 expositions est supérieur à la somme ou au produit des effets individuels, il y a donc une interaction synergique entre les deux expositions. 4.3 Synthèse Mathématiquement, les formulations sont équivalentes : échelle additive: \\(\\small p_{10} -p_{00} ≠ p_{11}- p_{01} ⇔ p_{11}≠(p_{10}+p_{01})- p_{00}\\) échelle multiplicative : \\(\\small p_{10} /p_{00} ≠ p_{11}/ p_{01} ⇔ p_{11}≠(p_{10} \\times p_{01})/p_{00}\\) La différence se joue plutôt sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Il existe des cas où l’identification d’une interaction ou d’une modification d’effet ne conduira pas à la même démarche et donc au même résultat [5]. Prenons le DAG suivant : Dans ce cas, il n’y a pas d’interaction entre A1 et A2, car si on intervient sur les 2 (\\(\\small do(A1, A2)\\)), il n’y a plus de chemin entre A2 et Y. Il peut par contre y avoir une modification de l’effet \\(\\small A1 \\rightarrow Y\\) par A2 (\\(\\small do(A1)\\)). Dans ce cas, pour estimer cet effet, L1 et L2 seront considérés comme des facteurs de confusion, mais pas L3. Références "],["la-question-des-échelles.html", "Chapter 5 La question des échelles 5.1 Mesures des interactions 5.2 Lien entre les deux échelles 5.3 Synthèse", " Chapter 5 La question des échelles 5.1 Mesures des interactions Echelle additive Une façon simple de mesurer l’interaction est de mesurer à quel point l’effet conjoint de deux facteurs est différents de la somme de leurs effets individuels [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) Exemple Mesure de l’interaction dans l’exemple 1 \\(\\small DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0)) = 0.8 - (0,3 + 0,1) = +0,4\\) soit \\(\\small p_{11} - p_{10} - p_{01} + p_{00} = 0,9 - 0,4 - 0,2 + 0,1 = +0,4\\) ou \\(\\small (p_{11} - p_{01}) - (p_{10} - p_{00}) = (0,9 - 0,2) - (0,4 - 0,1) = 0,7 - 0,3 = +0,4\\) ou \\(\\small (p_{11} - p_{10}) - (p_{01} - p_{00}) = (0,9 - 0,4) - (0,2 - 0,1) = 0,5 - 0,1 = +0,4\\) soit : Echelle multiplicative En cas d’outcome binaire, c’est souvent le RR ou l’OR qui est utilisé pour mesurer les effets. La mesure de l’interaction sur une échelle multiplicative serait donc [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) Exemple Mesure de l’nteraction dans l’exemple 1 \\(\\small \\frac{RR(X\\cap V)}{RR(X| V=0)*RR(V|X=0)} = 9/(4 \\times 2) = \\times 1,1\\) soit \\(\\small\\frac{p_{11} / p_{00}}{(p_{10} + p_{01}) / p_{00}} = \\frac{0,9 / 0,1}{(0,4 \\times 0,2) / 0,1} = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{01}}{p_{10} / p_{00}} = \\frac{0,9 / 0,2}{0,4 / 0,1} = \\times 4,5 / \\times 4 = \\times 1,1\\) ou \\(\\small \\frac{p_{11} / p_{10}}{p_{01} / p_{00}} = \\frac{0,9 / 0,4}{0,2 / 0,1} = \\times 2,25 - \\times 2 = \\times 1,1\\) ou : 5.2 Lien entre les deux échelles Un apparent paradoxe Mesurer l’interaction sur une seule échelle peut être trompeur [6]. On peut fréquemment observer une interaction positive dans une échelle (par exemple \\(\\small p11 - p10 - p01 + p00 &gt; 0\\)) et négative dans l’autre (par exemple \\(\\small p11.p00 / p10.p01 &lt;1\\)). Exemple Dans cet exemple (on a juste modifié la probabilité \\(p_{11}\\), on observe une interaction additive positive (l’effet de X augmente de +20% quand V=1 par rapport à V=0) mais une interaction multiplicative négative (l’effet de X est multiplié par 0,9 - donc diminue - quand V=1 par rapport à V=0). Il a même été démontré que si on n’observe pas d’interaction sur une échelle, alors on en observera obligatoirement sur l’autre échelle… [2]. Exemple Dans cet exemple, il n’y a pas d’interaction multiplicative (effet de X identique quelque soit V), mais sur l’echelle additive, on observe une interaction positive. et dans cet autre exemple, il n’y a pas d’interaction additive (effet de X identique quelque soit V), mais sur l’echelle multiplicative, on observe une interaction négative. Le continuum Dans un article de 2019 [7], Vanderweele décrit le continuum existant entre les 2 échelles. Par exemple, avec deux expositions ayant un effet positif (qui augmentent le risque) sur l’outcome en l’absence de l’autre exposition, lorsque l’effet joint est très important, l’interaction est positive sur les 2 échelles. Mais lorsque la taille de l’effet joint diminue, l’interaction multiplicative devient négative alors que l’interaction additive reste positive. Puis, lorsque la taille de l’effet joint diminue encore, l’interaction devient négative sur les deux échelles. Interaction pure et qualitative Dans ce continuum, deux cas particuliers d’interaction peuvent être retrouvées : Interaction pure de X en fonction de V, si X n’a un effet que dans une strate de V. Par exemple, \\(\\small p_{10} = p_{00}\\) et \\(\\small p_{11} ≠ p_{01}\\) Par exemple ici, V a un effet si X=0 mais pas si X=1 : Interaction qualitative de X1 en fonction de X2, , si l’effet de X1 dans une strate de X2 va dans la direction opposée de l’autre strate de X2 Par exemple ici, V a un effet positif si X=0 mais négatif si X=1 : 5.3 Synthèse Quelle échelle choisir pour mesurer un effet d’interaction ? Même si en pratique l’échelle multiplicative est plus utilisée en raison de l’utilisation des modèles logistiques [8], il semble y avoir un consensus pour privilégier l’échelle additive, plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Si on reprend l’exemple ci dessous : X représente un traitement dont on ne dispose que de 100 doses et Y un outcome de santé favorable (guérison). Il faut choisir si on donne 100 doses au groupe V = 0 ou au groupe V = 1. Si on donne 100 doses au groupe V = 0, 30 personnes seront guéries grace au traitement (30 personnes de plus que l’évolution naturelle, X=0) contre 50 personnes si on les donne au groupe V = 1. Donc il est préférable d’allouer les doses au groupe V=1. Pourtant si on avait réfléchi à partir de l’échelle multiplicative, on aurait choisi le groupe V=0 car l’effet du traitement est de RR=4 dans le groupe V = 0 et RR=3,5 dans le groupe v = 1… On peut donc conclure à un effet multiplicatif plus fort d’un traitement dans un groupe alors qu’en terme d’utilité (nombre de personnes favorablement impactées), l’échelle additive nous conduirait à choisir l’autre groupe… Idéalement, les interactions devraient cependant être reportées sur les 2 échelles [8] [2]. Références "],["types-de-paramètres.html", "Chapter 6 Types de paramètres 6.1 Avec les différences de risques (DR) 6.2 Avec les risques relatifs (RR) 6.3 Avec les Odds Ration (OR) 6.4 Excès de risque à partir des RR (RERI) 6.5 Autres", " Chapter 6 Types de paramètres Plusieurs paramètres peuvent être utilisés pour décrire une interaction, sur l’échelle additive ou multiplicative. 6.1 Avec les différences de risques (DR) On a déjà défini un paramètre d’interaction sur l’échelle additive (AI) à partir des différences d’effets [2] : \\(\\small AI = DR(X\\cap V) - (DR(X|V=0) + DR(V|X=0))\\) \\(\\small AI = (p_{11} - p_{00}) - [(p_{10} - p_{00}) + (p_{01} - p_{00})]\\) soit \\(\\small AI =p_{11} - p_{10} - p_{01} + p_{00}\\) 6.2 Avec les risques relatifs (RR) On a aussi défini un paramètre d’interaction sur l’échelle multiplicative (MI) à partir des risques relatifs [2] : \\(\\small MI = \\frac{RR_{11}}{RR_{10} \\times RR_{01}}\\) soit \\(\\small MI = \\frac{p_{11} / p_{00}}{(p_{10} / p_{00}) \\times (p_{01} / p_{00})}\\) soit \\(\\small MI = \\frac{p_{11} \\times p_{00}}{p_{10} \\times p_{01}}\\) 6.3 Avec les Odds Ration (OR) Souvent en épidémiologie, lorsque l’outcome Y est binaire, les effets sont mesurés par des odds ratio estimé à partir de modèle de régression logistique. Un paramètre d’interaction sur l’echelle multiplicative (MI_{OR}) peut être estimé à partir de ces OR [2] : \\(\\small MI_{OR} = \\frac{OR_{11}}{OR_{10} \\times OR_{01}}\\) En général, la mesure \\(\\small MI_{OR}\\) et \\(\\small MI_{RR}\\) seront proches si l’outcome est rare [2]. 6.4 Excès de risque à partir des RR (RERI) Lorsque seulement les risques relatifs sont donnés mais que l’on souhaite évaluer l’interaction sur l’échelle additive, “l’excès de risque du à l’interaction” (RERI) ou “interaction contrast ratio” (ICR), peut être estimé à partir des risques relatifs [2] : \\(\\small RERI = RR_{11} - RR_{10} - RR_{01} + 1\\) Il faut noter que, bien que le RERI donne la direction direction (positive, négative ou nulle) de l’interaction additive, nous ne pouvons pas utiliser le RERI pour évaluer l’ampleur de l’interaction additive, à moins de connaître au moins \\(\\small p_{00}\\). Si l’on a seulement l’OR et que l’outcome est rare, les OR peuvent approximé les RR, on a donc : \\(\\small RERI_{OR} = OR_{11} - OR_{10} - OR_{01} + 1 \\approx RERI_{RR}\\) 6.5 Autres D’autres paramètres ont aussi été proposé [2], tels que : Le “Synergie index” (SI) Il s’agit d’un paramètre explorant l’interaction additive : \\(\\small S = \\frac{RR_{11} - 1}{(RR_{10} - 1) + (RR_{01}-1)}\\). Il mesure à quel point le rapport de risque joint dépasse 1, et si cette mesure est supérieure à la somme de “à quel point” les rapports de risque de chaque exposition dépasse 1. Si le dénominateur est positif: si S &gt; 1, alors \\(\\small RERI_{RR}\\) &gt; 0 si S &lt; 1, alors \\(\\small RERI_{RR}\\) &lt; 0 L’interprétation de l’indice de synergie devient difficile dans les cas où l’effet de l’une des expositions est négatif et que le dénominateur de S est donc inférieur à 1. 6.5.1 Proportion attribuable (AP) Il s’agit aussi d’un paramètre explorant l’interaction additive : \\(\\small AP = \\frac{RR_{11} - RR_{10} - RR_{01} + 1}{RR_{11}}\\). Ce paramètre mesure la proportion du risque dans le groupe doublement exposé qui est due à l’interaction. L’AP est en lien avec le \\(\\small RERI_{RR}\\) : AP &gt; 0 si et seulement si \\(\\small RERI_{RR}\\) &gt; 0 AP &lt; 0 si et seulement si \\(\\small RERI_{RR}\\) &lt; 0. En fait \\(\\small AP = \\frac{RERI_{RR}}{RR_{11}-1}\\). Références "],["simulations.html", "Chapter 7 Simulations", " Chapter 7 Simulations Pour la description des différents types d’estimation, on a simulé des données selon le DAG suivant (toutes les variables sont binaires): Le code ayant permis de simuler les données est le suivant : &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ## Simulations On simule des données selon le DAG suivant (toutes les variables sont binaires): "],["section.html", "Chapter 8 &gt; ", " Chapter 8 &gt; e225b751959377b69a975cacad5b91fa4793254f rm(list=ls()) param.causal.model &lt;- function(p_L1 = 0.50, p_L2 = 0.20, p_L3 = 0.70, # baseline confounders b_A1 = 0.10, b_L1_A1 = 0.15, b_L2_A1 = 0.25, # modèle de A1 b_A2 = 0.15, b_L1_A2 = 0.20, b_L3_A2 = 0.20, # modèle de A2 b_Y = 0.10, # modèle de Y b_L1_Y = 0.02, b_L2_Y = 0.02, b_L3_Y = -0.02, b_A1_Y = 0.3, b_A2_Y = 0.1, b_A1A2_Y = 0.4 ) { # &lt;- effet d&#39;interaction Delta) # coefficients pour simuler l&#39;exposition # exposition A1 # vérif try(if(b_A1 + b_L1_A1 + b_L1_A1 &gt; 1) stop(&quot;la somme des coefficient du modèle A1 dépasse 100%&quot;)) # exposition A2 # vérif try(if(b_A2 + b_L1_A2 + b_L3_A2 &gt; 1) stop(&quot;la somme des coefficients du modèle A2 dépasse 100%&quot;)) # coefficients pour simuler l&#39;outcome, vérif try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &gt; 1) stop(&quot;la somme des coefficients du modèle Y dépasse 100%&quot;)) try(if(b_Y + b_L1_Y + b_L2_Y + b_L3_Y + b_A1_Y + b_A2_Y + b_A1A2_Y &lt; 0) stop(&quot;la somme des coefficients du modèle Y est inférieure à 0%&quot;)) coef &lt;- list(c(p_L1 = p_L1, p_L2 = p_L2, p_L3 = p_L3), c(b_A1 = b_A1, b_L1_A1 = b_L1_A1, b_L2_A1 = b_L2_A1), c(b_A2 = b_A2, b_L1_A2 = b_L1_A2, b_L3_A2 = b_L3_A2), c(b_Y = b_Y, b_L1_Y = b_L1_Y, b_L2_Y = b_L2_Y, b_L3_Y = b_L3_Y, b_A1_Y = b_A1_Y, b_A2_Y = b_A2_Y, b_A1A2_Y = b_A1A2_Y)) return(coef) } generate.data &lt;- function(N, b = param.causal.model()) { L1 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L1&quot;]) L2 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L2&quot;]) L3 &lt;- rbinom(N, size = 1, prob = b[[1]][&quot;p_L3&quot;]) A1 &lt;- rbinom(N, size = 1, prob = b[[2]][&quot;b_A1&quot;] + (b[[2]][&quot;b_L1_A1&quot;] * L1) + (b[[2]][&quot;b_L2_A1&quot;] * L2)) A2 &lt;- rbinom(N, size = 1, prob = b[[3]][&quot;b_A2&quot;] + (b[[3]][&quot;b_L1_A2&quot;] * L1) + (b[[3]][&quot;b_L3_A2&quot;] * L3)) Y &lt;- rbinom(N, size = 1, prob = (b[[4]][&quot;b_Y&quot;] + (b[[4]][&quot;b_L1_Y&quot;] * L1) + (b[[4]][&quot;b_L2_Y&quot;] * L2) + (b[[4]][&quot;b_L3_Y&quot;] * L3) + (b[[4]][&quot;b_A1_Y&quot;] * A1) + (b[[4]][&quot;b_A2_Y&quot;] * A2) + (b[[4]][&quot;b_A1A2_Y&quot;] * A1 * A2)) ) data.sim &lt;- data.frame(L1, L2, L3, A1, A2, Y) return(data.sim) } #### On simule une base de données set.seed(12345) # b = param.causal.model(b_A1A2_Y = -0.45) b = param.causal.model() df &lt;- generate.data(N = 10000, b = b) summary(df) prop.table(table(df$Y, df$A1, df$A2, deparse.level = 2)) Au final, les probabilités de l’outcome P(Y=1), dans chaque catégorie sont : A2 label levels value 0 A1 0 0.10 (0.30) 0 1 0.41 (0.49) 1 A1 0 0.20 (0.40) 1 1 0.90 (0.30) "],["a-partir-de-modèles-de-régression.html", "Chapter 9 A partir de modèles de régression 9.1 Régression logistique 9.2 Régression lineaire", " Chapter 9 A partir de modèles de régression Dans une première étape exploratoire, on peut simplement utiliser les modèles de régression habituels. 9.1 Régression logistique Lorsque l’on étudie un outcome binaire, on utilise souvent les modèles de régression logistique. ## ## Call: ## glm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## family = binomial, data = df_f) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2419 -0.6580 -0.4678 -0.4341 2.1949 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.16540 0.06708 -32.281 &lt; 2e-16 *** ## as.factor(A1)1 1.75607 0.07604 23.093 &lt; 2e-16 *** ## as.factor(A2)1 0.75332 0.06831 11.028 &lt; 2e-16 *** ## as.factor(L1)1 0.15753 0.05702 2.763 0.00573 ** ## as.factor(L2)1 0.14128 0.06878 2.054 0.03996 * ## as.factor(L3)1 -0.14926 0.06141 -2.431 0.01507 * ## as.factor(A1)1:as.factor(A2)1 1.78587 0.14131 12.638 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 11037.7 on 9999 degrees of freedom ## Residual deviance: 8460.4 on 9993 degrees of freedom ## AIC: 8474.4 ## ## Number of Fisher Scoring iterations: 4 Le coefficient as.factor(A1)1 correspond à l’effet de A1 dans la catégorie de référence de A2, soit \\(\\small OR = exp(1.756) =\\) 5.789. Le coefficient as.factor(A1)1:as.factor(A2)1 correspond à la différence d’effet de A1 quand on passe dans l’autre catégorie de A2. L’effet de A1 dans la catégorie A2=1 est donc \\(\\small OR = exp(1.756+1.786) =\\) 34.536. L’interaction multiplicative peut donc être estimée par \\(\\small IM = exp(1.786) =\\) 5.966, soit \\(\\small OR(A1|A2=0) - OR(A1|A2=1)\\). Ici l’interaction est significative. On aurait aussi pu décrire cette interaction à partir de l’effet d’A2 dans chaque strate de A1. On peut explorer l’interaction sur l’échelle additive en estimant le RERI par \\(\\small RERI \\approx OR_{11} - OR_{10} - OR_{01} + 1 =\\) 28.499, soit \\(\\small OR(A1|A2=0) - OR(A1|A2=1)\\). Ici l’interaction est significative. En résumé (le package finalfit permet de sortir les résultats proprement) : explanatory = c(&quot;as.factor(A1)&quot;, &quot;as.factor(A2)&quot;, &quot;as.factor(A1)*as.factor(A2)&quot;, &quot;as.factor(L1)&quot;, &quot;as.factor(L2)&quot;, &quot;as.factor(L3)&quot;) dependent = &quot;Y&quot; df_f %&gt;% finalfit(dependent, explanatory)-&gt; t cbind(names = c(&quot;A1|A2=0&quot;, &quot;A2|A1=0&quot;, &quot;Interaction&quot;), OR = t[c(12,14,13),6]) %&gt;% as.data.frame %&gt;% kbl() %&gt;% kable_classic() names OR A1|A2=0 5.79 (4.99-6.72, p&lt;0.001) A2|A1=0 2.12 (1.86-2.43, p&lt;0.001) Interaction 5.96 (4.54-7.90, p&lt;0.001) Attention, les modèles de régressions logistiques sont biaisés car les données sont générées à partir de modèles additifs. 9.2 Régression lineaire Même si l’outcome binaire, on peut en théorie utiliser un modèle de régression linéaire et explorer les effets sur une échelle additive. Si l’outcome est quantitatif, on utilise aussi, en général, les modèles de régression linéaire. ## ## Call: ## lm(formula = Y ~ as.factor(A1) + as.factor(A2) + as.factor(A1) * ## as.factor(A2) + as.factor(L1) + as.factor(L2) + as.factor(L3), ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93110 -0.19602 -0.10494 -0.08426 0.91574 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.103835 0.008146 12.746 &lt; 2e-16 *** ## as.factor(A1)1 0.300796 0.011592 25.948 &lt; 2e-16 *** ## as.factor(A2)1 0.092280 0.008671 10.642 &lt; 2e-16 *** ## as.factor(L1)1 0.020677 0.007495 2.759 0.00581 ** ## as.factor(L2)1 0.019476 0.009410 2.070 0.03851 * ## as.factor(L3)1 -0.019574 0.008085 -2.421 0.01549 * ## as.factor(A1)1:as.factor(A2)1 0.394034 0.017854 22.070 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3615 on 9993 degrees of freedom ## Multiple R-squared: 0.2856, Adjusted R-squared: 0.2852 ## F-statistic: 665.8 on 6 and 9993 DF, p-value: &lt; 2.2e-16 partie à compléter names OR A1|A2=0 0.30 (0.28 to 0.32, p&lt;0.001) A2|A1=0 0.09 (0.08 to 0.11, p&lt;0.001) Interaction 0.39 (0.36 to 0.43, p&lt;0.001) "],["analyses-confirmatoires.html", "Chapter 10 Analyses confirmatoires 10.1 Estimation par G-computation 10.2 Estimation par Modèle Structurel Marginal 10.3 Estimation avec TMLE", " Chapter 10 Analyses confirmatoires 10.1 Estimation par G-computation Il s’agit d’une “G-methods” aussi appelée “standardisation” par Hernàn. ## 1.a) on crée 4 tables correspondant aux 4 interventions contrefactuelles df.A1_0.A2_0 &lt;- df.A1_1.A2_0 &lt;- df.A1_0.A2_1 &lt;- df.A1_1.A2_1 &lt;- df df.A1_0.A2_0$A1 &lt;- df.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) df.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) df.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) df.A1_1.A2_1$A1 &lt;- df.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) ## 1.b) on modélise le critère de jugement # model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;binomial&quot;) # modèle logistique biaisé (il y a des interactions avec les baseline) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = df, family = &quot;gaussian&quot;) # modèle non biaisé # en pratique la régression logistique n&#39;est pas tellement biaisée, # mais peut être car il n&#39;y a pas la place de mettre beaucoup de confusion # par rapport aux effets importants de A1 et A2 ? (10 fois plus grands) ## 1.c) on prédit le critère de jugement sous les interventions contrefactuelles Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = df.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = df.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = df.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = df.A1_1.A2_1, type = &quot;response&quot;) ## 1.d) on va enregistrer l&#39;ensemble des résultats pertinents dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 26, nrow = nlevels(as.factor(df$A1)) * nlevels(as.factor(df$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;,&quot;RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;,&quot;RR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_0.A2_0) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) # risk difference # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) # relative risk # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1)) ## 1.e) Intervalles de confiance par bootstrap set.seed(5678) B &lt;- 2000 bootstrap.est &lt;- data.frame(matrix(NA, nrow = B, ncol = 15)) colnames(bootstrap.est) &lt;- c(&quot;p.A1is0.A2is0&quot;, &quot;p.A1is1.A2is0&quot;, &quot;p.A1is0.A2is1&quot;, &quot;p.A1is1.A2is1&quot;, &quot;RD.A1.A2is0&quot;, &quot;RD.A1.A2is1&quot;, &quot;RD.A2.A1is0&quot;, &quot;RD.A2.A1is1&quot;, &quot;lnRR.A1.A2is0&quot;, &quot;lnRR.A1.A2is1&quot;, &quot;lnRR.A2.A1is0&quot;, &quot;lnRR.A2.A1is1&quot;, &quot;INT.a&quot;, &quot;lnRERI&quot;, &quot;lnINT.m&quot;) for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(df), replace=T) bootData &lt;- df[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) # model (unbiased in this case) model.Y &lt;- glm(Y ~ L1 + L2 + L3 + A1 + A2 + A1:A2, data = bootData, # use BootData here +++ family = &quot;gaussian&quot;) # conterfactual data sets boot.A1_0.A2_0 &lt;- boot.A1_1.A2_0 &lt;- boot.A1_0.A2_1 &lt;- boot.A1_1.A2_1 &lt;- bootData boot.A1_0.A2_0$A1 &lt;- boot.A1_0.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_1.A2_0$A1 &lt;- rep(1, nrow(df)) boot.A1_1.A2_0$A2 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A1 &lt;- rep(0, nrow(df)) boot.A1_0.A2_1$A2 &lt;- rep(1, nrow(df)) boot.A1_1.A2_1$A1 &lt;- boot.A1_1.A2_1$A2 &lt;- rep(1, nrow(df)) # predict potential outcomes under counterfactual scenarios Y.A1_0.A2_0 &lt;- predict(model.Y, newdata = boot.A1_0.A2_0, type = &quot;response&quot;) Y.A1_1.A2_0 &lt;- predict(model.Y, newdata = boot.A1_1.A2_0, type = &quot;response&quot;) Y.A1_0.A2_1 &lt;- predict(model.Y, newdata = boot.A1_0.A2_1, type = &quot;response&quot;) Y.A1_1.A2_1 &lt;- predict(model.Y, newdata = boot.A1_1.A2_1, type = &quot;response&quot;) # save results in the bootstrap table bootstrap.est[b,&quot;p.A1is0.A2is0&quot;] &lt;- mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;p.A1is1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;p.A1is0.A2is1&quot;] &lt;- mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;p.A1is1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) bootstrap.est[b,&quot;RD.A1.A2is0&quot;] &lt;- mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A1.A2is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_0.A2_1) bootstrap.est[b,&quot;RD.A2.A1is0&quot;] &lt;- mean(Y.A1_0.A2_1) - mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;RD.A2.A1is1&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) bootstrap.est[b,&quot;lnRR.A1.A2is0&quot;] &lt;- log(mean(Y.A1_1.A2_0) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A1.A2is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_0.A2_1)) bootstrap.est[b,&quot;lnRR.A2.A1is0&quot;] &lt;- log(mean(Y.A1_0.A2_1) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnRR.A2.A1is1&quot;] &lt;- log(mean(Y.A1_1.A2_1) / mean(Y.A1_1.A2_0)) bootstrap.est[b,&quot;INT.a&quot;] &lt;- mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0) bootstrap.est[b,&quot;lnRERI&quot;] &lt;- log((mean(Y.A1_1.A2_1) - mean(Y.A1_1.A2_0) - mean(Y.A1_0.A2_1) + mean(Y.A1_0.A2_0)) / mean(Y.A1_0.A2_0)) bootstrap.est[b,&quot;lnINT.m&quot;] &lt;- log( (mean(Y.A1_1.A2_1) * mean(Y.A1_0.A2_0)) / (mean(Y.A1_1.A2_0) * mean(Y.A1_0.A2_1))) } # head(bootstrap.est) # summary(bootstrap.est) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * # exp(-1/2 * ((x-mean(bootstrap.est[,c])) / sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # ok, on a des belles lois normales dans les distributions bootstrap, tout va bien ! # pour les IC95%, je peux utiliser la déviation standard des distributions # pour des distributions plus asymétriques, on utiliserait plutôt les percentiles 2.5% et 97.5% # } # marginal effects in the k1 x k2 table # A1 = 0 et A2 = 0 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is0) # A1 = 1 et A2 = 0 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is0) # A1 = 0 et A2 = 1 int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is0.A2is1) # A1 = 1 et A2 = 1 int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$p.A1is1.A2is1) # risk difference # RD.A1.A2is0 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is0) # RD.A1.A2is1 int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A1.A2is1) # RD.A2.A1is0 int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is0) # RD.A2.A1is1 int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$RD.A2.A1is1) # relative risk # RR.A1.A2is0 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is0)) # RR.A1.A2is1 int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A1.A2is1)) # RR.A2.A1is0 int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is0)) # RR.A2.A1is1 int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRR.A2.A1is1)) # additive interaction int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * sd(bootstrap.est$INT.a) int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * sd(bootstrap.est$INT.a) # RERI int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnRERI)) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnRERI)) # multiplicative interaction int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * sd(bootstrap.est$lnINT.m)) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * sd(bootstrap.est$lnINT.m)) Au final, on a : A2=0 A2=1 RD.A2|A1 RR.A2|A1 A1=0 \\(p_{00}\\)=0.104 [0.095,0.113] \\(p_{01}\\)=0.197 [0.183,0.211] 0.092 [0.076,0.109] 1.89 [1.68,2.11] A1=1 \\(p_{10}\\)=0.405 [0.379,0.431] \\(p_{11}\\)=0.891 [0.87,0.912] 0.486 [0.453,0.519] 2.2 [2.06,2.36] RD.A1|A2 0.301 [0.273,0.329] 0.695 [0.67,0.72] RR.A1|A2 3.89 [3.48,4.34] 4.54 [4.21,4.89] Note: additive Interaction = 0.394 [0.358;0.43] RERI = 3.78 [3.38;4.23] multiplicative Interaction = 1.17 [1.02;1.33] 10.2 Estimation par Modèle Structurel Marginal # On récupère les Y prédit précédents, que l&#39;on fusionne Y &lt;- c(Y.A1_0.A2_0, Y.A1_1.A2_0, Y.A1_0.A2_1, Y.A1_1.A2_1) length(Y) # on aura une base de données de 40000 lignes # On récupère les valeurs d&#39;exposition qui ont servi dans les scénarios contrefactuels # (garder le même ordre que pour les Y.A1.A2) X &lt;- rbind(subset(df.A1_0.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_0, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_0.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;)), subset(df.A1_1.A2_1, select = c(&quot;A1&quot;, &quot;A2&quot;))) # dim(X) ## Modèle structurel marginal msm.RD &lt;- glm(Y ~ A1 + A2 + A1:A2, data = data.frame(Y,X), family = &quot;gaussian&quot;) # ne pas ajuster sur les facteurs de confusion msm.RD ## tableau des effets marignaux results.MSM &lt;- matrix(NA, ncol = 4, nrow = 4) colnames(results.MSM) &lt;- c(&quot;A2 = 0&quot;, &quot;A2 = 1&quot;, &quot;RD within strata of A1&quot;, &quot;RR within strata of A1&quot;) rownames(results.MSM) &lt;- c(&quot;A1 = 0&quot;, &quot;A1 = 1&quot;, &quot;RD within strata of A2&quot;, &quot;RR within strata of A2&quot;) # 4 risques marginaux results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A1&quot;] results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] &lt;- msm.RD$coefficients[&quot;(Intercept)&quot;] + msm.RD$coefficients[&quot;A2&quot;] + msm.RD$coefficients[&quot;A1&quot;] + msm.RD$coefficients[&quot;A1:A2&quot;] # within strata of A2 results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 0&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;RR within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] results.MSM[&quot;RD within strata of A2&quot;, &quot;A2 = 1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] # within strata of A1 results.MSM[&quot;A1 = 0&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 0&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RR within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] / results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM[&quot;A1 = 1&quot;, &quot;RD within strata of A1&quot;] &lt;- results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] - results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] results.MSM &lt;- round(results.MSM,3) RD.interaction &lt;- msm.RD$coefficients[&quot;A1:A2&quot;] RR.interaction &lt;- (results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 0&quot;]) / ( results.MSM[&quot;A1 = 0&quot;,&quot;A2 = 1&quot;] * results.MSM[&quot;A1 = 1&quot;,&quot;A2 = 0&quot;] ) Au final, on a (sans les IC): A2 = 0 A2 = 1 RD within strata of A1 RR within strata of A1 A1 = 0 0.099 0.198 0.099 2.008 A1 = 1 0.409 0.904 0.494 2.208 RD within strata of A2 0.311 0.705 NA NA RR within strata of A2 4.146 4.560 NA NA Note: additive Interaction = 0.395 multiplicative Interaction = 1.11 10.3 Estimation avec TMLE ## 3- int.ltmleMSM() pour estimer les différentes quantités d&#39;intérêt, ### par gcomputation, IPTW ou tmle int.ltmleMSM &lt;- function(data = data, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, final.Ynodes = final.Ynodes, SL.library = list(Q=&quot;SL.glm&quot;, g=&quot;SL.glm&quot;), gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 2000, boot.seed = 12345) { # regime= # binary array: n x numAnodes x numRegimes of counterfactual treatment or a list of &#39;rule&#39; functions regimes.MSM &lt;- array(NA, dim = c(nrow(data), 2, 4)) # 2 variables d&#39;exposition (A1, A2), 4 régimes d&#39;exposition (0,0) (1,0) (0,1) (1,1) regimes.MSM[,,1] &lt;- matrix(c(0,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé ni à A1, ni à A2 regimes.MSM[,,2] &lt;- matrix(c(1,0), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 uniquement regimes.MSM[,,3] &lt;- matrix(c(0,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A2 uniquement regimes.MSM[,,4] &lt;- matrix(c(1,1), ncol = 2, nrow = nrow(data), byrow = TRUE) # exposé à A1 et à A2 # summary.measures = valeurs des coefficients du MSM associés à chaque régime # array: num.regimes x num.summary.measures x num.final.Ynodes - # measures summarizing the regimes that will be used on the right hand side of working.msm # (baseline covariates may also be used in the right hand side of working.msm and do not need to be included in summary.measures) summary.measures.reg &lt;- array(NA, dim = c(4, 3, 1)) summary.measures.reg[,,1] &lt;- matrix(c(0, 0, 0, # aucun effet ni de A1, ni de A2 1, 0, 0, # effet de A1 isolé 0, 1, 0, # effet de A2 isolé 1, 1, 1), # effet de A1 + A2 + A1:A2 ncol = 3, nrow = 4, byrow = TRUE) colnames(summary.measures.reg) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A1:A2&quot;) if(gcomp == TRUE) { # test length SL.library$Q SL.library$Q &lt;- ifelse(length(SL.library$Q) &gt; 1, &quot;SL.glm&quot;, SL.library$Q) # simplify SL.library$g because g functions are useless with g-computation SL.library$g &lt;- &quot;SL.mean&quot; iptw.only &lt;- FALSE } ltmle_MSM &lt;- ltmleMSM(data = data, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res &lt;- data.frame(&quot;beta.Intercept&quot; = rep(NA, B), &quot;beta.A1&quot; = rep(NA, B), &quot;beta.A2&quot; = rep(NA, B), &quot;beta.A1A2&quot; = rep(NA, B)) if(gcomp == TRUE) { set.seed &lt;- boot.seed for (b in 1:B){ # sample the indices 1 to n with replacement bootIndices &lt;- sample(1:nrow(data), replace=T) bootData &lt;- data[bootIndices,] if ( round(b/100, 0) == b/100 ) print(paste0(&quot;bootstrap number &quot;,b)) boot_ltmle_MSM &lt;- ltmleMSM(data = bootData, Anodes = Anodes, Lnodes = Lnodes, Ynodes = Ynodes, Qform = Q_formulas, gform = g_formulas, #deterministic.g.function = det.g, regimes = regimes.MSM, # à la place de abar working.msm= &quot;Y ~ A1 + A2 + A1:A2&quot;, summary.measures = summary.measures.reg, final.Ynodes = final.Ynodes, msm.weights = NULL, SL.library = SL.library, gcomp = gcomp, iptw.only = iptw.only, survivalOutcome = survivalOutcome, estimate.time = FALSE, variance.method = variance.method) bootstrap.res$beta.Intercept[b] &lt;- boot_ltmle_MSM$beta[&quot;(Intercept)&quot;] bootstrap.res$beta.A1[b] &lt;- boot_ltmle_MSM$beta[&quot;A1&quot;] bootstrap.res$beta.A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A2&quot;] bootstrap.res$beta.A1A2[b] &lt;- boot_ltmle_MSM$beta[&quot;A1:A2&quot;] } } return(list(ltmle_MSM = ltmle_MSM, bootstrap.res = bootstrap.res)) } ### 4- summary.int() pour enregistrer l&#39;ensemble des estimations summary.int &lt;- function(data = data, ltmle_MSM = ltmle_MSM, estimator = c(&quot;gcomp&quot;, &quot;iptw&quot;, &quot;tmle&quot;)) { if(estimator == &quot;gcomp&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == FALSE) stop(&quot;The ltmle function did not use the gcomp estimator, but the iptw +/- tmle estimator&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta } if(estimator == &quot;iptw&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, iptw is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta.iptw IC &lt;- ltmle_MSM$ltmle_MSM$IC.iptw } if(estimator == &quot;tmle&quot;) { try(if(ltmle_MSM$ltmle_MSM$gcomp == TRUE) stop(&quot;The ltmle function used the gcomp estimator, tmle is not available&quot;)) beta &lt;- ltmle_MSM$ltmle_MSM$beta IC &lt;- ltmle_MSM$ltmle_MSM$IC } # on va enregitrer l&#39;ensemble des résultats pertinent dans une table de longueur k1 x k2 int.r &lt;- matrix(NA, ncol = 34, nrow = nlevels(as.factor(data$A1)) * nlevels(as.factor(data$A2))) int.r &lt;- as.data.frame(int.r) names(int.r) &lt;- c(&quot;A1&quot;,&quot;A2&quot;,&quot;p&quot;,&quot;sd.p&quot;,&quot;p.lo&quot;,&quot;p.up&quot;, &quot;RD.A1&quot;,&quot;sd.RD.A1&quot;,&quot;RD.A1.lo&quot;,&quot;RD.A1.up&quot;, &quot;RD.A2&quot;,&quot;sd.RD.A2&quot;,&quot;RD.A2.lo&quot;,&quot;RD.A2.up&quot;, &quot;RR.A1&quot;,&quot;sd.lnRR.A1&quot;,&quot;RR.A1.lo&quot;,&quot;RR.A1.up&quot;, &quot;RR.A2&quot;,&quot;sd.lnRR.A2&quot;,&quot;RR.A2.lo&quot;,&quot;RR.A2.up&quot;, &quot;a.INT&quot;, &quot;sd.a.INT&quot;, &quot;a.INT.lo&quot;, &quot;a.INT.up&quot;,&quot;RERI&quot;,&quot;sd.lnRERI&quot;,&quot;RERI.lo&quot;,&quot;RERI.up&quot;, &quot;m.INT&quot;, &quot;sd.ln.m.INT&quot;, &quot;m.INT.lo&quot;, &quot;m.INT.up&quot; ) int.r[,c(&quot;A1&quot;,&quot;A2&quot;)] &lt;- expand.grid(c(0,1), c(0,1)) # on peut retrouver les IC95% par delta method # A1 = 0 et A2 = 0 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;]) # A1 = 1 et A2 = 0 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;]) # A1 = 0 et A2 = 1 int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A2&quot;]) # A1 = 1 et A2 = 1 int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- plogis(beta[&quot;(Intercept)&quot;] + beta[&quot;A1&quot;] + beta[&quot;A2&quot;] + beta[&quot;A1:A2&quot;]) # RD.A1.A2is0 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RD.A2.A1is1 int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] # RR.A1.A2is0 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A1.A2is1 int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # RR.A2.A1is1 int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) # additive interaction int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] # RERI int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) # multiplicative interaction int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - log(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + log(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) ## IC95% if(estimator == &quot;iptw&quot; | estimator == &quot;tmle&quot;) { # A1 = 0 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]),0,0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]),0,0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 grad &lt;- rep(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), 4) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), 0, int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0], 0, 0) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 grad &lt;- c(int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0, 1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1], 0 ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]), int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI grad &lt;- c((int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1]) + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) - (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]), (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] * (1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1])) / (int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0]) ) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction grad &lt;- c(int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0], int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1], 1 - int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1]) v &lt;- t(grad) %*% var(IC) %*% grad int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sqrt(v / nrow(data)) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } if(estimator == &quot;gcomp&quot;) { ltmle_MSM$bootstrap.res$p.A1_0.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept) ltmle_MSM$bootstrap.res$p.A1_1.A2_0 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1) ltmle_MSM$bootstrap.res$p.A1_0.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A2) ltmle_MSM$bootstrap.res$p.A1_1.A2_1 &lt;- plogis(ltmle_MSM$bootstrap.res$beta.Intercept + ltmle_MSM$bootstrap.res$beta.A1 + ltmle_MSM$bootstrap.res$beta.A2 + ltmle_MSM$bootstrap.res$beta.A1A2) ltmle_MSM$bootstrap.res$RD.A1.A2_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A1.A2_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 ltmle_MSM$bootstrap.res$RD.A2.A1_0 &lt;- ltmle_MSM$bootstrap.res$p.A1_0.A2_1 - ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$RD.A2.A1_1 &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 ltmle_MSM$bootstrap.res$lnRR.A1.A2_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_0 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A1.A2_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_1) ltmle_MSM$bootstrap.res$lnRR.A2.A1_0 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_0.A2_1 / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$lnRR.A2.A1_1 &lt;- log(ltmle_MSM$bootstrap.res$p.A1_1.A2_1 / ltmle_MSM$bootstrap.res$p.A1_1.A2_0) ltmle_MSM$bootstrap.res$a.INT &lt;- ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0 ltmle_MSM$bootstrap.res$lnRERI &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 - ltmle_MSM$bootstrap.res$p.A1_1.A2_0 - ltmle_MSM$bootstrap.res$p.A1_0.A2_1 + ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / ltmle_MSM$bootstrap.res$p.A1_0.A2_0) ltmle_MSM$bootstrap.res$ln.m.INT &lt;- log((ltmle_MSM$bootstrap.res$p.A1_1.A2_1 * ltmle_MSM$bootstrap.res$p.A1_0.A2_0) / (ltmle_MSM$bootstrap.res$p.A1_1.A2_0 * ltmle_MSM$bootstrap.res$p.A1_0.A2_1)) # A1 = 0 et A2 = 0 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_0) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 0] # A1 = 1 et A2 = 0 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_0) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 0] # A1 = 0 et A2 = 1 int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_0.A2_1) int.r$p.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 0 &amp; int.r$A2 == 1] # A1 = 1 et A2 = 1 int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$p.A1_1.A2_1) int.r$p.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$p.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$p[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.p[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A1.A2is0 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_0) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] # RD.A1.A2is1 int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A1.A2_1) int.r$RD.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] # RD.A2.A1is0 int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_0) int.r$RD.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] # RD.A2.A1is1 int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$RD.A2.A1_1) int.r$RD.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$RD.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.RD.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] # RR.A1.A2is0 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_0) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 0] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 0]) # RR.A1.A2is1 int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A1.A2_1) int.r$RR.A1.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) int.r$RR.A1.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.lnRR.A1[int.r$A1 == 1 &amp; int.r$A2 == 1])) # RR.A2.A1is0 int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_0) int.r$RR.A2.lo[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 0 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 0 &amp; int.r$A2 == 1]) # RR.A2.A1is1 int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRR.A2.A1_1) int.r$RR.A2.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RR.A2.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRR.A2[int.r$A1 == 1 &amp; int.r$A2 == 1]) # additive interaction int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$a.INT) int.r$a.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] - qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] int.r$a.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- int.r$a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] + qnorm(0.975) * int.r$sd.a.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] # RERI int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$lnRERI) int.r$RERI.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$RERI.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$RERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.lnRERI[int.r$A1 == 1 &amp; int.r$A2 == 1]) # multiplicative interaction int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- sd(ltmle_MSM$bootstrap.res$ln.m.INT) int.r$m.INT.lo[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) - qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) int.r$m.INT.up[int.r$A1 == 1 &amp; int.r$A2 == 1] &lt;- exp(log(int.r$m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) + qnorm(0.975) * int.r$sd.ln.m.INT[int.r$A1 == 1 &amp; int.r$A2 == 1]) bootstrap.res &lt;- ltmle_MSM$bootstrap.res } return(list(int.r = int.r, bootstrap.res = bootstrap.res)) } ### Obtention du MSM par la fonction ltmle, estimation par gcomp, iptw ou tmle # avec la fonction int.ltmleMSM() # on définit les arguments de la fonction ltmleMSM du package ltmle library(ltmle) library(SuperLearner) ## arguments à renseigner Q_formulas = c(Y=&quot;Q.kplus1 ~ L1 + L2 + L3 + A1 * A2&quot;) # useful to add A1 * A2 interaction here g_formulas = c(&quot;A1 ~ L1 + L2&quot;, &quot;A2 ~ L1 + L3&quot;) SL.library = list(Q=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;), g=list(&quot;SL.glm&quot;, c(&quot;SL.glm&quot;, &quot;screen.corP&quot;), &quot;SL.xgboost&quot;, &quot;SL.rpartPrune&quot;, #&quot;SL.randomForest&quot;, &quot;SL.step.interaction&quot;, c(&quot;SL.step.interaction&quot;,&quot;screen.corP&quot;), &quot;SL.glmnet&quot;, &quot;SL.stepAIC&quot;, &quot;SL.mean&quot;)) ### estimation par IPTW et TMLE interaction.ltmle &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, SL.library = SL.library, gcomp = FALSE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;) ### estimation par g-computation # par défaut, il fait une régression logistique à partir de la formule Q_formulas # si on veut faire un régression linéaire pour le modèle additif, on peut créer une fonction de SuperLearner # à partir de la fonction SL.glm SL.glm.gaussian &lt;- function (Y, X, newX, family = &quot;gaussian&quot;, # tout est comme SL.glm, sauf cette famille &quot;gaussian&quot; obsWeights, model = TRUE, ...) { if (is.matrix(X)) { X = as.data.frame(X) } fit.glm &lt;- glm(Y ~ ., data = X, family = family, weights = obsWeights, model = model) if (is.matrix(newX)) { newX = as.data.frame(newX) } pred &lt;- predict(fit.glm, newdata = newX, type = &quot;response&quot;) fit &lt;- list(object = fit.glm) class(fit) &lt;- &quot;SL.glm&quot; out &lt;- list(pred = pred, fit = fit) return(out) } environment(SL.glm.gaussian) &lt;-asNamespace(&quot;SuperLearner&quot;) interaction.gcomp &lt;- int.ltmleMSM(data = df, Q_formulas = Q_formulas, g_formulas = g_formulas, Anodes = c(&quot;A1&quot;, &quot;A2&quot;), Lnodes = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L3&quot;), Ynodes = c(&quot;Y&quot;), final.Ynodes = &quot;Y&quot;, # SL.library = SL.library, SL.library = list(Q=&quot;SL.glm.gaussian&quot;, # g=&quot;SL.mean&quot;), gcomp = TRUE, # si FALSE, fait tmle + IPTW iptw.only = FALSE, # si (gcomp = FALSE et iptw.only = TRUE), fait uniquement iptw survivalOutcome = FALSE, variance.method = &quot;ic&quot;, B = 1000, # nombre d&#39;échantillons bootstrap boot.seed = 54321) # seed pour l&#39;échantillonnage bootstrap ### 3) Calcul des paramètres utiles pour l&#39;analyse de l&#39;interaction # avec la fonction summary.int() ### récupération des résultats tmle summary.tmle &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;tmle&quot;)) # summary.tmle$int.r ### récupération des résultats iptw summary.iptw &lt;- summary.int(data = df, ltmle_MSM = interaction.ltmle, estimator = c(&quot;iptw&quot;)) # summary.iptw$int.r ### récupération des résultats gcomputation summary.gcomp &lt;- summary.int(data = df, ltmle_MSM = interaction.gcomp, estimator = c(&quot;gcomp&quot;)) # summary.gcomp$int.r # head(summary.gcomp$bootstrap.res) # # vérifier la normalité des estimations bootstrap # bootstrap.est &lt;- subset(summary.gcomp$bootstrap.res, # select = # c(&quot;p.A1_0.A2_0&quot;, # &quot;p.A1_1.A2_0&quot;, # &quot;p.A1_0.A2_1&quot;, # &quot;p.A1_1.A2_1&quot;, # &quot;RD.A1.A2_0&quot;, # &quot;RD.A1.A2_1&quot;, # &quot;RD.A2.A1_0&quot;, # &quot;RD.A2.A1_1&quot;, # &quot;lnRR.A1.A2_0&quot;, # &quot;lnRR.A1.A2_1&quot;, # &quot;lnRR.A2.A1_0&quot;, # &quot;lnRR.A2.A1_1&quot;, # &quot;a.INT&quot;, # &quot;lnRERI&quot;, # &quot;ln.m.INT&quot;)) # par(mfrow = c(4,4)) # for(c in 1:ncol(bootstrap.est)) { # hist(bootstrap.est[,c], freq = FALSE, main = names(bootstrap.est)[c]) # lines(density(bootstrap.est[,c]), col = 2, lwd = 3) # curve(1/sqrt(var(bootstrap.est[,c]) * 2 * pi) * exp(-1/2*((x-mean(bootstrap.est[,c]))/sd(bootstrap.est[,c]))^2), # col = 1, lwd = 2, lty = 2, add = TRUE) # par(mfrow = c(1,1)) # } Au final, on a (présentation selon recommandation Knol et al. [8]): 10.3.1 TMLE ## $out.table ## A2=0 A2=1 RD.A2|A1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] 0.091 [0.073,0.109] ## A1=1 $p_{10}$=0.408 [0.378,0.439] $p_{11}$=0.903 [0.88,0.927] 0.495 [0.457,0.534] ## RD.A1|A2 0.304 [0.272,0.336] 0.708 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RR.A2|A1 ## A1=0 1.88 [1.67,2.11] ## A1=1 2.21 [2.04,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.404 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.89 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; 10.3.2 IPTW ## $out.table ## A2=0 A2=1 RD.A2|A1 ## A1=0 $p_{00}$=0.104 [0.095,0.113] $p_{01}$=0.195 [0.18,0.21] 0.091 [0.073,0.109] ## A1=1 $p_{10}$=0.408 [0.377,0.439] $p_{11}$=0.904 [0.88,0.927] 0.496 [0.457,0.535] ## RD.A1|A2 0.304 [0.272,0.336] 0.709 [0.68,0.737] ## RR.A1|A2 3.93 [3.5,4.41] 4.63 [4.55,4.72] ## RR.A2|A1 ## A1=0 1.88 [1.67,2.11] ## A1=1 2.22 [2.05,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.405 [0.362;0.447]&quot; ## [2] &quot;RERI = 3.9 [3.45;4.4]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.36]&quot; 10.3.3 G-computation ## $out.table ## A2=0 A2=1 RD.A2|A1 ## A1=0 $p_{00}$=0.104 [0.095,0.112] $p_{01}$=0.197 [0.183,0.211] 0.093 [0.076,0.11] ## A1=1 $p_{10}$=0.4 [0.373,0.427] $p_{11}$=0.893 [0.872,0.915] 0.494 [0.459,0.528] ## RD.A1|A2 0.296 [0.267,0.325] 0.697 [0.671,0.722] ## RR.A1|A2 3.86 [3.45,4.32] 4.54 [4.46,4.61] ## RR.A2|A1 ## A1=0 1.9 [1.69,2.13] ## A1=1 2.23 [2.08,2.4] ## RD.A1|A2 ## RR.A1|A2 ## ## $interaction.effects ## [1] &quot;additive Interaction = 0.4 [0.362;0.439]&quot; ## [2] &quot;RERI = 3.86 [3.46;4.31]&quot; ## [3] &quot;multiplicative Interaction = 1.18 [1.02;1.35]&quot; Références "],["représentations-graphiques.html", "Chapter 11 Représentations graphiques", " Chapter 11 Représentations graphiques "],["présentation-des-résultats.html", "Chapter 12 Présentation des résultats 12.1 Modification d’effet 12.2 Interaction 12.3 Proposition", " Chapter 12 Présentation des résultats Les recommandation de Knol et al. [8] sont : 12.1 Modification d’effet Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC pour A1 dans les strates de A2. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y a été ajustée. 12.2 Interaction Présenter les risques relatifs (RR), les OR ou les différences de risque (RD) avec les IC pour chaque strate de A1 et de A2 avec une seule catégorie de référence (éventuellement prise comme la strate présentant le plus faible risque de Y). Présenter les RR, OR ou RD avec les IC de l’effet de A1 sur Y dans les strates de A2 et de A2 sur Y dans les strates de A1. Présenter les mesures de la modification de l’effet sur des échelles additives (par exemple, RERI) et multiplicatives avec les IC. Énumérez les facteurs de confusion pour lesquels la relation entre A1 et Y et la relation entre A2 et Y ont été ajustées. 12.3 Proposition Présenter les effets marginaux ou les proportions dans chaque strate présenter les effets dans chaque strate dans une échelle multiplicative et additive Références "],["proposition-détapes.html", "Chapter 13 Proposition d’étapes", " Chapter 13 Proposition d’étapes formuler l’objectif predictif ou explicatif interaction ou modification d’effet? DAG, estimand, estimateur Description tableau croisé Analyses exploratoires régressions avec terme d’interaction analyses stratifiées marge Analyses confirmatoire g computation MSM "],["exemple-1---y-binaire.html", "Chapter 14 Exemple 1 - Y binaire", " Chapter 14 Exemple 1 - Y binaire "],["exemple-2---y-quantitatif.html", "Chapter 15 Exemple 2 - Y quantitatif", " Chapter 15 Exemple 2 - Y quantitatif 1. Objectifs Dans cette étude [ref], on s’est intéressé, de façon explicative, à l’effet de la défavorisation sociale précoce sur le taux de cholestérol LDL vers 45 ans; mais aussi à l’effet du sexe sur ce taux de cholestérol en fonction de la défavorisation sociale précoce. Ici on s’intéresse donc à deux modifications d’effet. 2. DAG, estimand, estimateur Le DAG (sans les médiateurs) était : &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &gt; ======= &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; e225b751959377b69a975cacad5b91fa4793254f Les estimands étaient définis sur l’échelle additive par : \\(\\small (Y_{s=1|d=0} - Y_{s=0|d=0}) - (Y_{s=1|d=1} - Y_{s=0|d=1})\\) \\(\\small (Y_{d=1|s=0} - Y_{d=0|s=0}) - (Y_{d=1|s=1} - Y_{d=0|s=1})\\) Ils sont ici équivalents car il n’y pas de facteurs de confusion, donc, par exemple, \\(\\small Y_{d=1|s=0} = Y_{s=0|d=1} = Y_{d=1,s=0}\\) Les effets ont été estimés par g-computation (standardisation). 3. Résultats "],["exemple-3---y-multinomial.html", "Chapter 16 Exemple 3 - Y multinomial", " Chapter 16 Exemple 3 - Y multinomial "],["exemple-4---x-quantitatif.html", "Chapter 17 Exemple 4 - X quantitatif", " Chapter 17 Exemple 4 - X quantitatif "],["synthèse-générale.html", "Chapter 18 Synthèse générale", " Chapter 18 Synthèse générale La première étape importantes consiste à définir précisément l’objectif. Et, si l’on est dans une démarche explicative, d’inférence causale, il s’agit de définir si la mesure d’un effet d’interaction est nécessaire pour y répondre (identifier précisément l’effet que l’on cherche à estimer, ou estimand). Le fait de choisir une démarche d’analyse d’interaction ou de modification d’effet repose sur : la façon dont la question est posée (effet de X selon V ou effet conjoint de X et V), sur les hypothèses causales formulées (scénarii \\(\\small do(X)\\) ou \\(\\small do(X,V)\\)) et donc sur les sets de facteurs de confusion à considérer (seulement sur \\(\\small X \\rightarrow Y\\) ou \\(\\small X.V \\rightarrow Y\\)). Concernant le choix de l’échelle, idéalement, les interactions devraient être reportées sur les 2 échelles [8] [2]. Cependant, l’échelle additive est plus appropriée pour évaluer l’utilité en santé publique [2] [8]. Concernant les paramètres, Références "],["pour-aller-plus-loin.html", "Chapter 19 Pour aller plus loin… 19.1 Ajouter de la complexité 19.2 Interaction avec confusion intermédiaire 19.3 Interaction et médiation", " Chapter 19 Pour aller plus loin… 19.1 Ajouter de la complexité A1 et A2 sont rarement indépendants. Scénario plus probable : &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &gt; ======= &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; e225b751959377b69a975cacad5b91fa4793254f 19.2 Interaction avec confusion intermédiaire 19.3 Interaction et médiation [9] [10] Références "],["références.html", "Chapter 20 Références", " Chapter 20 Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> 67ef8f7f26e24679e4f61da5df4cb4f40a091524
